<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Development</title>
  <!-- Site Icon -->
  <link rel="icon" href="../static/images/flavicon_3.png" type="image/x-icon" sizes="16x16">
  <!-- Stylesheets -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="../static/css/bootstrap.min.css">
  <link rel="stylesheet" href="../static/css/stylesheet.css">
</head>

<body>

  <div class="container">
    <nav class="navbar navbar-default nav-style">
      <div class="container-fluid">
        <div class="navbar-header">
          <a class="navbar-brand" href="/"><img src="../static/images/site title icon.png" style="height:75px;" alt="Predict Your Salary"></a>
        </div>
        <li><a href="/">Home</a></li>
        <li><a href="/predictions">Predictions</a></li>
        <li><a href="/factors">Factors</a></li>
        <li class="dropdown">
          <a class="dropdown-toggle" data-toggle="dropdown" href="#">Tools
          <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a class="dropdown-item" href="/tools">Industry Favorites</a></li>
            <li><a class="dropdown-item" href="/audit">Bootcamp Audit</a></li>

            <li class="dropdown-submenu"><a class="dropdown-item dropdown-toggle" href="#">Job Search</a>
              <ul class="dropdown-menu">
                <li><a class="dropdown-item" href="/requirements">Most Common Requirements</a></li>
                <li><a class="dropdown-item" href="/wordcloud">Indeed Requirements</a></li>
                <li><a class="dropdown-item" href="/wordcloud_nycnj">NJ/NY Requirements</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="/development">Development</a></li>
        <li><a href="/team">About Us</a></li>
    </div>
  </nav>

    <div class="view overlay my-4">
      <img src="../static/images/developement.png" class="img-fluid" alt="">
      <br>
      <br>
      <br>

      <h1>Brief introduction:</h1>
      <p>As most of the Bootcamp graduates will be looking for a new job, our group developed an app to explore the data professional world job market. Through our app, you can see a salary prediction based on your region, years of experience and primary tools used in a specific job title. We also explore the
        different tools used in the field and the expectations that each of these data jobs possess.
        By using our page, you can estimate how much you can expect to earn with the
        current skills you have as well as compare your expertise to those in the field. </p>


      <p> We used the skills that we learned in the Rutgers Bootcamp Course in order to accomplish this task. Below, you will find a step by step guide behind our developement process!</p>

      <p><i>All version control handled using GitHub.</i></p>

      <br>

      <h1>Step 1: Data Processing</h1>
      <div class="progress">
        <div class="progress-bar progress-bar-striped progress-bar-animated" role="progressbar" style="width: 10%"
          style="height: 40px" aria-valuenow="10" aria-valuemin="0" aria-valuemax="100">10%</div>
      </div>

      <br>

      <h4> Data Sources</h4>
      <p>The first data set was collected from 171 countries as a survey for a Kaggle Competition. The survey responders
        were data professionals who are currently working in the field. We used their responses for the visuals for
        salary, gender, country, age and multiple data tools that they use in their current jobs. <a
          href="https://www.kaggle.com/c/kaggle-survey-2019">Get Data Here</a>

      <p> Our second dataset was used to develop our machine learning model for the salary prediction. We used factors
        such as location, title, primary database use, management level, education and year experience to predict a
        salary for a data professional.
        <a href="https://www.brentozar.com/archive/2019/01/the-2019-data-professional-salary-survey-results/">Get Data
          Here</a>

        <br>

      <h4> Cleaning Dataset 1 Kaggle Survey:</h4>
      <ul>
        <li>The original CSV file was loaded in Jupyter Notebook and consisted of 246 columns and 19718 rows and was
          narrowed down to 109 columns and 19718 rows.</li>
        <li>Columns were renamed for easier data access.</li>
        <li>Age and Salary columns consisted of ranges therefore we took both of those fields and took their column
          average that was needed for our visualizations. </li>
        <li>Extra characters such as: $, :, > were subsequently stripped from the text and columns were converted to
          integers or floats as needed.</li>
        <li>A clean CSV was exported for the remaining work. </li>
      </ul>


      <h4>Cleaning Dataset 2 Brent Ozar Data:</h4>
      <ul>
        <li>The original CSV file was loaded in Jupyter Notebook and consisted of 6893 rows and 29 columns and was
          narrowed down to 6893 rows and 17 columns.</li>
        <li>By inspecting the column types we did the following a) converted them into needed types such as integers or
          floats, b) replaced extra characters such as commas or dollar signs c) replaced values in the years column
          that were not filled out correctly by the user. </li>
        <li>There were 10 job fields that were dropped from the whole dataset because they only consisted of 1-2 entries
          for each. </li>
        <li>Job titles within the same category such as Database Analyst were merged into one category or separated
          based on management. </li>
        <li>We merged a new dataset that included a world region based on country because we wanted our future data to
          be based on the region on the world rather than each individual country. </li>
        <li>A clean CSV was exported for the remaining work. </li>

      </ul>
      </p>

      <br>

      <h1>Step 2: Machine Learning</h1>
      <div class="progress">
        <div class="progress-bar progress-bar-striped bg-success progress-bar-animated" role="progressbar"
          style="width: 25%" aria-valuenow="25" aria-valuemin="0" aria-valuemax="100">25%</div>
      </div>

      <br>
      <p>Our machine learning model involved using a RandomForestClassifier and feature importance to narrow down the
        features for our model. We selected 10 features for our model, which are, in order: country, primary database,
        years with this database, region, manage staff, education is computer related, years with this type of job,
        telecommute days per week, employment sector, and job title. Because most of these features are categorical, and
        the model needs them to be numerical, pd.get_dummies was then used to convert categories to binary. </p>
      <h4>Scikit-Learn Model </h4>
      <ul>
        <li>Next we split the data into training and test using SKLearn and then scaled the data using Standard Scaler.
        </li>
        <li>Because our output was numerical, we then trained and tested our model using: Linear Regression, Gradient
          Boosting Regressor, Lasso, Ridge and Elastic Net. The best with the best fit was the Gradient Boosting
          Regressor, with an r2 value of 0.6.</li>
        <li>Lastly, we saved the Gradient Boosting Regressor model as a file to use in our salary predictor in the flask
          app. </li>


      </ul>

      <h4>Natural Language Processing </h4>
      <p>To see if we have the skills employers are looking for in the data science profession, we decided to look from the employer vantage point by analyzing the data science jobs available on <a data-from-md  title='http://indeed.com' href='http://indeed.com' type=''>indeed.com</a>. In this approach, we scraped job descriptions (Beautiful Soup) for all data science jobs available on <a data-from-md  title='http://indeed.com' href='http://indeed.com' type=''>indeed.com</a>.  We processed those job descriptions through a TSNE Visualizer in order to identify evident clusters within the dataset and ran a TF-IDF Vectorizer to see the most common words used in each cluster.</p>

            <ul>
        <li>
          We decided to look from the employer's point of view by analyzing available data science jobs indeed.com.
          In this approach, we scraped job descriptions (Beautiful Soup) for all data science jobs from the site.
        </li>
        <li>
          We processed those job descriptions through a TSNE Visualizer in order to identify evident clusters within
          the dataset and ran a TF-IDF Vectorizer to see the most common words used in each cluster. All job descriptions
          were then loaded into a TF-IDF Vectorizer and WordClouds were generated for each cluster. We will attempt to
          write a job summary based off most of the words.
        </li>
        <li>
          For amusement, we will attempt to create a job summary utilizing all the job descriptions from the dataset.
          We will use ntlk to tokenize the data by applying the CountVectorizer (one-hot encoding) and TF-IDF transformer
          (adding weights to the encoding).  We can than fit the data into a PCA (Principal Component Anlysis) model in an
          attempt to reduce the dimensionality of the data.
        </li>
      </ul>





      <br>

      <h1>Step 3: Flask App</h1>
      <div class="progress">
        <div class="progress-bar progress-bar-striped bg-info progress-bar-animated" role="progressbar"
          style="width: 50%" aria-valuenow="50" aria-valuemin="0" aria-valuemax="100">50%</div>
      </div>

      <br>

      <h4> Developing the Flask App </h4>
      <p>By using Python, Flask, and SQLAlchemy, we were able to connect our machine-learning model with the HTML page
        where users can submit a form.</p>

      <ul>
        <li>Flask and SQLAlchemy were used to run different queries of the data and render API’s of the JSONifed
          versions of those queries.</li>
        <li>/tools_data/ route focuses on the popularities of different technologies of the data world </li>
        <li>/tools_data/<jobtitle> does the same as the route above, except there is an added filter to it that allows
            the user to add a job title to the URL</li>
        <li>/recommendations_data provides a count on the most recommended technologies to start with as a data
          professional.</li>
        <li>/recommendations_data/<jobtitle> does the same as the route above, except there is an added filter to it
            that allows the user to add a job title to the URL</li>
        <li>/education_data provides a breakdown of the count of each education level of each job title and also
          provides the average salary of each job title</li>
        <li>/country_region_data provides a list of all the unique country names of the dataset and its corresponding
          region</li>
        <li>/predictions is the route where the user’s response from the form is sent to. Their response is then
          processed to fit the model, and a prediction is returned. That response is sent back to that page to be used
          with Jinja.</li>
        The rest of the routes are just simple routes that render html pages.
      </ul>


      <br>

      <h1>Step 4: Visualizations</h1>
      <div class="progress">
        <div class="progress-bar progress-bar-striped bg-warning progress-bar-animated" role="progressbar"
          style="width: 75%" aria-valuenow="75" aria-valuemin="0" aria-valuemax="100">75%</div>
      </div>
      <br>


      <p> Our visualizations focused primarly on factors affecting salary. </p>

      <h4>Tableau</h4>
      <ul>
        <li>The cleaned CSV file 1 was loaded into Tableau Desktop. </li>
        <li>The following visualizations were made based on factors affecting salary: gender, age, location, education
          level,
          certificates and computer science background were all analyzed for how they played a role in the salary
          amount.
        </li>
        <li>Another visualization compares what we learned in the bootcamp compared to the expectations of the field. </li>
        <li>The final dashboards were published in Tableau Public and finally embedded into our html page using the
          Tableau JavaScript API.</li>
      </ul>

      <h4>D3</h4>
      <ul>
        <li>Word cloud of the recommended first language to learn was built using a d3 version 3 base code and expanded upon to filter out values and repopulate only on changing inputs.</li>
        <li>Bubble chart of tool/language use in industry was built using d3 version 5 and d3.force was used to build movement animation.</li>
        <li>To allow for compatibility between the two versions of d3, d3 version 5 was assigned a new variable name (d3v5) and all scripts rewritten to account for this change.</li>
        <li>Filtering of the two visualization by job title was built out using d3 and onclick functions inside the HTML.</li>
      </ul>

      <br>
      <h1>Step 5: Webpage and AWS Deployment</h1>
      <div class="progress">
        <div class="progress-bar progress-bar-striped bg-danger progress-bar-animated" role="progressbar"
          style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">100%</div>
      </div>
      <br>

      <h4>HTML/CSS/Bootstrap</h4>
      <p>The final part of the project was telling the story of our data. The best tool for this job involved creating
        an interactive webpage allowing the user to explore their career options for their future career in data
        science. We wanted to introduce the page with the main highlights of the trade by summarizing the job titles.
        Our goal was to create a clean, modern, creative and a user friendly page. For this part, we used formatting
        provided by the Bootstrap library, direct CSS styling, and the Canva platform to create our graphics. Animations were built using CSS and JS.</p>

      <h4>App Deployment</h4>
      <p>We deployed our application on Heroku with AWS RDS as the backend database instead of using Heroku postgres DB which has limitation of 10K records. Intially, we tried to deploy using the AWS EC2 Server Apache2 web service but there were challenges as it keep defaulting to Python 2.7 which doesnt have SQL Alchemy packages used in our project.</p>

      <br>
      <img src="../static/images/tools_used.png" class="img-fluid" alt="">




      <!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
      <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
 -->

 <!-- Styling scripts -->
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
 <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

 <!-- Dropdown submenu script -->
 <script>
   $('.dropdown-menu a.dropdown-toggle').on('click', function(e) {
     if (!$(this).next().hasClass('show')) {
       $(this).parents('.dropdown-menu').first().find('.show').removeClass("show");
     }
     var $subMenu = $(this).next(".dropdown-menu");
     $subMenu.toggleClass('show');
     $(this).parents('li.nav-item.dropdown.show').on('hidden.bs.dropdown', function(e) {
       $('.dropdown-submenu .show').removeClass("show");
     });
     return false;
   });
 </script>

</body>

</html>
