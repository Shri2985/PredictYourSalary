,title,jobclass,url,jobdescription
0,Data Scientist,data scientist,/rc/clk?jk=5247c00d10cc43e8&fccid=5b63e967a659d38e&vjs=3,"Data Scientist Based in Manhattan, NY Competitive base salaries | Excellent benefits package | Global hyper-growth business I am looking to speak with exceptional Data Scientists to join our world-class innovation team at Wynden Stark LLC. You will be working collaboratively with other Data Scientists and Back-End Engineers to invent automated tools and services that incorporate probabilistic models, thereby adding value both to our internally business & external clients. You will develop and deploy state-of-the-art deep learning algorithms into real-time production environments, that automate processes in the Recruitment & HR sectors. Qualifications & Skills Required Graduate degree in a quantitative discipline, with a minimum of 2 years' hands-on industry experience Specialization in deep neural networks and/or natural language processing Excellent programming skills in Python in particular any data-related packages Professional experience or publication-level experience in the development, training and validation of machine learning models applied to large, high-dimensional data Experience with artificial neural networks; deep learning libraries; natural language processing; AP Is; cloud platforms; large-scale data tools; advanced mathematics We offer Attractive compensation & benefits package health, vision, dental, pension Generous PTO & Volunteer Day options High autonomy and flexibility Opportunities for travel across our global locations Company incentive trips based off individual performance Weekly cultural-driven activities Internal and external company events hosted on-site Personal brand growth opportunities publishing, patents, present at conferences, blogs Email kate. drury@gqrgm. com for a confidential discussion about your career. "
1,DATA SCIENTIST,data scientist,/rc/clk?jk=b3cdbfc56027234b&fccid=8524d30bbfaa4720&vjs=3,"Description On Point is a connected solutions business within Koch Engineered Solutions, a unit of Koch Industries, Inc. , that leverages unique engineering capabilities and expertise in combination with digital technology. The On Point solutions portfolio drives advanced, actionable insights to help customers identify root causes, analyze performance, and optimize equipment as well as plant operations to improve efficiency, yield, reliability, environmental performance, and safety. On Point Digital Solutions is looking for a Data Scientist to join our competitive data analytics and software development team. On Point is a new Koch engineered digital solutions company, focused to provide actionable insights for our customers to enable real time monitoring and optimization of their operations. On Point combines our deep subject matter expertise in combustion, distillation, separation, filtration with digital technology to deliver new solutions and technology which will create value for our customers. The Data Scientist will work with a dynamic team and be responsible for delivering A. I. algorithms to solve problem in industrial arena and expand the capability of our analytics platform to deploy the developed solutions at scale. A Day In The Life Typically Includes Research, design and develop machine learning algorithms as part of the product development team Perform Root Cause Analysis leveraging data science on large industrial problems related to process control, process engineering, and discrete manufacturing environments Collaborate with our process engineering teams as the expert in applied data science and machine learning techniques Use industry technologies, tools, and data mining frameworks for data analytics including data visualization for analyzing, optimizing, developing hypotheses, and drawing conclusions What You Will Need Basic Qualifications 3+ years of industry experience analyzing data sets and applying machine learning to assist decision making and solve industrial problems 3+ years of experience building and deploying scalable A. I. models using state of the art learning algorithms. 3+ years of experience on building models from ground up using python and packages like sklearn, pandas, xgboost, tensorflow and keras as well as tools such as Jupyter notebook Bachelor's Degree in Mathematics, Computer Science, Computer Engineering, Physics, Data Science, Engineering, or Statistics from an accredited institution What Will Put You Ahead Preferred Qualifications Master’s degree or Ph D in Mathematics, Computer Science, Computer Engineering, Physics, Data Science or Statistics from an accredited institution Ability to comprehend research papers and implement it 2+ years of experience working with web services like AWS, Azure or Google cloud. Experience deploying models to production and maintain models in a commercial setup The ability to communicate results clearly and a focus on driving impact Salary and benefits commensurate with experience. Equal Opportunity Employer. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information www. kochcareers. com/doc/Everify. pdf"
2,Data Scientist Senior Consultant - Remote NOW!,data scientist,/rc/clk?jk=e769ff6cbd5e40be&fccid=105ecfd0283f415f&vjs=3,"Data Scientist Senior Consultant Remote NOW!049489 Senior Consultant Data Scientist Overview The Data Science & Analytics practice group at Capgemini is expanding its footprint…rapidly. As part of the fastest growing digital practice within Capgemini, we work with the latest advanced analytics, machine learning, and big data technologies to extract meaning and value from data in a number of different industries ranging from Media & Entertainment to Life Sciences and everywhere in-between. Our team has worked with geospatial data, on social media sentiment analysis, built recommendation systems, created image classification algorithms, solved large-scale optimization problems, and harnessed the massive influx of data generated by the Io T. The Data Science & Analytics group is the fastest growing digital practice at Capgemini demanding agile innovation. As part of the Data Science & Analytics group, you will work in a collaborative environment with internal and client resources to understand key business goals, build solutions, and present findings to client executives while solving real-world problems. If you are passionate about solving problems in the realm of cognitive computing, big data, and machine learning while utilizing business acumen, statistical understanding, and technical know-how, the Data Science & Analytics practice group at Capgemini is the best place to grow your career. Role & Responsibilities Work with team to build out data science use case Po C related to staffing and new assignments. Leverage machine learning models and visualization tools. Assess current state data science governance model and architecture and propose future state Co E model. Leverage machine learning models and visualization tools. Assess current state data science governance model and architecture and propose future state Co E model. Work in collaborative environment with global teams to drive client engagements in a broad range of industries Aerospace & Defense, Automotive, Banking, Consumer Products & Retail, Financial Services, Healthcare, High Tech, Industrial Products, Insurance, Life Sciences, Manufacturing, Public Sector, Telecom, Media & Entertainment, and Energy & Utilities. Quickly understand client needs, develop solutions, and articulate findings to client executives. Provide data-driven recommendations to clients by clearly articulating complex technical concepts through generation and delivery of presentations. Analyze and model both structured and unstructured data from a number of distributed client and publicly available sources. Perform EDA and feature engineering to both inform the development of statistical models and generate improve model performance and flexibility. Design and build scalable machine learning models to meet the needs of given client engagement. Assist with the mentorship and development of consultants. Assist in growing data science practice by meeting business goals through client prospecting, responding to proposals, identifying and closing opportunities within identified client accounts Requirements 3-5 years professional work experience as a data scientist or on advanced analytics / statistics projects. Machine Learning R/Python programming Natural Language processing Text analytics Visualization platforms including Tableau and/or Power BI Ability to generate professional visualizations and reporting Ability to interface with client SM Es Ability to understand analytics business requirements and develop custom models and reporting Experience in consulting environment a plus Skills with Neo4J/Cyper Master’s degree from top tier college/university in Computer Science, Statistics, Economics, Physics, Engineering, Mathematics, or other closely related field. Excellent Python skills Experience with entity matching , record linkage and data cleansing probabilistic distance Experience with blocking methods Experience with Py Spark Ph D preferred. Strong understanding and application of statistical methods and skills distributions, experimental design, variance analysis, A/B testing, and regression. Statistical emphasis on data mining techniques, Bayesian Networks Inference, CHAID, CART, association rule, linear and non-linear regression, hierarchical mixed models/multi-level modeling, and ability to answer questions about underlying algorithms and processes. Experience with both Bayesian and frequentist methodologies. Mastery of statistical software, scripting languages, and packages e. g. R, Matlab, SAS, Python, Pearl, Scikit-learn, Caffe, SAP Predictive Analytics, KXEN, ect. . Knowledge of or experience working with database systems e. g. SQL, No SQL, Mongo DB, Postgres, ect. Experience working with big data distributed programming languages, and ecosystems e. g. S3, EC2, Hadoop/Map Reduce, Pig, Hive, Spark, SAP HANA ect. Expertise in machine learning algorithms and experience using the following ML techniques Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, SV Ms, Time Series, K Means, Clustering, NMF . Preferred experience with NLP, Graph Theory, Neural Networks RN Ns/CN Ns , sentiment analysis and Azure ML. Experience building scalable data pipelines and with data engineering/ feature engineering. Preferred experience with web-scraping. Experience building and deploying predictive models. Experience with Power Point and ability to clearly articulate findings and present solutions. Excellent team-oriented and interpersonal skills. Visit us at www. capgemini. com. People matter, results count. Job Programmer/Analyst Schedule Full-time Primary Location US-NY-New York Organization I&D"
3,Senior Data Scientist,data scientist,/rc/clk?jk=d9463ba6254007f7&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the development, validation and delivery of algorithms, statistical models and reporting tools. Solves complex analytical problems. Fundamental Components Develops and/or uses algorithms and statistical predictive models and determines analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes. Performs analyses of structured and unstructured data to solve multiple and/or complex business problems utilizing advanced statistical techniques and mathematical analyses and broad knowledge of the organization and/or industry. Collaborates with business partners to understand their problems and goals, develop predictive modeling, statistical analysis, data reports and performance metrics. Develops and participates in presentations and consultations to existing and prospective constituents on analytics results and solutions. Interacts with internal and external peers and managers to exchange complex information related to areas of specialization. Use strong knowledge in algorithms and predictive models to investigate problems, detect patterns and recommend solutions. Use strong programming skills to explore, examine and interpret large volumes of data in various forms. Background Experience Demonstrates strong ability to communicate technical concepts and implications to business partners. Anticipates and prevents problems and roadblocks before they occur. Strong knowledge of advanced analytics tools and languages to analyze large data sets from multiple data sources. Demonstrates proficiency in most areas of mathematical analysis methods, machine learning, statistical analyses, and predictive modeling and in-depth specialization in some areas. 5 or more years of progressively complex related experience. Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline. Master’s degree or Ph D preferred. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
4,"Data Scientist, Medidata - Core",data scientist,/rc/clk?jk=f44f655a0cafacd3&fccid=3e34ac4ae73849ba&vjs=3,"Medidata is leading the digital transformation of life sciences, creating hope for millions of patients. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 1,400+ customers and partners access the world's most-used platform for clinical development, commercial, and real-world data. Medidata, a Dassault Systèmes company, is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www. medidata. com. We know that diverse teams win and are fully committed to selecting leaders and employees that represent the markets in which we operate. We are still led by our Co-founders, Tarek Sherif and Glen de Vries, and have global operations in the US, Europe and Asia with over 2000+ employees. Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. Medidata’s solutions have powered over 17,000+ clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. Your Mission Medidata is looking for an experienced data scientist to research and develop predictive models to deliver novel insights for Medidata’s Data Science Platform Suite of products. Data Science Platform is a critical piece of Medidata’s strategy to grow to over one billion in revenue and includes ingestion and export of data in addition to apps that standardize clinical data and run statistical algorithms that unlock the value of our data assets for our customers. Successful candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata’s journey of powering smarter treatments and healthier people. Your primary responsibilities will include Solving some of the most complex problems in healthcare, translating complex data into meaningful insights Designing, developing, and validating predictive models for novel medical applications. Evaluating and assessing novel tools, algorithms, and technologies that enable data science capabilities Providing support functions around model-building, including data cleaning and code review Working directly with our team comprised of the brightest minds in technology, research, and mathematics as well as senior interfaces from leading life sciences companies across the globe Your Competencies Ability to translate business challenges into data pipelines & model framework, owning and driving successful projects Strong verbal and written communication skills to articulate highly technical methods to diverse audiences to shape decision-making Expert-level fluency in statistical tools and programming languages to be self-sufficient in handling data Python, R, SQL Professional experience programming in a compiled language C++, C, C#, Java strongly preferred Professional-level competency working in a Unix environment Professional-level competency working with source control management Git Research-level experience in developing predictive modeling algorithms classification, regressions, feature selection etc. for large datasets Your Education & Experience Ph. D. in Math, Statistics, Computer Science, Physics, Engineering, Bioinformatics, or another quantitative field with a strong foundation in statistical methodology and algorithm development; OR Masters in a quantitative field with 3+ years of professional experience in data science. Passionate about staying current with new developments in the data science and machine learning field Ability to work creatively and collaboratively in a fast-paced dynamic environment to propose intelligent solutions Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. #LI-AS1"
5,"Data Scientist: Bloomingdale's, New York, NY",data scientist,/rc/clk?jk=e4dacfd0f792f2be&fccid=5e6dd49bdb583f94&vjs=3,"Job Overview The Data Scientist will work in the Customer Analytics and database group to develop innovative customer analytics, customer story strategy, and contact optimization tools. The Data Scientist will create practical methods to forecast demand, understand customer preferences, simulate business outcomes, and optimize the marketing investment. The Data Scientist will work closely with cross functional groups to enhance analytical capabilities and develop persuasive interactive visualizations. Essential Functions Answer complex business questions by using appropriate statistical techniques on available data or designing and running experiments to gather data Rapidly develop novel applications of classification, forecasting, simulation, optimization, and summarization techniques Work closely with cross functional teams to encourage statistical best practices with respect to experimental design and data analysis Create compelling interactive visualizations and presentations to enhance decision making capabilities throughout the company Qualifications Education/Experience M. S. or Ph. D. in relevant technical field such as Statistics, Computer Science, Engineering, or Economics Expert knowledge of an analytical tool such as SPARK, R, Python Experience with relational databases and SQL Experience working with large data sets and distributed computing tools such as Hadoop, Map/Reduce, Hive, etc. Familiarity with visualization tools such as Tableau, Power BI or Spotfire Communication Skills Ability to communicate complex quantitative analyses in a clear, precise, and actionable manner Other Skills Strong passion for answering complex business questions using structured problem solving and rigorous data analysis This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment. "
6,2021 Software Engineering & Technology University Program (S.E.T.U.P) – Data Scientist,data scientist,/rc/clk?jk=0d82a0c5c4f0d778&fccid=b4048be2884af072&vjs=3,"Software Engineering & Technology University Program S. E. T. U. P The Software Engineering & Technology University Program S. E. T. U. P is a pipeline to recruit, develop, and retain high-potential entry-level technology professionals who are prepared and positioned to contribute to the successful execution of BNY Mellon’s technology strategy and initiatives. The 2021 S. E. T. U. P program will begin in Winter 2021 and Summer 2021. Program At A Glance Assignments across various job functions within Technology* that combine learning with skill development through practical work and projects; Extensive training curriculum and ongoing learning assignments that will help develop technical, professional, interpersonal and leadership skills; and Career development and networking support from a host of corporate leaders including executive mentors, peer mentors, business stakeholders and a dedicated program manager. Upon successful completion of the program and based on overall business need, S. E. T. U. P participants will be matched to a full-time role, taking into account factors such as business requirements, analyst preferences, and overall performance throughout the program. Program Locations US Pittsburgh, PA *New York, NY * Jersey City, NJ Program Highlights Small, selective program size that allows for more personal attention and support; Participants can further develop technical skills/expertise, enhance leadership abilities, build networks across the organization and accelerate their careers; Robust onboarding and training curriculum designed specifically for S. E. T. U. P participants; Visibility and exposure to senior leaders in small cohorts and/or one-to-one meetings; Full commitment from top-level management to make our program the premier technology talent pipeline program within the financial services industry Data Scientist Primary Responsibilities Participate on a team to apply scientific method to find solutions to real business problems. Perform data analysis, feature engineering and advanced methods to prepare and develop decisions from data. Leverage simple to advanced data techniques to support the team to deliver data analytic products for the firm. Performs analytics in support of the identification and understanding observed business outcomes. Collaborates with others to deliver on hypothesis testing and developing the mathematics to describe the business opportunity. Communicates effectively with analytics staff. Develops analytics, prepares and delivers both informational and decision-seeking presentations. Stays abreast of organization and management changes and has in-depth knowledge of company practices relevant to data science products. Maintains knowledge of company's total computing environment and planned changes in order to develop meaningful data science products. Grow and develop skills across the 3 domain specialties model science, feature science and Insight science capabilities. Stressing expertise in the core functional areas Computer Programming, Math&Analytic Methodology, Distributed computing and communications of complex results. Qualifications Program Eligibility/Qualifications Bachelor's degree required. Candidate is typically a recent college hire with a bachelor's degree in computer science engineering or a related discipline. 0-3 years of experience in software development preferred. Previous technology internship is a plus BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums. Primary Location United States-New York-New York Internal Jobcode 96003 Job Information Technology Organization Office of CIO-HR18027 Requisition Number 2007814"
7,"2021 Student Technology, Agile & Readiness Training (S.T.A.R.T) Program – Data Scientist Track",data scientist,/rc/clk?jk=1d0510313da815e6&fccid=b4048be2884af072&vjs=3,"Student Technology, Agile & Readiness Training S. T. A. R. T Program BNY Mellon’s S. T. A. R. T program provides high-potential students with a well-rounded, rewarding internship experience as well as an inside look into what it’s like to work for a global financial services organization that has been innovating and serving clients since 1784. We provide a best-in-class experience for our interns with engaging Agile Projects while providing first-hand knowledge of our culture, people, business, and impact within the marketplace. S. T. A. R. T interns will participate in a robust technical experience in Java Script, Python, CSS, Java, and more. From day one, summer analysts are immersed in BNY Mellon’s innovative and dynamic company culture as they will receive Meaningful and challenging work assignments; Networking opportunities with peers, senior leaders and executives; Exposure to different areas of business; Professional Learning Opportunities; Problem-solving, communications, time management, and leadership training Comprehensive professional etiquette and financial services fundamentals training; and Understanding of BNY Mellon’s commitment to diversity and inclusion. Program At A Glance 10-week U. S. summer internship assignment within an identified line of business or division of BNY Mellon* that combines learning with skill development through practical work and projects; Orientation/onboarding and virtual training curriculum; plus Career development and networking support from a host of corporate leaders, senior mentors, peer mentors, business stakeholders and a dedicated program manager. Potential Program Locations US Pittsburgh, PA * New York, NY * Jersey City, NJ Program Highlights Small, selective program size that allows for more personal attention and support; Participants can further develop their technical and business acumen skills/expertise, enhance leadership abilities and build networks across the organization; Robust onboarding and training curriculum designed specifically for summer analysts; Full commitment from top-level management to make our program the premier internship/talent pipeline program within the financial services industry; Opportunity to apply for one of our full-time pipeline leadership programs, including the Emerging Leaders Program ELP , Operations Campus Analyst Program OCAP and Software Engineering & Technology University Program S. E. T. U. P . Post-S. T. A. R. T Opportunities S. T. A. R. T aims to create a pipeline of talent to feed entry-level pipeline program and entry-level full-time position hiring needs. Upon successful completion of the program as well as overall performance and business need, there are potential full-time employment opportunities that our S. T. A. R. T interns may apply for, including the Emerging Leaders Program ELP , Operations Campus Analyst Program OCAP and Software Engineering & Technology University Program S. E. T. U. P and full-time direct hire roles. Data Scientist Primary Responsibilities Participate on a team to apply scientific method to find solutions to real business problems. Perform data analysis, feature engineering and advanced methods to prepare and develop decisions from data. Leverage simple to advanced data techniques to support the team to deliver data analytic products for the firm. Performs analytics in support of the identification and understanding observed business outcomes. Collaborates with others to deliver on hypothesis testing and developing the mathematics to describe the business opportunity. Communicates effectively with analytics staff. Develops analytics, prepares and delivers both informational and decision-seeking presentations. Stays abreast of organization and management changes and has in-depth knowledge of company practices relevant to data science products. Maintains knowledge of company's total computing environment and planned changes in order to develop meaningful data science products. Grow and develop skills across the 3 domain specialties model science, feature science and Insight science capabilities. Stressing expertise in the core functional areas Computer Programming, Math&Analytic Methodology, Distributed computing and communications of complex results. Qualifications Program Eligibility/Qualifications Enrollment in a 4-year undergraduate U. S. degree program with a strong focus on business-related and/or technology-related majors Rising junior or senior U. S. graduating in Dec 2021 or May 2022 Minimum cumulative GPA of 3. 0 or better U. S. Well-rounded and balanced background including demonstrated leadership abilities Strong written and oral communication skills For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments & safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark bnymellon. com/careers. Client Technology Solutions provides our business partners with client-focused, technology-based solutions. These enhance their ability to be successful through world-class software solutions and leading-edge infrastructure. Client Technology Solutions provides employees with the tools and resources to enhance their professional qualifications and careers. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums. Primary Location United States-New York-New York Internal Jobcode 05375 Job Information Technology Organization Office of CIO-HR18027 Requisition Number 2007842"
8,Healthcare Payer Data Scientist Consultant,data scientist,/rc/clk?jk=7ae368e26fcff9af&fccid=a4e4e2eaf26690c9&vjs=3,"We are Applied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence. You are A data maven with serious analytical and statistical chops. You know how to take massive amounts of data and find the insights our clients need to help their companies do more. The work Use statistics, data mining, machine learning, and deep learning techniques to deliver data driven insights for the healthcare industry Work with Accenture teams and clients to understand their challenges and create solutions Be a thought leader for your team on projects and about leading technologies Stay on top of trends in AI Here’s what you need At least 2 years designing and building healthcare data analysis solutions for the business payer or provider industry At least 2 years using new developments in AI, machine learning, cognitive systems, and robotics to build amazing analytical tools At least of 2 years working with tools like SAS, Python, SPSS, R, or SQL At least 2 years working with data integration tools to streamline processes in platforms like Cerner EMR, Apache Spark, Map Reduce, Mongo DB and Couchbase Bonus points if You can use data mining techniques to solve real world business problems You’re fluent in SQL and scripting languages like Python and Perl You know how to use statistical analysis, data visualization and cleaning tools and techniques You’ve designed and built data analysis solutions for a business payer or provider You have a Masters or MBA in statistics, or mathematics Double bonus points if you have a Ph D in analytics, statistics or another quantitative discipline Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture i. e. , H1-B visa, F-1 visa OPT , TN visa or any other non-immigrant status . Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration. Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities. Equal Employment Opportunity All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. Accenture is committed to providing veteran employment opportunities to our service men and women. "
9,Data Scientist,data scientist,/rc/clk?jk=a85e8c19ca219c49&fccid=046813bd6f2871bb&vjs=3,"At White Ops, we are all about keeping it human. We are the global leader in bot mitigation. We protect more than 200 enterprises—including the largest internet platforms—from sophisticated bots by verifying the humanity of nearly one trillion online interactions every week. The most sophisticated bots look and act like humans when they click on ads, visit websites, fill out forms, take over accounts, and commit payment fraud. We stop them. Founded in 2012 in a Brooklyn sci-fi bookstore, our Bot Mitigation Platform protects enterprises from the sophisticated bots that threaten them. It's an ongoing war that we fight passionately every day. Join our mission to stop bots, disrupt the economics of cyber crime, and keep it human. The Data Science team at White Ops is building the algorithms that run behind the scenes and power our bot detection engine. We deploy state-of-the-art statistical and machine learning models at a massive scale that distinguish human activity from automated bots. What you'll do Develop algorithms i. e. rule based, statistical, machine learning that automatically detect emerging threats Evaluate and implement data analysis strategies to improve detection efficacy at internet-scale Ad hoc investigations to respond to emerging threats with velocity and quality Collaborate with product, engineering, customer success, to define and develop new product features that will delight our customers Research for case studies and white papers on bot detection Contribute to toolkits and platforms that scale detection analytics Who you are You have a strong sense of ownership which drives you to find ways to do things better, faster, and cheaper You look to find new and innovative ways to solve complex problems through rigorous experimentation, all by yourself You are open, transparent and work in tight collaboration with anyone You constantly drive for correctness and performance, in that order You are a data scientist, analyst, engineer and have solved difficult problems to prove it You have proficiency with SQL, Python You have familiarity with tools, libraries and platforms such as Jupyter notebooks, Pandas, Num Py etc Track record of independent, creative problem solving with large amounts of complex data Knowledge of how the Internet works Comfort with ad tech terminology Background in data science for security is a plus Knowledge of how web browsers work is a plus Benefits and Perks Unlimited vacation policy Stock options, 401 k , and commuter benefits Competitive salary and commission structure Medical and dental insurance for all full-time employees Fully paid parental leave Professional development fund Great coaching from senior leaders and challenging development opportunities Life at White Ops Our HQ office is located in the heart of New York City. We are growing the company deliberately with a keen eye towards maintaining a culture that values diversity, lifestyle, and career growth. We are doing meaningful work and we need people to join our mighty team. We are proud of our overwhelmingly positive presence on Glassdoor and Built in NYC. We have offices located in NYC, DC, Victoria, and London. "
10,Data Scientist,data scientist,/company/My-Beauty-Matches/jobs/Data-Scientist-15b991e859afd91b?fccid=93021c34e1079d3a&vjs=3,"Role · This is a 3-6 month full-time data scientist contract position with potential to become permanent position based on the performance. · Reporting to the Project Manager, focusing on exploratory data analysis and A/B testing for a live beauty product recommendations engine. · You’ll explore the rich behaviour data collected as people shop using our recommendations and the results from our A/B tests. · You’ll work closely with our technical and product teams to come up with questions and hypotheses about what’s working or could be improved, you’ll help design and interpret future A/B tests, and so you’ll help guide our product roadmap for improving the algorithm. Requirements · You have expertise in exploratory data analysis and visualisation e. g. with SQL, Python/Pandas, R, or similar . · You love using data to answer high-level questions about how people use a product. · MS or Ph D in computer science, data mining, machine learning, statistics, math, engineering, operations research, or other quantitative disciplines· You have 2+ years of experience. · You’re self-sufficient but work well in a team. It would be a bonus if · You have experience with Mongo DB, A/B testing. · You have experience in e-commerce. · You have experience with recommendations algorithms. · You are interested in beauty products. · You can communicate technical ideas, analyses and results clearly. · You have worked in a startup or to deadlines. Benefits · Work remotely from any location with flexible hours. · Opportunity to grow with the company and be offered a permanent role. · Work in one of the hottest Beauty Tech startups, getting hands-on experience and learning more about the beauty industry. · Really make an impact instead of making coffees and photocopies see your ideas come to life without layers of hierarchy. · A very friendly atmosphere. Application We are proud to be an equal opportunities employer, and embrace diversity and flexibility in our work culture. This is intended as a remote role with flexible hours. If this sounds like you, please send us your CV and a cover letter answering the following 1. How soon can you join?2. Do you have a MS or Ph D in computer science, data mining, machine learning, statistics, math, engineering, operations research, or another quantitative disciplines ?3. What is your monthly salary expectation?4. How many years of Python or R experience you have?5. Do you have an e-commerce recommender system experience?6. How many years of experience do you have?Job Types Full-time, Contract Salary $31,372. 00 $88,825. 00 per year Schedule Monday to Friday Experience e-commerce 1 year Required Education Master's Required Work Location Fully Remote"
11,Data Scientist,data scientist,/rc/clk?jk=80c4dd468ffc82c4&fccid=804fc30e8bbb93e8&vjs=3,"Who we are At The Council of State Governments Justice Center, a nonprofit, nonpartisan organization, we develop research-driven strategies to increase public safety and strengthen communities. We combine the power of a membership association, representing state officials in all three branches of government, with the expertise of a policy and research team focused on assisting others to attain measurable results. We help achieve those results by Bringing people together from both sides of the aisle to foster collaboration. Driving the criminal justice field forward with original research. Providing expert assistance to translate the latest research into policy and practice. Building momentum for policy change at the state and local level. What we're doing We are helping state policymakers across the country reduce spending on corrections and reinvest in strategies that reduce recidivism and increase public safety. Through our Justice Reinvestment program, we have helped nearly 30 states use data to unpack what's driving trends in systems of safety and justice. We help policymakers then use that data to design and implement historic reforms that address pressing criminal justice challenges, and result in millions being reinvested in reducing recidivism, improving treatment, and How you'll fit in The Data Scientist will join the Research team to work on an initiative to collect and provide reliable criminal justice system data to improve policymakers' ability to access, understand, compare, and utilize key public safety data. The position will entail collecting and manipulating data; manipulating data; creating and tailoring visualization of interrelated data sets; and presenting research findings. What you'll do Pull, manipulate, structure criminal justice data utilizing tables within different data sources with different products Coordinates research and analytic activities utilizing various data points unstructured and structured and employ programming to clean, massage, and organize the data. Focus on translating data into actionable dashboard using cutting-age visualization tool and technology and present to various audiences. Responsible for providing data visualizations reports and dashboards to support stakeholder requirements Analyze existing reports to improve performance and developing new reports using best data visualization practices and processes Work with multiple complex data sources Work with a wide range of stakeholders and functional teams along with policy analysts to develop data visualizations that provide insights for government Implement processes that improve and lead to greater data quality What you'll bring Knowledge criminal justice systems and settings, public policy, or government MA in Data Visualization, Computer Science, Business Analytics, Statistics, or related technical field is preferred. 3 years of experience using data for social impact, including working with nonprofit, government, or similar. Knowledge of scientific research methods, principles, and techniques. Knowledge of statistical concepts, methods, and application to data analysis. Demonstrated experience applying data science methods to real-world data problems Experience utilizing visualization tools to take advantage of the growing volume of available information For this position, the ideal candidate should have some or all of the following qualifications Research, writing, and editing skills; Oral communication skills, including the ability to effectively present information in a concise way; Excellent interpersonal skills, including maturity, keen judgment, and self-confidence, with a sense of humor and ability to maintain balance and perspective; Ability to work with and find consensus among people from a wide array of backgrounds and perspectives, as well as an appreciation for the need to address issues in nonpartisan and non-polarizing ways; Efficient, organized work style; Expertise in relational database design and concepts; Hand-on experience with SQL and statistical software packages Python, R, STATA Experience Very good communication, interpersonal and critical thinking skills are required. Knowledge and experience with the following tools proficiency in all is not required SQL Python STATA R Education Masters degree or equivalent in the field of Data Visualization, Computer Sciences, Business Analytics, Statistics, Mathematics, or related field. Who you'll join The CSG Justice Center is one of the most robust criminal justice organizations in the country, comprised of approximately 100 employees with decades of experience in law enforcement, community corrections, court administration, housing, mental health and addiction services, state prisons, local jails, juvenile justice, education, workforce development and victim advocacy. Staff here have served governors, state legislators and members of Congress on both sides of the political spectrum. What bonds us together is a shared commitment to our mission, to a common set of values, and to the national, bipartisan advisory board that guides our work. Learn more about careersat the CSG Justice Center. How you apply If you're interested in helping us drive change across the country, you should upload the following elements with your application Cover letter with salary expectations Resume At least three professional references"
12,"Data Scientist, Survey Specialist",data scientist,/rc/clk?jk=7b669f86909f4b4d&fccid=b386980fa584d9b2&vjs=3,"ABOUT THE JOB The ACLU seeks a full-time position of Data Scientist in the Data and Analytics Department of the ACLU's National office in New York, NY. ACLU Analytics partners with teams across the organization to enable the ACLU to make smart, evidence-based decisions and bring quantitative insights on our issues to the courtroom and the public. Our team's work ranges from social science research for litigation & advocacy, to analysis & reporting for fundraising and engagement, to building and maintaining our data infrastructure. We strive to ensure the ACLU leads by example in the ethical use of data and technology. This includes maintaining our privacy and security standards, pushing for transparent data practices from government and corporate actors, and helping to steward high standards for algorithmic fairness, accountability, and transparency. Reporting to the Director of Engagement Analytics, the Data Scientist will be focused on the use of innovative reporting and advanced analytics to transform the way the ACLU approaches quantitative opinion and messaging research. The Data Scientist will also use data science methods to inform how the ACLU engages with its audiences including donors, advocates / organizers, and the general public on our issues and brand. This includes survey and opinion research design and methodology, survey and polling execution in partnership with vendors, in-depth statistical analysis, building models, and presentation of findings to stakeholders. RESPONSIBILITIES You will be part of the Engagement Analytics team, and work with team members across Analytics as well as stakeholders in our Fundraising, Political Advocacy, Digital, and Communications teams. You will also mentor other Reporting Analysts and Data Scientists on the Analytics team. Below is a sampling of projects you can expect to dive into Launch surveys and message tests with the objective of using research findings to craft brand messaging guidance for various post-2020 election scenarios Design and advise on issue-based messaging research and survey instruments for a variety of issues relevant to the ACLU, including voting rights, LGBTQ+ rights, and criminal justice reform Analyze survey, brand tracking, and other quantitative opinion research results to determine attributes about the ACLU's current donor base and potential expansion audiences Handle technical aspects of survey design, including identifying relevant survey universes and appropriate sampling frames, develop innovative sampling designs and statistical analysis procedures for complex projects and proposals, performing sample size and power calculations for complex sample surveys, and performing nonresponse bias analysis and adjustments for missing data MINIMUM QUALIFICATIONS Deep knowledge of quantitative survey methods, including sampling design, nonresponse adjustment procedures, and analysis of data from complex sample surveys Experience designing modern multi-modal surveys and evaluating scripts Fluency in Python or R, or similar analytics and programming languages Familiarity with survey experiments and causal inference Hands-on project experience with statistical methods, machine learning techniques, testing, and/or predictive analysis in service of real-world business challenges Excellent communication skills; ability to synthesize quantitative results for a wide range of non-technical stakeholders across various functions and levels of an organization Commitment to the mission of the ACLU Demonstrate a commitment to diversity within the office using a personal approach that values all individuals and respects differences in regards to race, ethnicity, age, gender identity and expression, sexual orientation, religion, disability and socio-economic circumstance Commitment to work collaboratively and respectfully toward resolving obstacles and/or conflicts PREFERRED QUALIFICATIONS Graduate degree in a relevant field statistics, demography, economics, psychology, social sciences, sociology, policy analysis, survey methodology, biostatistics and 2-3 years of experience OR 5+ years of experience executing market research or public opinion and survey research projects in an academic, political campaign, non-profit, or business setting Strong data visualization skills Experience productionizing models or analysis for long-term usage Proficiency in command line tools A strong foundation in relational databases and database query languages e. g. SQL Familiarity with opinion research in a field relevant to the ACLU's goals and mission, such as reproductive freedom, criminal justice, and voting rights Experience managing research projects end-to-end, including project, vendor, and stakeholder management Experience in political polling Relationships with and knowledge of major opinion research and survey vendors Experience working with the voter file and voter mobilization tools & tactics ABOUT THE ACLU For 100 years, the ACLU has worked to defend and preserve the individual rights and liberties guaranteed by the Constitution and laws of the United States. Whether it's ending mass incarceration, achieving full equality for the LGBT community, establishing new privacy protections for our digital age, or preserving the right to vote or the right to have an abortion, the ACLU takes up the toughest civil liberties cases and issues to defend all people from government abuse and overreach. The Department of Education has determined that employment in this position at the ACLU does not qualify for the Public Service Loan Forgiveness Program. The ACLU is an equal opportunity employer. We value a diverse workforce and an inclusive culture. The ACLU encourages applications from all qualified individuals without regard to race, color, religion, gender, sexual orientation, gender identity or expression, age, national origin, marital status, citizenship, disability, veteran status and record of arrest or conviction. The ACLU undertakes affirmative action strategies in its recruitment and employment efforts to assure that all qualified persons, including persons with disabilities, have full opportunities for employment in all positions. The ACLU is committed to providing reasonable accommodation to individuals with disabilities. If you are a qualified individual with a disability and need assistance applying online, please e-mail benefits. hrdept@aclu. org. If you are selected for an interview, you will receive additional information regarding how to request an accommodation for the interview process. "
13,"Data Scientist, Survey Specialist",data scientist,/rc/clk?jk=9910ec56dada016e&fccid=b386980fa584d9b2&vjs=3,"Title Data Scientist Office National Offices Location New York, NY Apply for this job Job postings are hosted on Greenhouse. See their privacy policy. ABOUT THE JOB The ACLU seeks a full-time position of Data Scientist in the Data and Analytics Department of the ACLU’s National office in New York, NY. ACLU Analytics partners with teams across the organization to enable the ACLU to make smart, evidence-based decisions and bring quantitative insights on our issues to the courtroom and the public. Our team's work ranges from social science research for litigation & advocacy, to analysis & reporting for fundraising and engagement, to building and maintaining our data infrastructure. We strive to ensure the ACLU leads by example in the ethical use of data and technology. This includes maintaining our privacy and security standards, pushing for transparent data practices from government and corporate actors, and helping to steward high standards for algorithmic fairness, accountability, and transparency. Reporting to the Director of Engagement Analytics, the Data Scientist will be focused on the use of innovative reporting and advanced analytics to transform the way the ACLU approaches quantitative opinion and messaging research. The Data Scientist will also use data science methods to inform how the ACLU engages with its audiences including donors, advocates / organizers, and the general public on our issues and brand. This includes survey and opinion research design and methodology, survey and polling execution in partnership with vendors, in-depth statistical analysis, building models, and presentation of findings to stakeholders. RESPONSIBILITIES You will be part of the Engagement Analytics team, and work with team members across Analytics as well as stakeholders in our Fundraising, Political Advocacy, Digital, and Communications teams. You will also mentor other Reporting Analysts and Data Scientists on the Analytics team. Below is a sampling of projects you can expect to dive into Launch surveys and message tests with the objective of using research findings to craft brand messaging guidance for various post-2020 election scenarios Design and advise on issue-based messaging research and survey instruments for a variety of issues relevant to the ACLU, including voting rights, LGBTQ+ rights, and criminal justice reform Analyze survey, brand tracking, and other quantitative opinion research results to determine attributes about the ACLU’s current donor base and potential expansion audiences Handle technical aspects of survey design, including identifying relevant survey universes and appropriate sampling frames, develop innovative sampling designs and statistical analysis procedures for complex projects and proposals, performing sample size and power calculations for complex sample surveys, and performing nonresponse bias analysis and adjustments for missing data MINIMUM QUALIFICATIONS Deep knowledge of quantitative survey methods, including sampling design, nonresponse adjustment procedures, and analysis of data from complex sample surveys Experience designing modern multi-modal surveys and evaluating scripts Fluency in Python or R, or similar analytics and programming languages Familiarity with survey experiments and causal inference Hands-on project experience with statistical methods, machine learning techniques, testing, and/or predictive analysis in service of real-world business challenges Excellent communication skills; ability to synthesize quantitative results for a wide range of non-technical stakeholders across various functions and levels of an organization Commitment to the mission of the ACLU Demonstrate a commitment to diversity within the office using a personal approach that values all individuals and respects differences in regards to race, ethnicity, age, gender identity and expression, sexual orientation, religion, disability and socio-economic circumstance Commitment to work collaboratively and respectfully toward resolving obstacles and/or conflicts PREFERRED QUALIFICATIONS Graduate degree in a relevant field statistics, demography, economics, psychology, social sciences, sociology, policy analysis, survey methodology, biostatistics and 2-3 years of experience OR 5+ years of experience executing market research or public opinion and survey research projects in an academic, political campaign, non-profit, or business setting Strong data visualization skills Experience productionizing models or analysis for long-term usage Proficiency in command line tools A strong foundation in relational databases and database query languages e. g. SQL Familiarity with opinion research in a field relevant to the ACLU’s goals and mission, such as reproductive freedom, criminal justice, and voting rights Experience managing research projects end-to-end, including project, vendor, and stakeholder management Experience in political polling Relationships with and knowledge of major opinion research and survey vendors Experience working with the voter file and voter mobilization tools & tactics ABOUT THE ACLU For 100 years, the ACLU has worked to defend and preserve the individual rights and liberties guaranteed by the Constitution and laws of the United States. Whether it’s ending mass incarceration, achieving full equality for the LGBT community, establishing new privacy protections for our digital age, or preserving the right to vote or the right to have an abortion, the ACLU takes up the toughest civil liberties cases and issues to defend all people from government abuse and overreach. The Department of Education has determined that employment in this position at the ACLU does not qualify for the Public Service Loan Forgiveness Program. The ACLU is an equal opportunity employer. We value a diverse workforce and an inclusive culture. The ACLU encourages applications from all qualified individuals without regard to race, color, religion, gender, sexual orientation, gender identity or expression, age, national origin, marital status, citizenship, disability, veteran status and record of arrest or conviction. The ACLU undertakes affirmative action strategies in its recruitment and employment efforts to assure that all qualified persons, including persons with disabilities, have full opportunities for employment in all positions. The ACLU is committed to providing reasonable accommodation to individuals with disabilities. If you are a qualified individual with a disability and need assistance applying online, please e-mail benefits. hrdept@aclu. org. If you are selected for an interview, you will receive additional information regarding how to request an accommodation for the interview process. "
14,Senior Data Scientist,data scientist,/rc/clk?jk=6476729be9152bee&fccid=fe2d21eef233e94a&vjs=3,"Bachelor's Degree5+ years of experience with data scripting languages e. g SQL, Python, R etc. or statistical/mathematical software e. g. R, SAS, or Matlab 4+ years working as a Data Scientist MS with 5+ years of industry experience or Bachelors with 8+ years of experience in Quantitative field CS, ML, Mathematics, Statistics, Physics Experience in creating data driven visualizations to describe an end-to-end system Highly skilled in data and math/statistical methods i. e. modeling, algorithms Evidence of using of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. in data analysis projects Amazon is building a world class advertising business and defining and delivering a collection of self-service performance advertising products that drive discovery and sales of merchandise. Our products are strategically important to our Retail and Marketplace businesses driving long term growth. We deliver billions of ad impressions and millions of clicks daily and are breaking fresh ground to create truly innovative products. We are highly motivated, collaborative and fun-loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities. We are seeking a Senior Data Scientist to join the Sponsored Products Auction team. This team is responsible for building and evaluating mechanisms to rank and price billions of ad impressions per day. We're looking for a Data Scientist to define measures of auction efficiency and produce insights into the quality of inputs to our auction supply, demand, ML model predictions . This role will also be responsible for more exploratory data analysis to identify opportunities for improvement of the system, working closely with Economists, Applied Scientists and Engineers to improve our product. In this role, you will Perform hands-on data analysis and modeling with very large data sets to develop insights into different aspects of our business monetization, shopper experience, advertiser experience, marketplace dynamics Define and build tools and processes to monitor these metrics weekly business reviews, dashboards, automated alarming Use data to identify opportunities to improve our systems. Work with scientists and economists to model complex interactions in our systems Lead and mentor junior data scientists, with an opportunity to grow into a team leader Be a member of the Amazon-wide Machine Learning Community, participating in internal and external Meetups, Hackathons and Conferences Ph D in Computer Science or related quantitative field Experience with online advertising Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For those with disabilities who would like to request an accommodation, please visit https //www. amazon. jobs/en/disability/us. #sspajobs #adsto"
15,Data Scientist,data scientist,/rc/clk?jk=e97a899366dc892a&fccid=be3b11aa573faee7&vjs=3,"Participates in the development, validation and delivery of algorithms, statistical models and reporting tools. Solves moderately complex analytical problems. Fundamental Components Develops, validates and executes algorithms and predictive models to investigate problems, detect patterns and recommend solutions. Explores, examines and interprets large volumes of data in various forms. Performs analyses of structured and unstructured data to solve moderately complex business problems. utilizing advanced statistical techniques and mathematical analyses. Develops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Uses data visualization techniques to effectively communicate analytical results and support business decisions. Creates and evaluates the data needs of assigned projects and assures the integrity of the data. Explores existing data and recommends additional sources of data for improvements. Documents projects including business objectives, data gathering and processing, detailed set of results and analytical metrics. Background Experience Demonstrates good written and verbal communication skills. Able to present information to various audiences. Effectively resolves problems and roadblocks as they occur. Demonstrates proficiency in several areas of data modeling, machine learning algorithms, statistical analysis, data engineering and data visualization. Ability to work with large data sets from multiple data sources. 3 years of relevant programming or analytic experience. Masters degree preferred. Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
16,Model Monitoring Data Scientist,data scientist,/rc/clk?jk=9427014b6495c959&fccid=b4df24bc350094d0&vjs=3,"We are a Fortune 100 Financial Services firm widely regarded for our diverse culture, training, and career opportunities. We operate at the bleeding edge of our the industry and have invested heavily into growing our Data Science and Machine Learning presence. We are looking for a passionate, entrepreneurial minded Data Scientist to join our team and take ownership of mission critical, high-impact data science products that will be leveraged by every business vertical within the organization. You are someone that thrives in a fast-paced, constantly evolving environment. You are motivated by solving complex problems with high-impact and creative technical solutions. This position will be a key cross-functional collaborator with industry leading Software Engineers, SRE's, Machine Learning Engineers, Data Scientists and Product Managers. Qualifications Education Master’s degree in computer science Hands-on professional experience with statistics or machine learning 3-5 years in a similar role Proficiency across Python, R, and visualization tools Tableau, Shiny, Dash, etc Solid understanding of statistical models and machine learning concepts Experience with databases and writing complex SQL queries Excellent communication skills"
17,Data Scientist,data scientist,/rc/clk?jk=78db02f34eda3015&fccid=4b5d257051285786&vjs=3,"Job Summary We are looking for a Data Scientist to join our Data Governance team. This role requires acute attention to detail, a strong sense of accountability, collaboration skills, and experience deploying production models. In this role you will lead and directly contribute to the success of our data strategy by building and deploying predictive analytical models to detect anomalies and identify outliers to ensure our data is of high quality. Your models and processes will have a direct impact on all data processes and inform holistic usage of data across our organization. At DSS, data is central to measuring all aspects of the business, and critical to its operations and growth. Data Governance works closely with the data engineering, back-end services, front-end technology and embedded analytic teams. The data scientist will work with stakeholders to create mission critical models monitoring backend data ingestions and front-end endpoints to identify and begin remediation of data issues before they reach critical mass; serving as our first line of defense against bad data influencing business decisions. Responsibilities Rapidly prototype, build and deploy production level analytical models to monitor critical datasets via streaming and batch processes Work with streaming data platform teams to deploy models into runtime of our data validation layer Execute on the data governance data science roadmap and projects based on stakeholder needs across the data organization in collaboration with Machine Learning and Innovation & Customer Modeling data capabilities teams Leverage methods from diverse disciplines such as machine learning, deep learning, artificial intelligence, statistical modelling, information theory, information retrieval and other areas to gain data insights, draw conclusions and work with business partners to put those insights into action Serve as a thought leader for the larger data organization to outline best processes as it related to software development lifecycle, unit testing and production level deployment of models Serve as a source of knowledge and delegator for junior data scientists and analysts performing peer code reviews and auditing of production level models and functions Serve as a critical stakeholder across data governance helping to drive model based data instrumentation during dataset implementation with ongoing data quality monitoring and feature engineering as needed. Work as a lead data steward hand-in-hand with the managers of the critical data systems to convey data quality needs related to ingestion errors. Document and implement data quality policies, standards, and procedures for both legacy and new data environments. Basic Qualifications 3+ years of relevant experience in data science, advanced statistics or business analytics including experience with programming languages e. g. Python, Pyspark, SQL . Familiarity with statistical computing packages numpy/scikit-learn, Tensorflow, Keras, Pytorch, Mat Plot Lib High familiarity with data platforms and applications such as Databricks, Jupyter, Snowflake, Redshift, Airflow, Gitlab Experience with distributed computing Hadoop, Spark Exceptional interpersonal skills and written communication skills Strong experience in documenting model methodologies, performance and hypothesis testing Ability to evaluate risks and provide recommendations / solutions in a timely manner Bachelor's degree required in information science, data management, computer science or a related field preferred"
18,Senior Data Scientist - SoFi Invest,data scientist,/rc/clk?jk=bc5c4cfdcb5cca3a&fccid=7c3a1f1f98dde031&vjs=3,"Who we are So Fi is a digital personal finance company whose mission is to help its members achieve financial independence to realize their ambitions, whether that be to buy a house one day, start a family on their own terms or be debt free. We aim to be at the center of our members’ financial lives, and to help every member Get Their Money Right®. By joining So Fi, you’ll become part of a forward-thinking company that is transforming financial services by embracing technology to build innovative loan products, investment tools, and more. One of the fastest growing fintech companies, we’ve grown from 250 employees in 2015 to over 1,500 employees today, and are well on our way to reaching 1 million members. With offices across the US, we offer the excitement of a rapidly growing startup with the stability of a seasoned management team and some of the best talent around. As an employer, we strive to hire employees who are committed to both our company’s mission and our desire to build the best culture in the world. If you are driven, passionate about what you do, and excited about the So Fi mission, we would love to hear from you. The role The Data Science team is looking to add a Senior Data Scientist on the Business Intelligence team who will help shape product development decisions and strategy through data-driven insights. The Senior Data Scientist will be working closely with Business Unit, product, finance partners to build So Fi Invest into a top choice for anyone who wants to invest their money into the market. Success in this role hinges on your technical aptitude, quantitative abilities, and business acumen you know how to plow through data with SQL/Python/R/Tableau, surface insights using math/statistics/ML techniques, and measure the business impact using efficiency/conversion/profit metrics. What you’ll do Work with amazing product and business managers to identify strategic opportunities, measure KP Is to craft compelling stories, make data-driven recommendations, and drive informed actions. Craft, analyze, and present customer behavior metrics, such as funnel conversion, user churn, cart abandonment, and cross-sell metrics. Work with product managers to design tests to improve these metrics. Own end-to-end product analytics workflow including formulating success metrics, socializing them across the organization, and creating dashboards/reports. Write specs and work with engineering to capture new data and transfer data into data warehouse. Act as a curator of data by defining, instrumenting, and tracking necessary analytics of our products by working cross functionally with engineering and product teams. Build self-service systems to enable others in the organization to leverage your work. You stay focused on what’s best for the company – sometimes that requires being flexible, other times it requires being steadfast. What you’ll need 3-5 years of relevant work experience Intellectual curiosity and aptitude to pick up new technical skills Ability to initiate and drive projects to completion with minimal guidance High EQ with ability to influence outcomes and communicate technical content to general audiences Degree in Computer Science, Math, Physics, Engineering or quantitative field SQL, R, Python and Tableau Bachelor’s degree required, Masters preferred. Nice to have Project management expertise Experience in a financial organization Why you’ll love working here Competitive salary packages and bonuses Comprehensive medical, dental, vision and life insurance benefits Generous vacation and holidays Paid parental leave for eligible employees 401 k and education on retirement planning Tuition reimbursement on approved programs Monthly contribution up to $200 to help you pay off your student loans Great health & well-being benefits including telehealth parental support, subsidized gym program Employer paid lunch program except for remote employees Fully stocked kitchen snacks and drinks Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. So Fi provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion including religious dress and grooming practices , sex including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding , gender, gender identity, gender expression, national origin, ancestry, age 40 or over , physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law. "
19,Senior Data Scientist,data scientist,/rc/clk?jk=93bf554467af6caf&fccid=3c371b712ace44cf&vjs=3,"DISH is a Fortune 250 company with more than $13 billion in annual revenue that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U. S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television. Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers. We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story. Opportunity is here. We are DISH. DISH Media Sales, the advertising sales division of DISH Network, provides smart and effective media solutions that complement those of traditional national cable, and is headquartered in New York, with offices in Chicago, Denver and Los Angeles. We are an innovative, passionate and fun group working hard and finding reward in radically changing TV advertising. The Data & Analytics team within Media Sales works closely with the corporate DISH Viewer Measurement team to combine powerful household level reporting data, analytics and expertise that helps our sales staff succeed in a rapidly evolving environment. Constructing automated processes through scripting languages, using data to identify and create models of an advertiser’s target audience, measuring the success of an advertiser’s campaign and providing visualization through self-service tools are just a few of the day to day tasks this team is responsible for. We are now seeking a Data Scientist that will support the on-air addressable Media Sales team and Data Sales efforts through Strong Data Science Stack e. g. R, Python, SQL, producing visualizations in Tableau, etc. Developing critical analytics and propensity models using key DISH data sources including the viewer measurement data and DISH CRM data. Working with the Account Executives and clients to develop client proposals based on conducted research. Demonstrating thought leadership by writing papers, attending conferences and promoting yourself as subject matter expert on topics such subscriber and network viewership trends. Developing key relationship with Analytics heads at advertising agencies, data partners, internal divisions and other key industry players. Architect and develop new advertising products using state of the art machine learning algorithms either in a big data environment such as Hadoop or in a cloud-based environment such as AWS. Oversees training and education of less senior Data Scientists #LI-KP1 A successful Data Scientist will have the following Bachelor’s Degree from a four-year college or university; advanced degree in a STEM field preferred. AWS Certification highly valued. 5 years of working experience utilizing data modeling techniques. Familiarity with big data projects, tools, and technique, segmentation and ad targeting. Experience with SQL and scripting tools; willingness to be hands on where needed. Excellent communication skills and have the proven ability to manage multiple projects and meeting deadlines. Experience working with IT and / or DB As in respect to projects involving large databases. Strong work ethic; personable and must be able to prioritize tasks and deliverables in a fast-paced Sales environment. 52135"
20,"Manager, Data Scientist",data scientist,/rc/clk?jk=82cb25bd85770c2e&fccid=4b0e713ef50478fa&vjs=3,"Explore the possibilities across our global house of brands. Defined by inclusivity rather than exclusivity, Tapestry embraces the exploration of individuality and invests in helping you grow personally and professionally. Every individual in our global house has the opportunity to make an impact, learn and be part of our growing and unique story. At Tapestry, we have the freedom to express ourselves and run with our best ideas across Coach, Kate Spade New York, and Stuart Weitzman. We share a profound belief in both our individual and collective potential, and know that with hard work and dedication, anything is possible. Primary Purpose This Data Scientist will join our ongoing customer personalization and product recommendation initiatives focused on improving the relevancy, response rate, and customer experience across all brands. You will be responsible for building and maintaining relationships with our business partners, developing and maintaining models serving personalized content, and expanding the scope and reach of data-driven customer recommendations. The successful candidate will leverage their experience in Data Science to. . . Collect, clean, integrate, analyze, and visualize internal and external data on customer and product characteristics to identify opportunities and guide strategy Develop customer-level product and promotion recommendations for all Tapestry brands and regions Deploy personalization models as batch and real-time API processes integrating with our internal AWS environment and external partners Partner with various Tapestry business teams including Ecommerce, CRM, and Marketing to identify the best opportunities and the right solutions The accomplished individual will possess. . . Experience developing and deploying machine learning models Python and SQL coding skills 4+ years of experience as a Data Scientist BA/BS in a technical field Computer Science, Statistics, Data Science, etc. An understanding of Agile workflows and product development cycles The outstanding individual will possess. . . A Masters or Ph D in a technical field Expertise in AWS, Azure, and/or GCP Experience working with non-technical teams and business partners Knowledge of retail and an interest in fashion Our Competencies for All Employees Drive for Results Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results. Customer Focus Is dedicated to meeting the expectations and requirements of internal and external customers; gets first-hand customer information and uses it for improvements in products and services; acts with customers in mind; establishes and maintains effective relationships with customers and gains their trust and respect. Creativity Comes up with a lot of new and unique ideas; easily makes connections among previously unrelated notions; tends to be seen as original and value-added in brainstorming settings. Interpersonal Savvy Relates well to all kinds of people, up, down, and sideways, inside and outside the organization; builds appropriate rapport; builds constructive and effective relationships; uses diplomacy and tact; can diffuse even high-tension situations comfortably. Learning on the Fly Learns quickly when facing new problems; a relentless and versatile learner; open to change; analyzes both successes and failures for clues to improvement; experiments and will try anything to find solutions; enjoys the challenge of unfamiliar tasks; quickly grasps the essence and the underlying structure of anything. Perseverance Pursues everything with energy, drive, and a need to finish; seldom gives up before finishing, especially in the face of resistance or setbacks. Dealing with Ambiguity Can effectively cope with change; can shift gears comfortably; can decide and act without having the total picture; isn’t upset when things are up in the air; doesn’t have to finish things before moving on; can comfortably handle risk and uncertainty. Our Competencies for All People Managers Strategic Agility Sees ahead clearly; can anticipate future consequences and trends accurately; has broad knowledge and perspective; is future oriented; can articulately paint credible pictures and visions of possibilities and likelihoods; can create competitive and breakthrough strategies and plans. Building Effective Teams Blends people into teams when needed; creates strong morale and spirit in their team; shares wins and successes; fosters open dialogue; lets people finish and be responsible for their work; defines success in terms of the whole team; creates a feeling of belonging in the team. Managerial Courage Doesn’t hold back anything that needs to be said; provides current, direct, complete, and “actionable” positive and corrective feedback to others; lets people know where they stand; faces up to people problems on any person or situation not including direct reports quickly and directly; is not afraid to take negative action when necessary. Tapestry, Inc. is an equal opportunity and affirmative action employer and we pride ourselves on hiring and developing the best people. All employment decisions including recruitment, hiring, promotion, compensation, transfer, training, discipline and termination are based on the applicant’s or employee’s qualifications as they relate to the requirements of the position under consideration. These decisions are made without regard to age, sex, sexual orientation, gender identity, genetic characteristics, race, color, creed, religion, ethnicity, national origin, alienage, citizenship, disability, marital status, military status, pregnancy, or any other legally-recognized protected basis prohibited by applicable law. #LI-JV1 Visit Tapestry, Inc. at http //www. tapestry. com/"
21,"Data Scientist, Messaging",data scientist,/rc/clk?jk=479c653217e596f2&fccid=fe404d18bb9eef1e&vjs=3,"Music for everyone, no credit card needed. It’s a promise our platform was built on. And here in the Free team, we’re still building on it. We work on a massive scale, bringing people from all kinds of disciplines and all parts of the business together to deliver an amazing freemium experience to more than 129 million users worldwide. Want to help shape the future of free? Here’s how. Spotify’s Free Mission is the home of the team responsible for the success of Spotify’s freemium business model that allows us to deliver an amazing free experience to more than 141 million+ users worldwide and deliver user-centered messaging to all 248 million+ users, both Free and Premium. As a Data Scientist, Messaging, your mission is to turn terabytes of data into insights, and get a deep understanding of the world of music and listeners. Together with us you will study user behavior, evaluate strategic initiatives and experiment with new features. Above all, your work will impact the way the world experiences music. We are looking for a Data Scientist, to help us in the following areas How can we drive a better Messaging user experience? How do users interact with Spotify messages Email, In App, Push ?How do we build support for new messaging channels that serve marketing needs while improving the listener experience?How do we engage our audiences with relevant and personalized messages through the right channel at the right time? How do we ensure that our listeners find value in our messaging channels and stay opted in?How can we build a cohesive experience for our users across the messaging and ads landscape? How do we accurately measure the impact & effectiveness of messaging promotions? What you’ll do Work closely with cross-functional teams of data scientists, user researchers, product owners, designers and engineers who are passionate about Spotify’s success Perform analyses on large sets of data to extract actionable insights that will help drive product decisions Impact product strategy by leading foundational research & experimentation initiatives to develop a deeper understanding of how product, platforms & promotions influence user behavior Mentor and coach other data scientists Work from our offices in New York Who you are You know how to understand and solve loosely defined problems and come up with relevant answers and actionable insights to guide product development decisions You have a deep understanding of numbers, as well as a strong business sense You have 2+ years of relevant experience, with a degree in economics, psychology, computer science, statistics, or mathematics or another quantitative discipline You have the technical competence to perform advanced analytics coding skills such as Python, Java, or Scala , experience with analytics & visualization tools Pandas, R, SPSS, SQL, Hadoop, Tableau and experience performing analysis with large datasets You possess statistical competence such as regression modeling, a/b testing, significance testing etc It is a plus if you Have demonstrated experience with hands-on statistical modeling and possess knowledge about machine learning such as predictive modeling, decision trees, classification models, clustering techniques Worked in an analytics role within an marketing/messaging function You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 286 million users. "
22,eCom Senior Data Scientist,data scientist,/rc/clk?jk=293b77d06db5fffc&fccid=2973259ddc967948&vjs=3,"Auto req ID 213982BR Job Description Pepsi Co operates in an environment undergoing immense and rapid change, driven by e Commerce and emergent retail technologies. To ensure continued success in the food and beverage space, Pepsi Co has assembled a dedicated e Commerce team – tasked with optimizing e Commerce operations and developing innovations that will give Pepsi Co a sustainable competitive advantage. While tied closely to broader Pepsi Co, the e Commerce group more closely resembles a start-up environment; embracing the core values of having a bias for action, being results-oriented, maintaining a community-focus, and prioritizing people. Pepsi Co’s Data Science and Analytics group is a team of data scientists, technology specialists, and business innovators who operate within e Commerce to build industry-leading systems and solutions. By focusing on machine learning and automation, the Data Science & Analytics group is pushing the bounds of possibility for Pepsi Co and its strategic partners. What Pepsi Co Data Science & Analytics does Build machine learning systems to understand the cross-channel grocery ecosystem Perform statistical analysis across diverse datasets to drive and measure performance Work with Pepsi Co’s strategic partners to expand their technical capabilities, thereby creating a more robust data environment Utilize natural language understanding techniques to uncover insights from contextual data Develop scalable tools to drive automation and optimize business operations As a Senior Data Scientist, you will play a critical role in executing the global e Commerce growth agenda. You will be tasked with identifying, designing, and implementing data science/machine learning solutions to business problems. You will collaborate with the larger data science and analytics teams to create a robust, shared codebase; on which Pepsi Co will build automated e Commerce systems. You will work with internal business stakeholders and strategic partners to identify opportunities for collaborative development and foster a data-driven culture between relevant teams. Responsibilities Work with the larger data science team to analyze large data sets and develop custom models/algorithms to uncover trends, patterns and insights Write clean, organized machine learning code using standard software engineering methodologies Provide critical thought leadership to enhance organizational capabilities by utilizing big and small data; work with DSA team members to identify and execute on opportunities for enhancement Qualifications/Requirements Master’s Degree in Data Science or equivalent Development experience in Python, Java, C or equivalent programming language Experience and or familiarity Num Py, Pandas, Pytorch, Postgres SQL, MSSQL, My SQL Experience using Tensorflow to design custom deep learning systems Expert in mathematical models underlying data science methods, understanding the trade-offs between different solutions Experience in designing systems to solve business problems Demonstrated ability to effectively and concisely communicate with both business and technical audiences Relocation Eligible Not Applicable Job Type Regular All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. Pepsi Co is an Equal Opportunity Employer Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance. If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View Pepsi Co EEO Policy Please view our Pay Transparency Statement"
23,Bayesian Data Scientist QED,data scientist,/rc/clk?jk=5ab611d4c5d45fd2&fccid=1c76c3a36f6c7557&vjs=3,"Your role Do you love to use data to make decisions? Are you excited by the prospect of using Bayesian methods to make business decisions? We are looking for an entrepreneurial self-starter to join UBS Asset Management's Quantitative Evidence and Data Science QED Team in Europe or the US. This is a unique opportunity to shape the future of investing and scale our team's success into our client base. Your main goal as a Data Scientist is to build great models that can be used in scalable investment decision workflows. Specifically you will work with other technical specialists to build ETL and model pipelines build scalable model libraries customized to investment use cases work with the QED analysts to build solutions to Investment questions that span the entire data science workflow ETL, Modeling, Visualization provide model expertise and advice to teamwide modeling activities Your Career Comeback We are open to applications from career returners. Find out more about our program on ubs. com/careercomeback. Your team You will be working in the Quantitative Evidence and Data Science QED team based out of the US or UK. The team operates within UBS Asset Management division. Reporting into the divisional Head of Investments, we work with investment teams globally across all asset classes to leverage data and data science in the investment process in order to create better outcomes for our clients. Our diverse team composition, which combines investment, client-facing and data science skills, has allowed us to rapidly drive the adoption of data science in the investment process. Your expertise You are knowledgeable in SQL and Py Spark is a plus focused and eager for learning and self-improvement open for career comeback applicants with flexible work schedules to accommodate family and cultural commitments open for remote work in European or US Time Zones You have ideally 2+ years experience in a data related role track record of building decision pipelines using data science and statistics innovative and entrepreneurial spirit experience using Bayesian Inference o to inform decision making o to build Cross-Sectional and Time Series models o in the context of Natural Language Processing or Generation extensive Python experience in the Pydata stack Pandas, Numpy, Matplotlib expertise in Bayesian and Probabilistic Programming librairies Stan, Py MC3, Arviz, FB Prophet Ph D or Masters in applied technical field with experience using Bayesian Statistics in that field LI-UBS Your colleagues About us Expert advice. Wealth management. Investment banking. Asset management. Retail banking in Switzerland. And all the support functions. That's what we do. And we do it for private and institutional clients as well as corporations around the world. We are about 60,000 employees in all major financial centers, in more than 50 countries. Do you want to be one of us? Join us We're a truly global, collaborative and friendly group of people. Having a diverse, inclusive and respectful workplace is important to us. And we support your career development, internal mobility and work-life balance. If this sounds interesting, apply now. Disclaimer / Policy Statements UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce. Apply now Save"
24,Data Scientist/ML Engineer,data scientist,/rc/clk?jk=15e9b5772430624a&fccid=cf94a2aa6eb7d672&vjs=3,"Become part of the team Do you crave a collaborative organization where your contributions will make a strong impact? Do you want to develop products in the latest cloud-based technologies building ecosystems rather than creating client-facing slides? Are you ready to roll up your sleeves and embrace a work culture that’s insanely passionate and committed to bringing the latest advanced analytics to life? The Data Science & AI group at PA Consulting is your dream community. As part of the fastest growing innovation practice within PA Consulting, you will work with the latest advanced analytics, machine learning, and big data technologies to generate actionable insights from data and develop innovative data products. We focus on Life Science, Healthcare, Energy & Utilities and CPG sectors and work with various data sets, from social media to public health data. Our domain focus is broad and covers everything from computer vision and NLP, recommender engines, classification and clustering algorithms, linear programming and optimization. What we're looking for 2-5 years' professional experience as a data scientist, software engineer or statistical modeler Master's degree from top tier university in Computer Science, Statistics, Economics, Physics, Engineering, Mathematics or similar field Expertise in Machine Learning algorithms and methods Strong understanding and application of statistical methods Experience writing production level code in one of the following Python, Java, C++, C Experience with database systems preferred Experience working with big data distributed programming languages and ecosystems e. g. S3, EC2, Hadoop/Map Reduce, Pig, Hive, Spark Experience building scalable data pipelines with data/feature engineering Experience Webscraping leveraging Beaituflsoup, Selenium, Scrappy etc preferred Experience with front end UI , HTML5, Java Script, CSS, R Shiny, Tableau preferred Benefits Group medical insurance Health Savings Account with company match Teladoc and informed Nurse line resources Long term care plan Group dental insurance Vision plan 401 k Savings Plan with company profit sharing contribution Commuter and Parking tax-savings benefit 15 days paid vacation days with the opportunity to buy five additional days 10 paid Holidays plus 10 paid sick days Company and Voluntary income protection benefits Gym and health incentive reimbursement Pet and legal insurance Plans Employee Assistance Plan Annual performance-based bonus About us We believe in the power of ingenuity to build a positive human future in a technology-driven world. As strategies, technologies and innovation collide, we create opportunity from complexity. Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results. An innovation and transformation consultancy, we are over 3,200 specialists in consumer, defence and security, energy and utilities, financial services, government, health and life sciences, manufacturing, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists. We operate globally from offices across the UK, US, Europe, and the Nordics. PA. Bringing Ingenuity to Life. Inclusion & Diversity We believe that diversity makes us a stronger firm and look to employ people with different ideas, styles and skill sets. This diversity stimulates a rich, creative environment – one in which our people develop, and our clients enjoy enduring results. We’re committed to recruiting, promoting and rewarding our people solely based on their ability to contribute to PA’s goals, without regard to their sex, race, disability, religion, national origin, ethnicity, sexual orientation, age or marital status. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. VEVRAA Federal Contractor. Should you need any adjustments to the recruitment process, at either application or interview, please contact us on recruitmentenquiries@paconsulting. com"
25,"Senior Data Scientist, Analytics",data scientist,/rc/clk?jk=87d96daea46a7905&fccid=9ecb91618c39a24f&vjs=3,"Company Description As an Etsy employee, you can do the work you love, be yourself, and make an impact in the lives of millions. Our commitments to diversity and inclusion, team culture and the spaces where we work all reflect our mission to keep commerce human. Job Description The Product Analytics team is charged with driving the success of product efforts at Etsy by providing actionable insights, industry-leading measurement techniques, and access to the metrics, data, and tools needed for decision-making. We partner with cross-functional peers through all stages of development identifying initial opportunities, refining the user experience, analyzing the impact of our efforts, and highlighting improvement areas. Ultimately, the team’s work strengthens Etsy and helps continuously improve the Etsy experience! Learning new skills and techniques is not only a requirement but a perk of the job! We are always looking for opportunities to grow. Our mission is to guide product with data-driven insights, telling the story of how we improve the experience for our users to teams, to senior management, and to the community. This role is located in our Brooklyn, NY and part of our Analytics and Strategic Finance team. About the Role You will partner with our Buyer-focused Product team to achieve results-oriented goals around growth and experience Play a meaningful role in deciding how we can best engage and grow Etsy users through analysis of behavioral and transactional data Design and analyze rigorous experiments, help teams set great hypotheses, and deliver robust analysis of experiment results Transform raw data into meaningful and impactful analysis characterized by strong data governance, technique transparency, and clear documentation Raise the skill level of the entire analytics team through the creation of outstanding work, mentorship, and the introduction of better practices, processes, and tools Qualifications About You 4 plus years experience as a data scientist or data analyst in which you have extracted meaning from large data sets with little engineering support Guided product teams to identify high-impact opportunities, set meaningful goals against key outcomes, and driven decisions Mastery of SQL, and experience with R/Python and other scripting/automation techniques; bonus points for experience with a Big Data ecosystem and Looker, Tableau, or other data visualization software You love trying new analytical approaches to answer nuanced questions and advancing your skill set is important to you. You are an expert in sophisticated analyses and regularly use statistical methods in your work. Deep understanding of experimentation theory and practice You distill highly complex problems into narratives that are concise, actionable, and memorable, and express your recommendations with both conviction and finesse Passionate about reproducible work and have picked up tools to this end Acted as a mentor, guiding others to make their own work better and more efficient Additional Information At Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided a reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. While Etsy supports visa sponsorship, sponsorship opportunities may be limited to certain roles and skillsets. "
26,Senior Data Scientist,data scientist,/rc/clk?jk=0a31e553c2404f39&fccid=353eb997fc901045&vjs=3,"Crossix is a health-focused technology company dedicated to advancing healthcare marketing with analytics and innovative planning, targeting, measurement, and optimization solutions. Positioned at the center of big data, innovative technology, and multichannel media, Crossix, a Veeva Company, provides our clients with insights to help make strategic business decisions and drive improved patient outcomes. Crossix knows that our employees are integral to our success, which is why we have created an inclusive culture where everyone can thrive. Crossix is headquartered in midtown Manhattan with opportunities to work in our NYC office and other locations around the country. Along with competitive salaries and benefits, we invest in opportunities for professional development and career growth, and provide other amenities like a beautiful rooftop, team bonding activities, etc. Crossix is seeking intellectually curious, resourceful, and collaborative Senior Data Scientists to join the Veeva Data Cloud team. This is an excellent opportunity to help us build the technology and data science products that power Veeva Data Cloud and be at the forefront of innovation in the healthcare technology space. The team is guided by its core values as it works to solve the most challenging problems in healthcare data and analytics Singular Focus Speed Humility Ownership Challenge What You'll Do Apply machine learning, data mining, and statistical analysis techniques to large health data sets to build new products and methodologies Own responsibilities related to project and people leadership, including leading data scientists and analysts Collaborate closely with a team of data scientists, product managers, and executives to discover and deliver product offerings from prototype to massive scale Explore and find meaning in high volumes of data to evaluate data quality and extract actionable insights that will help drive business decisions; execute data querying, data cleansing, and experiment design Rapidly build prototype product solutions, communicate findings, and iterate Draw from prior experience and technical expertise to identify product improvements and inform testing plans; break overall objectives down into underlying problems that can be prioritized and solved Work with product and engineering teams to improve and implement methods and features Master core parts of the Crossix technology platform. Technologies include Spark, SQL, Python, R, AWS, and proprietary data mining software What You've Done 7+ years of hands-on data science experience, demonstrating increasing responsibility and impact over time, including experience leading projects Advanced and in-depth knowledge and professional experience in machine learning and statistics Highly proficient in Python and SQL; experience working with AWS preferred Strong organizational, management and leadership skills Strong communication skills and agility to work across internal teams Who You Are Have a desire and preference for working in a fast-paced, entrepreneurial environment Enjoy having clear ownership of a goal even if the path to get there is not entirely clear Have a curiosity to figure out new problems Are humble and truly think about the success of the group before your own contribution Are comfortable challenging existing norms, thinking and teammates, always doing so respectfully Perks & Benefits Flexible PTO Allocations for continuous learning & development Health & wellness programs If this role and our exciting company culture seem appealing to you, please apply! We want to continue to grow our diverse team of hardworking and humble people who are passionate about their work. We hope that’s you!"
27,Principal Data Scientist,data scientist,/rc/clk?jk=f64ff5d1dab97897&fccid=353eb997fc901045&vjs=3,"Crossix is a health-focused technology company dedicated to advancing healthcare marketing with analytics and innovative planning, targeting, measurement, and optimization solutions. Positioned at the center of big data, innovative technology, and multichannel media, Crossix, a Veeva Company, provides our clients with insights to help make strategic business decisions and drive improved patient outcomes. Crossix knows that our employees are integral to our success, which is why we have created an inclusive culture where everyone can thrive. Crossix is headquartered in midtown Manhattan with opportunities to work in our NYC office and other locations around the country. Along with competitive salaries and benefits, we invest in opportunities for professional development and career growth, and provide other amenities like a beautiful rooftop, team bonding activities, etc. Crossix is seeking intellectually curious, resourceful, and collaborative Principal Data Scientists to join the Veeva Data Cloud team. This is an excellent opportunity to help us build the technology and data science products that power Veeva Data Cloud and be at the forefront of innovation in the healthcare technology space. The team is guided by its core values as it works to solve the most challenging problems in healthcare data and analytics Singular Focus Speed Humility Ownership Challenge What You'll Do Apply machine learning, data mining, and statistical analysis techniques to large health data sets to build new products and methodologies Own responsibilities related to project and people leadership, including leading data scientists and analysts Collaborate closely with a team of data scientists, product managers, and executives to discover and deliver product offerings from prototype to massive scale Explore and find meaning in high volumes of data to evaluate data quality and extract actionable insights that will help drive business decisions; execute data querying, data cleansing, and experiment design Rapidly build prototype product solutions, communicate findings, and iterate Draw from prior experience and technical expertise to identify product improvements and inform testing plans; break overall objectives down into underlying problems that can be prioritized and solved Work with product and engineering teams to improve and implement methods and features Master core parts of the Crossix technology platform. Technologies include Spark, SQL, Python, R, AWS, and proprietary data mining software What You've Done 10+ years of hands-on data science experience, demonstrating increasing responsibility and impact over time, including experience leading projects Extensive and in-depth knowledge and professional experience in machine learning and statistics Highly proficient in Python and SQL; experience working with AWS preferred Strong organizational, management and leadership skills Strong communication skills and agility to work across internal teams Who You Are Have a desire and preference for working in a fast-paced, entrepreneurial environment Enjoy having clear ownership of a goal even if the path to get there is not entirely clear Have a curiosity to figure out new problems Are humble and truly think about the success of the group before your own contribution Are comfortable challenging existing norms, thinking and teammates, always doing so respectfully Perks & Benefits Flexible PTO Allocations for continuous learning & development Health & wellness programs If this role and our exciting company culture seem appealing to you, please apply! We want to continue to grow our diverse team of hardworking and humble people who are passionate about their work. We hope that’s you!"
28,"Lead Data Scientist, Pricing",data scientist,/rc/clk?jk=c1d25a3a897ae831&fccid=6df24d87c9f05a07&vjs=3,"Overview You’ll explore new business opportunities and partner with our most critical lines of business to find new ways to drive sales growth, and improve gross margin for the company. As part of the Pricing – Data Science Group, the Lead Data Scientist will develop innovative analytical approaches to inform and execute pricing strategies. The Lead Data Scientist will create practical and scalable methods to forecast customer demand, recommend pricing actions, simulate business outcomes, and optimize major investments. The Data Scientist will work closely with cross functional groups to enhance analytical capabilities, identify new growth opportunities, and develop persuasive interactive visualizations. Perform other duties as assigned. Essential Functions Answer complex business questions by using appropriate statistical techniques on available data or designing and running experiments to gather data. Adapt standard or develop novel applications of classification, forecasting, simulation, optimization, and summarization techniques. Work closely with cross functional teams to encourage best practices for experimental design and data analysis. Create compelling interactive visualizations and presentations to enhance decision making capabilities throughout the company. Regular, dependable attendance and punctuality. Qualifications Education/Experience At least one year of industry experience with Master’s or Ph. D. required in relevant technical field such as Applied Mathematics, Computer Science, Engineering, Physics, or Economics. Experience working with large data sets. Expert knowledge of analytical tools such as SAS, R, Matlab, or STATA and scripting languages such as Python, Perl, or PHP. Communication Skills Excellent written and verbal communication skills. Ability to communicate complex quantitative analyses in a clear, precise, and actionable manner. Mathematical Skills Basic math functions such as addition, subtraction, multiplication, division, and analytical skills. Reasoning Ability Must be able to work independently with minimal supervision and make sound decisions. Physical Demands This position involves regular walking, standing, sitting for extended periods of time, hearing, and talking. May occasionally involve stooping, kneeling, or crouching. May involve close vision, color vision, depth perception, focus adjustment, and viewing computer monitor for extended periods of time. Involves manual dexterity for using keyboard, mouse, and other office equipment. May involve moving or lifting items under 10 pounds. Other Skills Strong passion for answering complex business questions using structured problem solving and rigorous data analysis. Curiosity and desire to learn about business needs and strategies from close collaboration with business partners such as the CRM, Marketing Effectiveness, and Customer Analytics teams. Familiarity with visualization tools such as Tableau, Looker or Spotfire. Familiarity with relational databases and SQL. Work Hours Ability to work a flexible schedule based on department and company needs. This job description is not all inclusive. In addition, Macy's, Inc. reserves the right to amend this job description at any time. Macy’s, Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment. "
29,"Senior Data Scientist, Ad Intelligence",data scientist,/rc/clk?jk=db5c01c653255365&fccid=c659788ec6cc356e&vjs=3,"Who We Are Sensor Tower is the leading solution for mobile marketers, app developers, and industry analysts who demand superior competitive insights into the mobile market economy. We serve independent and Fortune 500 customers alike, spanning the mobile games, travel & hospitality, music, finance, and broadcast entertainment markets. Our suite of products consist of free, “SMB”, and Enterprise-tiered solutions including Store Intelligence, Usage Intelligence, Ad Intelligence, and App Intelligence. We are a privately held company headquartered in San Francisco and was a member of Angel Pad’s startup incubator program in 2013. Our insights are cited by the world’s leading news and finance publications, including the Wall Street Journal, The New York Times, Forbes, Fortune, Bloomberg, CNBC, The Washington Post, and Reuters. Role Summary As a Senior Data Scientist, Ad Intelligence, you will be the expert of all things related to mobile advertising. You will leverage your mobile advertising and mobile industry knowledge to drive strategies to improve the accuracy and offerings of our Ad Intelligence solution. This solution allows one to uncover insights like which app publishers are advertising the most on mobile app campaigns, which creatives are performing best across certain ad networks, and which ad network a publisher is marketing on the most. The Ad Intelligence solution presents complex challenges involving large-scale data >140 billion data records and high load on a daily basis. To learn more about one of these challenges, here's a link to our blog post describing how to speed up the backend with graph theory. You can read more about our industry-leading solution here. Requirements 2+ years of experience in a technical role within the mobile advertising space i. e. experience at mobile attribution companies, mobile ad networks, mobile ad agencies Master's degree or above in mathematics, statistics, or computer science 4+ years applied experience in business intelligence, data mining, analytics, or statistical modeling in technology or mobile industries A passion for data analysis and presentation of data insights Ability to communicate effectively with technical developers and non-technical marketing business partners Extensive programming experience Python, Ruby or a similar language Moderate experience implementing machine learning algorithms Good working knowledge of Mongo DB or similar DB technologies Strong proficiency with one or more statistical visualization or graphing toolkits MS Excel, a Javascript framework like Highcharts or Tableau Extra Credit! Enjoy working in small, fast-paced teams where you can take initiative and accountability, and generate results every day Detail-oriented, organized, and focused on delighting customers Why Join Sensor Tower? We were named one of the 50 Tech Companies to Know in 2020 by Built In After seven years of building Sensor Tower off of $1MM Seed, 2013 , we're excited to announce our $45MM growth investment. Read more about it here, in the words of our co-founders. We have a birds-eye view of the entire mobile app ecosystem, and we keep our teams constantly abreast of the latest mobile app trends, news, and best practices. You align with our Core Values Customer-Focused, Innovative, Continuously Learning, Action-Oriented, Respectful, Data & Metrics-Driven. We grant options to all of our employees because we recognize that everybody plays an integral role in our success; thus all employees should be invested in Sensor Tower both figuratively and literally . We offer unlimited PTO, Health and Wellness stipends, flexible work hours, 401K, team trips white water rafting, Hawaii, and weekend Tahoe mansion trips to name a few , and more Covid-19 Specific benefits Stipend to set up your home office and/or gym, Wi Fi stipend, daily Uber Eats delivery stipend, and virtual team events For more information about our Engineering team and what it is like working with them please visit our team page here. Sensor Tower is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. If you have a disability or special need that requires accommodation, please let us know. "
30,"Lead Data Scientist, Peacock",data scientist,/rc/clk?jk=69af398a110574fa&fccid=35d653c09c2712b6&vjs=3,"Meet Peacock, NBC Universal’s new, wildly entertaining streaming service that combines timeless shows and movies with timely news, sports and pop-culture. We’re growing our team of smart, hungry, and upbeat doers who crave the chance to build something new at the epicenter of content, tech, and culture. We need fearless leaders and pop-culture fiends who work hard and fan hard. Creative problem-solvers who just so happen to be the reigning champs at Parks & Rec trivia night. So if this sounds like you, join our flock. And we promise, we won’t put your stapler in Jell-O. Position Overview As part of the Direct-to-Consumer Decision Sciences team, the Lead Data Scientist will be responsible for creating analytical solutions for one or more verticals of NBCU’s video streaming service including, but not limited to, the recommender system, automated marketing, personalized advertisement, commerce and revenue optimization systems, customer journey and CRM solutions. In this role, the Lead Data Scientist will use advanced data science methodologies including collaborative filtering, deep learning, reinforcement learning, on-line modeling etc. and work closely with business owners, teammates and engineers to build a state-of-the-art real-time video streaming service. Responsibilities include, but are not limited to Lead a group of data scientists in the development of analytical models using statistical, machine learning and data mining methodologies. Advise, help to resolve issues and handle non-standard cases. Define procedures for cleansing, discretization, imputation, selection, generalization etc. to create high quality features for the modeling process. Work with business stakeholders to define business requirements including KPI and acceptance criteria. Use big data, relational and non-relational data sources to access data at the appropriate level of granularity for the needs of specific analytical projects. Maintains up to date knowledge of the relevant data set structures and participate in defining necessary upgrades and modifications. Collaborate with software and data architects in building real-time and automated batch implementations of the data science solutions and integrating them into the streaming service architecture. Drive work on improving the codebase and machine learning lifecycle infrastructure. Qualifications/Requirements Advanced Master or Ph D degree with specialization in Statistics, Computer Science, Data Science, Economics, Mathematics, Operations Research or another quantitative field or equivalent. 5+ years of combined experience in advanced analytics in industry or research. Experience in leading small teams or/and being a lead data scientist on large commercial projects. Deep knowledge of statistical methods and machine learning with special emphasis on the advanced algorithms like neural networks, SVM, random forests, bagging, gradient boosting machines, k-means++, deep learning or reinforcement learning. Expert level in 5+ classes of algorithms. Experience implementing scalable, distributed, and highly available systems using Google Cloud. Experience with data visualization tools and techniques. Understanding of algorithmic complexity of model training and testing, particularly for real-time and near real-time models. Proficient in at least one statistical R, Python and one programming Julia, Java, Scala or similar languages. Strong skills in data processing using SQL and Py Spark. Desired Characteristics Working experience with commercial recommender systems or a lead role in an advanced research recommender system project. Experience with reinforcement learning based systems. Working experience with deep learning, particularly in the areas different form the computer vision. Experience with multi-billion record datasets and leading projects that span the disciplines of data science and data engineering Knowledge of enterprise-level digital analytics platforms e. g. Adobe Analytics, Google Analytics, etc. Experience with television ratings and digital measurement tools Nielsen, Rentrak, Com Score etc. Experience building streaming data pipelines using Kafka, Spark or Flink Experience with large-scale video assets Team oriented and collaborative approach with a demonstrated aptitude and willingness to learn new methods and tools Pride and ownership in your work and confident representation of your team to other parts of NBC Universal Sub-Business Peacock Decision Science Career Level Experienced City New York State/Province New York Country United States About Us At NBC Universal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us. Notices NBC Universal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBC Universal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable. "
31,Lead Data Scientist,data scientist,/rc/clk?jk=314f2bc98d66fde0&fccid=df4d25d87e00f676&vjs=3,"Location New York or Remote Position Overview Deep Labs is seeking an experienced, enthusiastic Lead Data Scientist to help grow the Deep Labs AI team while designing & building solutions for some of our client's most challenging problems. The ideal candidate will have a strong background working with data, experience applying machine learning and deep learning techniques to real-world problems, and a natural curiosity and desire to experiment, evaluate and solve problems as part of a fast-paced and growing, product-driven technology team. The role reports to the VP, Data Science & Engineering. Responsibilities Build regression, classification and clustering models using Machine Learning and/or Deep Learning algorithms and techniques e. g. CNN, LSTM, GAN, transfer learning, reinforcement learning, graph learning Identify opportunities to enrich and integrate data from multiple, diverse sources Build, train and evaluate prototype models, using a tool or framework of choice Work closely with engineering/infrastructure teams to help transition models from development to production Follow a methodical, analytical approach and have a natural curiosity to understand and get the best from data Collaboration across teams communicate effectively across a variety of technical and non-technical audiences Experience Minimum of 7 years of experience in a dedicated data scientist role modeling fraud/risk, in one or more of the following industries finance, payments, banking, risk, fraud, identity or behavioral analytics Strong knowledge and experience of Python preferred or R, and associated data manipulation/analysis libraries Working knowledge of open-source machine/deep learning tools and frameworks e. g. Tensorflow, pytorch, h2o. ai, Databricks, scikit-learn Experience using data mining & descriptive analytics to explore and profile new datasets, perform feature selection and extraction, and develop ethical and interpretable models Knowledge and/or work experience of public cloud environments e. g. Google GCP or Amazon AWS Education Masters or Ph D level qualification in computer science, mathematics, or related discipline Specific Skills Knowledge and experience of good software engineering practices Keen interest and desire to stay abreast of the latest developments in AI research and industry Demonstrated ability to take initiative and interact within a global team environment Strong analytical and quantitative skills; strong attention to detail Why Deep Labs? At Deep Labs there is tremendous potential to learn and grow, while also contributing to policies, decisions, and the direction of the company. We offer competitive compensation and benefits, and provide a highly open, honest, and fun work environment. Our company is committed to equal employment opportunity. We will not discriminate against employees or applicants for employment on any legally-recognized basis ""protected class"" including, but not limited to age, race, gender, pregnancy, religion or creed, color, national origin, sexual orientation, disability, genetic characteristics, military or veteran status, uniform service member status or any other protected class under federal, state or local law. "
32,"Data Scientist (Genomics), Acorn AI Labs",data scientist,/rc/clk?jk=c7172c11e98e38c5&fccid=3e34ac4ae73849ba&vjs=3,"Medidata Conquering Diseases Together Medidata is leading the digital transformation of life sciences, creating hope for millions of patients. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 1,400 customers and partners access the world's most-used platform for clinical development, commercial, and real-world data. Medidata, a Dassault Systèmes company, is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www. medidata. com. Acorn AI is one of the largest AI companies exclusively dedicated to life sciences. It’s built on Medidata’s platform that includes the industry’s largest structured, standardized and growing clinical trial data repository consisting of 17,000+ trials and 4. 5M patients. Our team is composed of over 40 Ph D/Masters statisticians, data scientists, analytical product leads, former FDA biostatisticians and computational genomicists. Your Mission Acorn AI’s is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary Acorn AI platform and advanced analytics. In this role, you will research and develop statistical models. At Acorn AI, we never work alone. This role will partner heavily with all of the key stakeholder functions including Product, Delivery, Engineering, partnerships and Biostatistics. Successful candidates will be skilled in analytical/quantitative thinking, structured communication and excited about building the next horizon of Medidata’s journey of powering smarter treatments and healthier people. Solve some of the most complex problems in healthcare, translating complex data into meaningful insights Design, develop and validate statistical models for novel medical applications. Areas of team focus include Clinical Trial analytics Provide support functions around model-building, including data cleaning and code review Ability to understand and peer review complex, multivariable statistical models and data analytics solutions using machine learning algorithms Ability to work independently on complex and diverse issues and propose solutions Bring to production developed methods and code for integration with existing/new products Work directly with our team comprised of the brightest minds in technology, research and mathematics as well as senior interfaces from leading life sciences companies across the globe Your Competencies Ability to translate business challenges into data pipelines & model framework, owning and driving successful projects Strong verbal and written communication; collaborative focus Fluency in statistical tools and programming languages that allow you to be self-sufficient in handling data R, Python, SQL Ability to apply Machine Learning techniques classification, regressions, feature selection etc. Your Education & Experience Bachelors or Masters in Math, Statistics, Computer Science, Physics, Engineering, Bioinformatics or another quantitative field with a strong foundation in statistical methodology 3-4+ years of experience with statistical analysis in a healthcare related field Experience with large healthcare datasets, Machine Learning techniques classification, regressions, feature selection etc. Experience using Git version control Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. Medidata’s solutions have powered over 14,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. #LI-AS1"
33,"Senior Data Scientist, Audit Transformation",data scientist,/rc/clk?jk=165cbaeb47b477f0&fccid=9e215d88a6b33622&vjs=3,"Are you passionate about AI and its potential to help people make better decisions? Do you dream of building algorithms that understand underlying relationships among data sets? Are you interested in applying machine learning to novel business challenges? If you answered YES! to these questions, come and join our team NOW, we NEED you! As a Senior Data Scientist at Deloitte, you will be helping to bring large quantities of data to life, revealing anomalies in complex data patterns and by offering deep causal insights to better support decision-making. You will be joining a growing team of talented professionals dedicated to bringing to reality Deloitte’s vision of Cognitive Digital Audit by designing, building and maintaining cutting-edge applications that improve the efficiency and quality of our audit services. You will be leveraging the most advanced technologies in machine learning, natural language processing, time-series modeling and reinforcement learning to lead audit into the future of business and technology innovation. Our Audit Transformation organization is rapidly expanding and needs brilliant data scientists like you to drive our continuing innovation and growth. If you’re intellectually curious, hardworking and solution-oriented, you’ll fit right into our fast-paced, collaborative environment, and along the way, you’ll find exceptional development opportunities limited only by your hunger for learning and applying new technologies in our exciting, start-up-like environment. You will be both sharing your expertise with other professionals, and coaching and mentoring data scientists and interns, to accelerate the development of their data intuition and knowledge in Artificial Intelligence. What you will be doing As a Senior Data Scientist, you will be tasked with solving data problems end to end, by collaborating very closely with our practitioners. This means Having and growing a deep understanding of the problem space and of our business needs. Having a profound understanding of the state of the art of a multitude of fields in Artificial Intelligence, including and not limited to NLP, probabilistic graphical models, time-series analysis, and weak supervised learning, among others. Performing exploratory data analysis to understand relationships, and opportunities to influence outcomes, while being able to quickly iterate over common feature transformation and model types in order to find the best predictive models Being well versed in interpreting and explaining the intuition behind the exhibited behaviors and performance of the adopted models Developing proofs of concept to verify your ideas, including counterfactual explanations for interpretability Closing the loop to make sure that the proposed solution is performing as it should and is correctly understood Working closely with business representatives Because this is a senior role, we are looking for candidates that have an established track record and network in the Artificial Intelligence community, can carry out research in an autonomous fashion, and who can interact closely with key stakeholders with varying levels of machine learning experience. The team Audit Transformation Our audits are fueled by more than just technology – what really sets us apart are our insightful professionals, collaborative culture, and commitment to innovation and continuous improvement. Our audit professionals apply a streamlined, intelligent approach to the audit, enabled by innovative tools and technologies. Quality is our top priority, and by focusing on innovation, we continue to raise the bar on quality and deliver greater value to our clients. Learn more about Deloitte Audit. Qualifications Required Ph D in a quantitative field computer science, engineering, mathematics, physics, machine learning, statistics Minimum six 6 years of experience in advanced technology industry with know-how in Python, R, Weka, Matlab, Tensorflow, Pytorch, Pyro, and big data platforms. Minimum six 6 years of experience in data analytics, data modeling and data sciences Minimum six 6 years of experience being the technical lead for multiple project teams simultaneously. Ability to travel when necessary Preferred Prior scientific publication history. Outstanding academic track record as evidenced by top tier publications. Strong algorithm development experience Skills with Java, C++, or other programming language, as well as with R, MATLAB, Python or similar scripting language Profound mathematical background combined with a strong intuition on how to transform theoretical ideas into products Strong competency for statistical software packages SAS, SPSS, MATLAB Solid understanding of advanced analytics statistics, simulation, optimization, etc. Modeling expertise using statistical techniques such as logistic and linear regression, decision trees, neural network or clustering techniques Strong project management and delivery experience, including budget oversight and staffing of project teams including time management Strong oral and written communication skills, including presentation skills MS Visio, MS Power Point, MS Word, MS Excel Ability to maintain a flexible work schedule including overtime Previous experience mentoring, training and developing junior members of the team; experience in employee performance reviews Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment How you’ll grow At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center. Benefits At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you. Deloitte’s culture Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte. Corporate citizenship Deloitte is led by a purpose to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world. Recruiter tips We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals. "
34,Data Scientist (Enterprise BI),data scientist,/company/Brookdale/jobs/Data-Scientist-a553f38b29e3a632?fccid=0a932946342be19c&vjs=3,"We’re looking for a Data Scientist to be a part of the Enterprise BI team Sr. Manager or Director level to work out of our NYC offices once we reopen. For now, candidate will be working remotely until further notice. This individual will manage one or more analysts and focus on customer-level advanced analytics for the direct-to-consumer and business-to-business channels. Experience with customer-level advanced analytics for the direct-to-consumer and business-to-business channels. responsible for day to day coding but we also need someone with a Business analysis mind that can make recommendations on the data they pull. Advanced expertise in SAS, SQL, Tableau and Python would be preferred. Salary $130K + bonus 15% 20% + Full Benefits package Responsibilities include, but are not limited to As a hands-on analytics leader, deliver actionable insights while prioritizing and managing the workload of junior analysts Collaborate with business owners and propose appropriate analytics solutions to address critical business questions Create Tableau dashboards to empower business owners Develop advanced analytics solutions to address some open business questions, e. g. ,Multi-Brand Customer Segmentation Next Best Offer/ Next Best Brand Channel Attribution Customer Retention Key Competencies Strong analytical mind, problem-solving skills and foundation in Statistics Ability to collaborate well with individuals on-site and in remote offices Intermediate to advanced level of expertise using SAS, SQL, Tableau, Python Outstanding analytical skill set a clear expert in the Analytics/ Data Science field Experience working with large volumes of data, combining and reconciling data from different sources Experience in MS Office applications with very strong Excel skills Requirements Minimum B. S. in Mathematics, Statistics, Data Analytics or related quant field5+ years in a similar analytical role generating insights through data analytics Strong communication skills both written and verbal Ability to work independently on multiple concurrent assignments Self-driven for continual learning and ongoing training/development Highly organized, detail-oriented Please advise currently, we are only considering US Citizens or Green Card Holders. Job Type Full-time Pay $100,000. 00 $130,000. 00 per year Benefits 401 k Dental Insurance Employee Discount Flexible Spending Account Health Insurance Paid Time Off Referral Program Tuition Reimbursement Vision Insurance Schedule Monday to Friday Supplemental Pay Bonus Pay Work authorization United States Required Work Location One location Work Remotely Temporarily due to COVID-19"
35,Data Scientist (Enterprise BI),data scientist,/company/Brookdale-Associates/jobs/Data-Scientist-f440f56ebf47c9b2?fccid=9cda7373db4e5ef5&vjs=3,"We’re looking for a Data Scientist to be a part of the Enterprise BI team Sr. Manager or Director level to work out of our NYC offices once we reopen. For now, candidate will be working remotely until further notice. This individual will manage one or more analysts and focus on customer-level advanced analytics for the direct-to-consumer and business-to-business channels. Experience with customer-level advanced analytics for the direct-to-consumer and business-to-business channels. responsible for day to day coding but we also need someone with a Business analysis mind that can make recommendations on the data they pull. Advanced expertise in SAS, SQL, Tableau and Python would be preferred. Salary $130K + bonus 15% 20% + Full Benefits package Responsibilities include, but are not limited to As a hands-on analytics leader, deliver actionable insights while prioritizing and managing the workload of junior analysts Collaborate with business owners and propose appropriate analytics solutions to address critical business questions Create Tableau dashboards to empower business owners Develop advanced analytics solutions to address some open business questions, e. g. ,Multi-Brand Customer Segmentation Next Best Offer/ Next Best Brand Channel Attribution Customer Retention Key Competencies Strong analytical mind, problem-solving skills and foundation in Statistics Ability to collaborate well with individuals on-site and in remote offices Intermediate to advanced level of expertise using SAS, SQL, Tableau, Python Outstanding analytical skill set a clear expert in the Analytics/ Data Science field Experience working with large volumes of data, combining and reconciling data from different sources Experience in MS Office applications with very strong Excel skills Requirements Minimum B. S. in Mathematics, Statistics, Data Analytics or related quant field5+ years in a similar analytical role generating insights through data analytics Strong communication skills both written and verbal Ability to work independently on multiple concurrent assignments Self-driven for continual learning and ongoing training/development Highly organized, detail-oriented Please advise currently, we are only considering US Citizens or Green Card Holders. Job Type Full-time Pay $100,000. 00 $130,000. 00 per year Benefits 401 k Dental Insurance Flexible Spending Account Health Insurance Paid Time Off Professional Development Assistance Referral Program Tuition Reimbursement Vision Insurance Schedule Monday to Friday Experience Coding 5 years Required Data Science 5 years Required Work authorization United States Required Additional Compensation Bonuses This Company Describes Its Culture as Detail-oriented -- quality and precision-focused Innovative -- innovative and risk-taking Outcome-oriented -- results-focused with strong performance culture Stable -- traditional, stable, strong processes Team-oriented -- cooperative and collaborative Work Remotely Temporarily due to COVID-19"
36,Senior Data Scientist,data scientist,/company/Komodo-Health/jobs/Senior-Data-Scientist-7e2bc148b9ab4b23?fccid=5fdb919ed7fb3935&vjs=3,"Who We Are Komodo Health is addressing the global burden of disease through the development of the world's most actionable map of healthcare data. Our solutions drive a more transparent, efficient and productive healthcare ecosystem through the creation of quantitative solutions to qualitative problems. As a fast growing startup that has already partnered with multiple Fortune 500 companies, we have very ambitious goals, and oodles of opportunity for our team members to grow themselves while achieving company success. We value our culture of collaboration, encouraging growth, and constructive debate, as well as delivering innovative solutions that ""wow"" our customers. The Opportunity at Komodo Health We are looking for someone who wants to use one of the biggest, most complete healthcare datasets to solve real world problems that deliver value to our clients while being part of the collaborative, fun, nurturing Komodo community! This role is key as we continue to enhance our capabilities as a group to build repeatable solutions that add to our menu of services for Life Science products, while providing critical value to Life Science customers. As one of the members of this business-critical team, you will Partner with Product to understand team input required to enable successful productization of requests Understand the nuances of healthcare datasets used at Komodo Build a working relationship with the team and client-facing stakeholders Actively follow and improve the coding best practices followed by the team Deliver against high standards for execution for client deliverables What you bring to Komodo Possess a strong blend of technology and analytical skill Demonstrable success in conducting analyses to generate business insights Experience tackling a variety of analytics projects Ability to build strong visualization to tell a compelling data story Prior experience in both optimizing and automating data queries Solution/Delivery-oriented team player willing to go above and beyond High motivation, work ethic, and self-discipline to organize and complete tasks After 3 months, you will Be actively involved in setting the direction for the team to best scale and automate delivery for client requests from our Life Science customers Support Life Science customers and products through custom applications of our healthcare map along with actionable insights that enable Komodo to meet our business goals Iterate to push the boundaries on what problems can be solved through data analytics At Komodo Health, we don't just accept difference — we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products, and our community. Komodo Health is proud to be an equal opportunity workplace. Job Type Full-time"
37,Data Scientist - Machine Learning,data scientist,/rc/clk?jk=6e9323fa06a21820&fccid=0da34673cfff5985&vjs=3,"About the Company At Covera Health, we have proven that advanced data science can prevent serious misdiagnoses which result in poor health outcomes for patients and increased costs for payers. Using our proprietary framework, we help patients recover better, faster, and more affordably. Today, we are working with some of the largest healthcare payers in the country to potentially impact millions of patient lives. We are passionate about improving healthcare for every patient. About the role We are looking for a talented Data Scientist to join us in our commitment to improve the quality of care for patients in every community across the nation. In this role, you will contribute to mission-critical internal and customer-facing analyses and scientific research activities on healthcare provider characteristics, care quality, and patient care patterns and outcomes. This role will be integral to delivering on our commitments to providing meaningful, consistent, and high-quality assessments of provider quality. You will support every step of the analytics journey, from data collection and pipelining, through to production quality model development and monitoring. You must be flexible to changing requirements and detail-oriented in a high-growth environment. In this role, you will be a key member of our growing Data Science team. You will work closely with various team members across the company, supporting our evolving analytics/ML needs and helping build and mature our analytic capabilities. You will be expected to Work with a variety of healthcare data, including healthcare provider profile data, medical images, medical reports, longitudinal claims data, and proprietary diagnostic imaging exam quality assessment data Contribute to the enhancement and extension of the data analytics capabilities and models Collaborate with data science team members and other colleagues on data analysis projects and research activities Communicate the results of analysis and research projects to internal and external stakeholders Contribute to the data science team’s strategic roadmap and identify new opportunities and risks Perform other duties and contribute to additional initiatives/projects, as deemed appropriate by the Company’s management team 1-3 years of full-time experience working as a Statistician/ Machine Learning Engineer/ Data Scientist MS in Applied Mathematics, Statistics, Machine Learning, Computer Science, Physics; or BS with 3+ years of applied machine learning experience Experience developing analytics or machine learning models for use in a production setting Strong experience with Python and in particular experience managing and manipulating data using standard Python libraries pandas, etc Comfortable interacting with databases and large datasets Experience developing code that is leveraged by a wider team and/or contributing to a collaborative code base Exposure to configuring and executing analyses in the cloud e. g. using AWS resources Strong communicator with excellent ability to work in a team environment Experience working with medical data is a plus Benefits You will be a full-time employee with competitive salary, stock options, and great benefits. These benefits include medical, dental, and vision insurance with premiums fully covered , FSA, pre-tax commuter benefits, flexible paid time off, and a comfortable office space filled with a variety of quality snacks and beverages. Most importantly, you’ll get to know each of us and we love to work together to find solutions. We are a smart, fun, focused, and unique team of people who are truly passionate about changing healthcare for the better! Even while remote, you'll experience the Covera culture as we offer a host of virtual experiences including a book club, fitness club, guest speakers, dynamic slack channels, and more!"
38,Senior Data Scientist,data scientist,/rc/clk?jk=48558604544a2a71&fccid=8aeb039f4b39e4c7&vjs=3,"The $21B Powersports market motorcycles, AT Vs, UT Vs, PW Cs, snowmobiles is all about fast and fun, but the purchasing process is slow and frustrating. Octane enables consumers to live their passion by making powersports purchases instant, seamless, and widely available. Octane launched a credit product, branded as Roadrunner Financial, in 2016 to provide financing to prime and near-prime consumers who have historically been underserved by incumbent banks, expanding coverage to 50% of responsible consumers. Together with our automated underwriting and digital buying experience, Octane and Road Runner cut the time to close a transaction from hours to minutes. We are both the platform and the lender, which means we have both high growth and positive unit economics—rare for a fintech. Octane has raised >$100M in equity from IA Ventures, Valar Ventures, Contour Venture Partners, Citi Ventures, and other leading investors. We are seeking a talented data scientist to lead projects related to credit risk analytics and model development. You will be working with a versatile team of data scientists and financial professionals to design and implement models underlying the core functions of Octane's lending business and loan portfolios. Responsibilities Lead and manage the development of machine learning powered predictive models for Octane's real-time credit and technology systems Continuously drive improvements in Octane's modeling approach and architecture Design and implement customer-facing experiments that improve our user experience and grow loan originations Manage and mine large data sets of proprietary and third-party data to draw insights and recommendations for policy and strategy decisions Implement statistical methods to solve specific business problems utilizing code Python Champion data analytics and help advance our culture of data-driven decision-making Requirements Knowledge of various data science disciplines including descriptive analytics, optimization, probability, hypothesis testing, regression, forecasting and machine learning Understanding of end-to-end development of machine learning systems, especially feature engineering and feature selection Experience with both model development and systems integration Excellent Python and SQL skills and familiarity with AWS A track record of taking initiative and driving results, and a passion to build, design and transform a young organization Strong communication skills and experience working across teams MS in a quantitative field, followed by 3+ years experience or BS in a quantitative field, followed by 5+ years experience Industry experience within Consumer Credit, Fin Tech, Financial Services, or similar preferred Benefits Robust Health Care Plans Medical, Dental & Vision Generous Parental Leave Up to 5 weeks PTO self-managed Retirement Plan 401k with Company contribution Educational Assistance/Tuition Reimbursement up to $3K/year Life Insurance Basic, Voluntary & AD&D Short Term / Long Term Disability & Life insurance Pre-Tax Commuter Benefits Office snacks, full fridge, and drinks Weekly catered Lunches Monthly team outings Fun company outings like bowling, yoga, cycling, mixology, cooking classes, & more! Octane Lending is an equal opportunity employer committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status with respect to recruitment, hiring, promotion and other terms and conditions of employment. "
39,"Staff Data Scientist, Marketing Analytics",data scientist,/rc/clk?jk=b4aa559b6da3ddab&fccid=9ecb91618c39a24f&vjs=3,"Company Description As an Etsy employee, you can do the work you love, be yourself, and make an impact in the lives of millions. Our commitments to diversity and inclusion, team culture and the spaces where we work all reflect our mission to keep commerce human. Job Description The Marketing Analytics team helps drive the success of marketing efforts at Etsy. Are you looking to come to a company that uses data in a meaningful way? We provide actionable insights, industry-leading measurement techniques, and access to the metrics, data, and tools needed for decision-making and optimization. We are on the Analytics and Strategic Finance team. We partner with our peers in marketing, analytics, research, product, engineering, and finance through all stages of development. We identify initial opportunities, refining the user experience, analyzing the impact of our efforts, and highlighting improvement areas. Ultimately, the team’s work strengthens Etsy and helps continuously improve the Etsy experience. Learning new skills and techniques is not only a requirement but a perk of the job! We are always looking for opportunities to improve. Our mission is to demystify marketing with data-driven insights, telling the story of how we attract and retain our users to teams, to senior management, and to the community! This role is located in our Brooklyn, NY office. Develop, implement, and refine core marketing measurement frameworks and models, including attribution, churn, CLV, ROI, experiment methodology, and segmentation Play a foundational role in deciding how we can best engage and grow Etsy users by determining and helping execute on opportunities for growth through analysis of behavioral and transactional data Transform raw data into meaningful and impactful analysis characterized by strong data governance, technique transparency, and aggressive documentation Design and analyze rigorous experiments, help teams set great hypotheses, and deliver robust analysis of experiment results Raise the skill level of the entire analytics team through the creation of outstanding work, mentorship, and the introduction of better practices, processes, and tools Qualifications 6+ years experience as a data scientist or data analyst. You have extracted meaning from big data sets with little engineering support. Mastery of SQL and either R or Python. Bonus points for experience with the Hadoop ecosystem and Looker, Tableau, or other data visualization software. An expert in sophisticated analyses and regularly use advanced statistical methods in your work. Experience guiding and scaling marketing efforts and a deep understanding of marketing theory and practice. with a particular focus on measurement techniques, such as customer lifetime value, survival and attrition models, attribution models, marketing mix models, and experimental design. You distill highly complex problems into narratives that are concise, actionable, and memorable, and express your recommendations with both conviction and finesse. Passionate about reproducible work and have picked up tools to this end; you have acted as a mentor, teaching others how to make their own work better and more efficient. Additional Information At Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided a reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. While Etsy supports visa sponsorship, sponsorship opportunities may be limited to certain roles and skillsets. "
40,Data scientist,data scientist,/rc/clk?jk=a6414cd113ba9dad&fccid=0da3ed18c20a248d&vjs=3,"e Bay is a global commerce leader that allows you to influence how the world buys, sells, and gives. You’ll be part of a work culture that’s been genuinely committed to diversity and inclusion since its founding more than 20 years ago. Here, you can be yourself, do your best work, and have a meaningful impact on people across the globe. We seek people with drive, ideas, and a passion for helping small businesses succeed to help shape the future of e Bay—does this sound like you? If so, we’d love to talk to you! Description We are looking to hire an analytics & insights Data Scientist to partner closely with the e Bay Advertising product team. As a critical member of the advertising product Insights team, this role will have the unique opportunity to influence decision making of the Ads Product roadmap with their insights and thought leadership. The Team With a vision to identify opportunities to drive transformative change in the customer experience and result in significant business growth, we mine the largest datasets in the world of e-Commerce and use best in class analytic and big data techniques to extract impactful insights. We achieve this by partnering closely with the advertising product and engineering teams, and identify opportunities to shape the product roadmaps. To be successful in this role 5+ years of experience in analytics/data science Require a minimum of Master’s degree in Statistics, Mathematics, Computer Science, Data Science, Economics, Business Administration or Business Analytics. You would have to strike a balance between strategic thinking and actual hands-on analyses using tools & packages such as SQL, R, Python, Tableau etc. You possess super strong technical skills to turn big data in Hadoop into actionable insights. At the same time, you also have the ability to see the big picture and connect the dots to evaluate how the insights impact e Bay’s ecosystem. In addition to delivering insights on existing Product & Tech initiatives, you will come up with innovative product growth opportunities based on insights, and create momentum through influence. Proactive collaboration and effective communication are critical Additionally, the following background and experience is preferred Proven track record of turning insights into product growth opportunities in prior roles with increasing scope and responsibilities Ability to present complex analyses and insights effectively in simple terms Good experience with both descriptive and inferential statistics – ability to build basic prototype models Experience in a leading technology company Background in ecommerce product experience A Ph. D degree is a plus. Experience with advertising pricing mechanism is a plus Benefits Benefits are an essential part of your total compensation for the work you do every day. Whether you’re single, in a growing family, or nearing retirement, e Bay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal & paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important. Here at e Bay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel e Bay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about e Bay’s Diversity & Inclusion click here https //www. ebayinc. com/our-company/diversity-inclusion/. This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies View our privacy policy View our accessibility info e Bay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay. com. We will make every effort to respond to your request for disability assistance as soon as possible. For more information see EEO is the Law Poster EEO is the Law Poster Supplement This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies View our privacy policy View our accessibility info e Bay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay. com. We will make every effort to respond to your request for disability assistance as soon as possible. For more information see EEO is the Law Poster EEO is the Law Poster Supplement"
41,Data Scientist (Associate)-Investment Management Company,data scientist,/rc/clk?jk=4d7e3ca31be5d622&fccid=bd976cc171c690e0&vjs=3,"Requisition no 507947 Work type Full Time Location Other NYC Locations School/Department IMC Grade Grade 12 Categories Other Position Summary Columbia Investment Management Company, LLC, “IMC” is a wholly-owned subsidiary of Columbia University charged with stewarding Columbia’s endowment for the current and future support of University operations and with preserving the purchasing power of the endowment over the long-term after inflation. Almost all assets across the $10B+ endowment are managed externally in a diversified strategy that uses active and passive management techniques across a wide range of asset classes. The IMC seeks professionals who can contribute materially to the management of the portfolio. Independent thinking and open dialogue are actively encouraged, with team members contributing to the skills, thinking, qualitative and quantitative analysis related to the overall portfolio, including equities, real estate, other real assets, private equity and a variety of public market strategies. The IMC environment is one in which staff members are expected to develop professionally, work collaboratively, and assume greater responsibilities according to ability and impact. Columbia IMC is looking to add a data scientist to the investment team. The IMC has a long history of using data to support investment decision making centered on manager selection and is continually growing and refining its use of data and scientific methods. We are looking for someone who can think independently and work creatively with data. The IMC data science effort is integrated into the investment team and is an active contributor to real and material investment outcomes. We operate a modern data science platform Jupyter Hub-based on AWS and work primarily in Python in the Py Data stack with tabular, relational, time series, and text data. This is a hands-on and very visible position. As a consequence of the responsibilities below, you will be impactful in our collaborative investment deliberations and you will learn and enhance our unique approach to endowment portfolio management. Responsibilities Perform exploratory data analysis, build models, and run experiments to generate data-driven insights across asset classes and geographies which validate or reject our investment hypotheses and refine our assumptions and inputs for all stages of our investment process. Author production-grade reports, analytics, and dashboards for the above and for portfolio risk measurement. Work on studies and models to encode and automate portions of our investment process. Interface with data vendors evaluate the information content of datasets large and small and work with our data engineering team to onboard/ingest/map data. Become a resident expert in our data science platform, existing datasets, and schemas. Minimum Qualifications Bachelor’s degree in a scientific or technical discipline or equivalent and a minimum of three years related experience required. Preferred Qualifications D3. js experience is desirable Institutional financial, risk management, and/or investment domain experience is a plus. Other Requirements 2+ years working in an applied data analysis capacity required Fluency evidenced by academics, projects, work experience, MOO Cs, etc. in data science foundations including probability and statistics, software engineering practices, version control with git & Git Lab/Git Hub, basic Linux CLI/bash, and Jupyter notebooks Excellent written and verbal communication and presentation skills applied to data visualization, storytelling, and dashboarding Python stack, Tableau Tabular and time series data manipulation and analysis in Python/pandas and SQL Familiarity with machine learning methods and the model building/evaluation process e. g. , familiarity with clustering, regression, tree-based modelling, dimensionality reduction, pipelines, and cross validation in scikit-learn Experience working with data in the “real world” e. g. , gathering, cleaning, joining datasets; understanding the assumptions and limitations of common modeling techniques; working with uncertainty in predicted quantities or class membership; presenting to stakeholders with diverse backgrounds and delivering production-grade “data products” Basic experience with Excel You must possess a commitment to and excitement for continuous learning e. g. , reading papers, attending academic and industry conferences, discovering new packages and tools, learning new and unfamiliar technologies and financial topics, etc. . Must have a passion for excellent customer service and commitment to exceptional quality Equal Opportunity Employer / Disability / Veteran Columbia University is committed to the hiring of qualified local residents. Applications open Jul 24 2020 Eastern Daylight Time Applications close "
42,"Model Monitoring Data Scientist, Senior Associate #82595",data scientist,/rc/clk?jk=5f88c7537a46d58f&fccid=f0595ebb13247329&vjs=3,"A career at New York Life offers many opportunities. To be part of a growing and successful business. To reach your full potential, whatever your specialty. Above all, to make a difference in the world by helping people achieve financial security. It’s a career journey you can be proud of, and you’ll find plenty of support along the way. Our development programs range from skill-building to management training, and we value our diverse and inclusive workplace where all voices can be heard. Recognized as one of Fortune’s World’s Most Admired Companies, New York Life is committed to improving local communities through a culture of employee giving and service, supported by our Foundation. It all adds up to a rewarding career at a company where doing right by our customers is part of who we are, as a mutual company without outside shareholders. We invite you to bring your talents to New York Life, so we can continue to help families and businesses “Be Good At Life. ” To learn more, please visit Linked In, our Newsroom and the Careers page of www. New York Life. com. The Center for Data Science and Artificial Intelligence CDS Ai is the innovative corporate Analytics group within New York Life. We are a rapidly growing entrepreneurial department which aims to design, create and offer innovative data-driven solutions for many parts of the enterprise. We are aided by New York Life’s existing business with a large market share in individual life insurance. We have the freedom to explore external data sources, new statistical techniques, and are excited about delivering a whole new generation of predictive analytics and artificial intelligence solutions. In fact, we are building one of the first multivariate model-based continuous risk differentiations in the industry. We are also working on models for differentiated advertising allocation by geography, channel and segment. Geographic analytics on agents and customers, application fraud detection, agent success prediction and client prospecting analytics off-line and on-line are other exciting examples of enormous incremental value from analytics. Our products are implemented into real-time core business processes and decisions that drive the company e. g. underwriting, pricing, agent recruiting, prospecting, advertising allocation, new product development . We work with data ranging from demographics, credit and geo data to detailed medical data medical test results, diagnosis, prescriptions and social media information. We have a modern computing environment with a solid suite of data science/modeling tools and packages, and a large but manageable group of well-trained professionals at various levels to support you. Life insurance is on the verge of huge change. This is a chance to drive the transformation of an industry. You will apply your communications skills working closely with business partners, data scientists, data engineers and technical implementation teams across the company to establish model monitoring and reporting plans. You will help to establish a scalable monitoring and reporting capability for the data science team and our business partners using various programs, libraries and visualization tools to monitor and report on a wide variety of model types. You will participate in other model governance activities like data validation or helping with model validation activities. Responsibilities Work with Business Partners, Tech teams and CDS Ai team to define/confirm monitoring requirements Data sources Metrics/KP Is Build of system/ dashboards to monitor Business metrics Statistical model performance and real time monitoring and alerting Review reporting results with Business Partners on a regular basis, Provide insights based upon monitoring results Technical owner of the monitoring solution s and responsible for daily operations, owner of performance reports and related presentations. Develop monitoring libraries to streamline the monitoring process Determine timing for model re-fit when model performance deteriorates Coordinate model refit with data scientists Maintain and report on statistics of model usage, errors, and exceptions. Participation in ad-hock model governance activities such as data review/validation Qualifications Education Master’s degree in computer science or related field Some formal training statistics or machine learning 3-5 years in a similar role Fluent in Python, R, and visualization Expert with BI tools such as Tableau, Shiny, Dash Good understanding of statistical models and machine learning concepts Working experience with databases and writing complex queries Excellent communication skills Working experience dealing with complex data SF LI-KH1 EOE M/F/D/V If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at 212 576-5811. "
43,eCommerce Senior Data Scientist,data scientist,/rc/clk?jk=87f5d01c229bd3b4&fccid=2973259ddc967948&vjs=3,"Auto req ID 183118BR Job Description Pepsi Co operates in an environment undergoing immense and rapid change, driven by e Commerce and emergent retail technologies. To ensure continued success in the food and beverage space, Pepsi Co has assembled a dedicated e Commerce team – tasked with optimizing e Commerce operations and developing innovations that will give Pepsi Co a sustainable competitive advantage. While tied closely to broader Pepsi Co, the e Commerce group more closely resembles a start-up environment; embracing the core values of having bias for action, being results oriented, maintaining a community-focus, and prioritizing people. Pepsi Co’s Data Science and Analytics group is a team of data scientists, technology specialists, and business innovators who operate within e Commerce to build industry-leading systems and solutions. By focusing on machine learning and automation, the Data Science & Analytics group is pushing the bounds of possibility for Pepsi Co and its strategic partners. What Pepsi Co Data Science & Analytics does Build machine learning systems to understand the cross-channel grocery ecosystem Perform statistical analysis across diverse datasets to drive and measure performance Work with Pepsi Co’s strategic partners to expand their technical capabilities, thereby creating a more robust data environment Utilize natural language understanding techniques to uncover insights from contextual data Develop scalable tools to drive automation and optimize business operations As a Sr. Data Scientist, you will play a critical role in shaping and executing the global ecommerce growth agenda. You will be tasked with identifying, designing, and implementing data science/machine learning solutions to business problems. You will collaborate with the larger data science and analytics teams to create a robust, shared codebase; on which Pepsi Co will build automated e Commerce systems. You will work with internal business stakeholders and strategic partners to identify opportunities for collaborative development and foster a data-driven culture between relevant teams. Responsibilities You will work with the larger e Commerce organization to analyze large data sets and develop custom models/algorithms to uncover trends, patterns and insights You will establish standards for writing clean, organized machine learning code to streamline machine learning workflow within the DSA team You will provide critical thought leadership to enhance organizational capabilities by utilizing a variety of data sets; work with business stakeholders to identify and execute on opportunities for enhancement You will partner with other analytics leaders within Pepsi Co to foster enhanced analytical capabilities globally Qualifications/Requirements Master’s Degree in Data Science or equivalent Minimum 5 years of relevant work experience Experience designing and developing machine learning solutions using a variety of data sources Expertise customizing Modern ML systems, including NLP, Deep RL, and/or Graph Neural Networks. Experience and functional familiarity with software development and data engineering best practices Expertise designing custom deep learning systems using Tensorflow or Pytorch Expert in mathematical models underlying data science methods, understanding the trade-offs between different solutions Demonstrated ability to effectively and concisely communicate with both business and technical audiences Relocation Eligible Not Applicable Job Type Pipeline All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. Pepsi Co is an Equal Opportunity Employer Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance. If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View Pepsi Co EEO Policy Please view our Pay Transparency Statement"
44,"Data Scientist/Engineer (SAS, R code, Python & Sql (Open to remote Workers))",data scientist,/rc/clk?jk=cd5d26bc44469b3d&fccid=3e34ac4ae73849ba&vjs=3,"Medidata is leading the digital transformation of life sciences, creating hope for millions of patients. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 1,400 customers and partners access the world's most-used platform for clinical development, commercial, and real-world data. Medidata, a Dassault Systèmes company, is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www. medidata. com. Acorn AI is one of the largest AI companies exclusively dedicated to life sciences. It’s built on Medidata’s platform that includes the industry’s largest structured, standardized and growing clinical trial data repository consisting of 17,000+ trials and 4. 5M patients. Our team is composed of over 40 Ph D/Masters statisticians, data scientists, analytical product leads, former FDA biostatisticians and computational genomicists. Your Mission Power smarter treatments and healthier people with innovative analytic applications and advanced data systems Create innovative applications and data flows for research and production settings with engineering best practices, transparency, and scalability. Develop advanced data systems to extract, assess, integrate, transform, clean, analyze and visualize datasets for complex analytics and statistical modeling. Solve complex business questions where situations or data require in-depth evaluation of variable factors. Assess analytical data sources, conduct hands-on exploration to determine their value, and make them available enterprise-wide Collaborate with data scientists, business leaders and cross-functional stakeholders to implement data science solutions based on business priorities and technology initiatives. Your Competencies Advanced skills in data transformation, statistical processing, modeling dataset construction and manipulation of structured and unstructured data sources. Deep experience in data engineering technologies including SAS, R, Python, SQL in a cloud setting. Familiarity in NLP and entity recognition are a plus. Familiarity with statistical concepts and experience supporting complex predictive model builds and implementations. Demonstrated ability to collaborate with all levels of data science, technology personnel and senior leadership. Clear, concise communication abilities – writing, verbal, presentation – to all levels of technical and non-technical audiences. Entrepreneurial spirit and commitment to creating rigorous, high-quality insights from data, at scale. Your Education & Experience Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar. Master’s degree or Ph D preferred. 5+ years professional experience as a data scientist, data engineer, data analyst, or related role Experience with clinical trial data is not required, but interest to learn and understand how these data drive medical research is paramount Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. Medidata’s solutions have powered over 20,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. #LI-AS1"
45,Data Scientist,data scientist,/pagead/clk?mo=r&ad=-6NYlbfkN0AMr11YIOo206dX9CE572HoIBzjTozchv3QrfVeNVhstmAbPwFeZPDdIHWIRTi_7rD14lCNYGuwtEHA9YNHEeYrn3dcDEiIg_Ik5NF1f4VEKEnmaJokHG5qxmDtvgjd2KF3cZS1nO_oXoK1Y5_vS7D1LjkOzTVcVLRBGqi0YSDl9Vi2tB_MCtHDEY1N9_1FRT7gN7Ttgx5Yl10ZiAqxhnpAY_xDc9_Enb-EfOROOl2Rzm7QwL68fBmOwA8URx3GHeLwSTpK0derOLmwlLqtgmgoLcaHWm1CDt8--w4gKj4_sSDih-JyWvowTxjp6TK6sDVLJ5LfYZ-fvF_KPz9SquWHqE6c3zqgMzN80L8XkWk-SJLmwdsdy31XpOJhV_1FVDaRwE62aaKUQaZCjYDAKqG-g7vQm49UFI12mUnD-pW4tCZOtBbdoDtACyCSFzaxT5CKRijH0yPYQ8UU6vCdIAQ97jiWPLIypKAqeu0LrSMvaScxX8-xBLPKm9K8TuOM0sXJIZuu849E_Q==&p=12&fvj=1&vjs=3,"Role Description As a data scientist at Triplebyte, you’ll have the opportunity to work on a variety of challenges to help us scale. You'll be part of a small team, who are leveraging data to fix technical hiring. Your day to day, will include a mix of dataset acquisition, statistical modeling, exploratory data analysis, and software engineering. You’ll report directly to Triplebytes' Head of Machine Learning and will work alongside a team of 6-8 machine learning engineers and data scientists. Fields your work will touch on Psychometrics Recommender systems Time series analysis Survival analysis Bayesian inference Probabilistic programming This is an ideal role for a data scientist who wants the scope and responsibility to own features/products from the inception and research phase through to measuring real-world results. About Triplebyte Triplebyte is a hiring marketplace used by companies like Apple, Dropbox, Stripe, and Instacart to hire the best technical talent. We are on a mission to build a meritocratic hiring process, and we do all our evaluation background blind. Our ultimate goal is to collect the largest dataset and use this to build the world's best technical hiring process. No other company has successfully done what we're doing in this field. We are growing extremely quickly, working on a problem that is fundamental, sitting at the crossroads between the workforce and employers. Ten years from now, it'll look silly to use anything other than Triplebyte for technical hiring. Company Culture We have a laid-back, friendly office culture. Over lunch you'll often find us discussing the latest in technology, books, and pop culture, and then maybe getting in a quick game of chess or babyfoot foosball . Since we're an early-stage company, we move fast, and it's important that each member of our team is able to take ownership of projects by defining problems, brainstorming solutions, and running experiments. Job Type Full-time Pay $145,000. 00 $225,000. 00 per year Benefits 401 k Dental Insurance Flexible Schedule Flexible Spending Account Health Insurance Paid Time Off Parental Leave Professional Development Assistance Referral Program Relocation Assistance Vision Insurance Schedule Monday to Friday Work Location Fully Remote This Job Is Ideal for Someone Who Is Achievement-oriented -- enjoys taking on challenges, even if they might fail Autonomous/Independent -- enjoys working with little direction Innovative -- prefers working in unconventional ways or on tasks that require creativity This Company Describes Its Culture as Innovative -- innovative and risk-taking Outcome-oriented -- results-focused with strong performance culture Team-oriented -- cooperative and collaborative Company's website https //triplebyte. com/Company's Facebook page https //www. facebook. com/triplebyte/Benefit Conditions Only full-time employees eligible Work Remotely Yes"
46,Data Scientist,data scientist,"/company/Attorney-General,-New-York-State/jobs/Data-Scientist-5f0ca6041f887ce6?fccid=5181fc3a650a862e&vjs=3","Executive Division Research & Analytics Department – New York City Data Scientist Reference No. EXEC_DAT_NYC_6138Goal The Research and Analytics Department in the Office of the New York State Attorney General OAG is seeking candidates for a full-time Data Scientist position to explore datasets, develop new databases, and use predictive modeling to detect anomalies in information pertinent to OAG investigations. The goal of the data scientist at the OAG is to ensure that our most complicated cases and policy arenas have the benefit of advanced analytical tools to understand how otherwise elusive trends can be laid bare once we dive into the underlying data. Purview As part of an innovative team housed in the Executive Division, you will work with senior staff and divisions throughout the OAG to support the OAG's major initiatives, investigations, and policy development. Our team researches and writes analytical reports garnering national attention, directs multi-million dollar grant programs, and supports the office’s major investigations. See for example Preliminary Report on NYPD Response to Protests, Opioids lawsuit, lawsuit protecting tenants from lead poisoning, FX options fraud, and the Virtual Markets Integrity Initiative. Utilizing a vast collection of public and confidential information, you will work with data covering the breadth of the OAG’s purview building advanced analytic tools to inform investigations that affect the lives of all New Yorkers. Employing over 1,700 people across New York State, the OAG is both the “People's Lawyer” and the State's chief legal officer. The Data Scientist will support OAG work in numerous legal and regulatory arenas, including protecting the rights of investors and consumers, coordinating state-wide criminal investigations, promoting civil rights, economic and social justice, encouraging harm-reducing public health strategies, and preserving the State’s environment. The OAG also advises the Executive branch of State government and defends actions and proceedings on behalf of the State. Responsibilities The OAG has a rich source of public and confidential data. The Data Scientist will be primarily responsible for · Exploring and analyzing data· Communicating findings through written reports and visualizations Additional responsibilities may include · Developing predictive modeling to detect anomalies· Developing or programming new databases or data tools· Performing other duties as requested Qualifications/Skills · Advanced degree in computer science, mathematics, applied statistics, physics, engineering or similar disciplines with demonstrated research capability or bachelor’s degree and equivalent experience and training;· Experience with the following technologies o Programming, scripting and statistical languages e. g. Python, R, use of dataframes o Data visualization software e. g. Tableau, d3, plotly, matplotlib, bokeh o Relational databases and advanced SQL queries· Excellent oral, written, and presentation communication skills· Ability to communicate technical concepts and results to a non-technical audience· Highly organized, self-directed, and curious· Personable and positive, with a high level of integrity· Interest in public policy and social/economic justice Candidates with additional experience in any of the following are preferred o 2+ years demonstrated qualitative, quantitative or investigative research abilities;o Git/github and linux command line toolso Graph databases e. g. Neo4J o Cloud, distributed and container computing platforms e. g. AWS, Redshift, EC2, docker o Machine learning frameworks e. g. supervised and unsupervised methods such as neural networks, random forests, and generalized linear models o Analyzing social networks e. g. Facebook, Twitter, Instagram o Natural Language Processing or Entity Resolution OAG employees serve more than 19 million state residents through a wide variety of occupations. We offer a comprehensive New York State benefits package, including paid leave, health, dental, vision and retirement benefits, and family-friendly policies. As an employee of the OAG, you will join a team of dedicated individuals who work to serve the people of our State. HOW TO APPLY Applicationsarebeingreceivedonline. To apply, please visit our careers website https //ag. ny. gov/job-postings. Applicants must be prepared to submit a complete application consisting of the following · Cover Letter Youmayaddress to Legal Recruitment · Resume· Transcript if fewer than five 5 years post-graduate;· Listof three 3 referenceswithcontactinformationandemailaddresses· Writing Sample Pleasenote Failureto submit a complete application will delay the consideration of your application. If you have questions regarding a position with the OAG and the application process or you need assistance with submitting your application, please contact Legal Recruitment via email at recruitment@ ag. ny. govor phone at 212-416-8080. Formoreinformationaboutthe OAG, please visit our website ag. ny. gov Job Type Full-time Benefits Dental Insurance Health Insurance Retirement Plan Vision Insurance Schedule Monday to Friday Company's website ag. ny. gov"
47,"Data Scientist, Platform Analytics",data scientist,/rc/clk?jk=58cd145794f880a3&fccid=4b5d257051285786&vjs=3,"Job Summary Comprised of Disney’s international media businesses and the Company’s various streaming services, the Direct-to-Consumer and International DTCI segment aligns technology, content and distribution platforms to expand the Company’s global footprint and deliver world-class, personalized entertainment experiences to consumers around the world. The Walt Disney Company’s Direct-to-Consumer and International segment DTCI is a global, multiplatform media, technology and distribution organization for high-quality content created by Disney’s Studio Entertainment and Media Networks groups. DTCI includes Disney’s international media operations and the Company’s direct-to-consumer businesses globally, including the upcoming Disney-branded direct-to-consumer streaming service, the Company’s ownership stake in Hulu, and the ESPN+ sports streaming service, programmed in partnership with ESPN. BAMTECH Media, developer of the ESPN+ and Disney-branded streaming platforms, oversees all consumer-facing digital technology and products across the Company as part of the Direct-to-Consumer and International segment. Data Scientists in the Platform Analytics team are the analytical partner to the Disney Streaming Services product and engineering teams. They use data to empower decision-makers with information, predictions, and insights that ultimately influence the experiences of millions of users worldwide. Scientists on the team build models, perform statistical analysis, and create visualizations to provide scalable, persistent capability that is iteratively improved through direct interaction with cross-functional business partners. Responsibilities Business Performance Analysis, Visualization, and Regression Management Be the primary partner for cross-functional stakeholders to understand performance of the platform. When a metric is ahead or behind expectations, dig in to drive meaningful, actionable insights that help the business performance improve or double-down on success. Deep-Dive Analysis Analyze user behavioral data to identify patterns, uncover opportunities, and create common understanding of how people are interacting with the platform and content. Forecasting, and Opportunity Sizing Both to set goals and help evaluate potential opportunities, this analyst will be tasked with forecasting and opportunity sizing. This will help business stakeholders evaluate trade-offs in different approaches, and create targets for driving business performance. Partnership Partner closely with business stakeholders to identify and unlock opportunities, and with other data teams to improve platform capabilities around data modeling, testing platforms, data visualization, and data architecture. Basic Qualifications 5+ years of analytical experience. 3+ years of experience using SQL 3+ years of experience using Python or other statistical programming language Deep understanding of statistical concepts like hypothesis testing, Bayesian decision-making, etc. Hands-on experience in advanced data analysis Strong communication skills, as well as written and verbal presentation skills. Preferred Qualifications Ability to think strategically, analyze and interpret market and consumer information, preferably about a subscription service. Familiarity with data exploration and data visualization tools like Tableau, Looker, Chartio, etc. Preferred Education Advanced degree in an analytical field or equivalent experience. "
48,"Data Scientist, Android Go-To-Market",data scientist,/rc/clk?jk=3debac83f3c241d2&fccid=a5b4499d9e91a5c6&vjs=3,"Note By applying to this position your application is automatically submitted to the following locations New York, NY, USA; Mountain View, CA, USA Minimum qualifications Bachelor's degree in Math, Computer Science, Economics, or a related field, or equivalent practical experience. 6 years of experience as an analyst or in an analytical role i. e. sales support, market research, financial analysis . Experience analyzing data and creating reports with database query tools e. g. SQL and visualization tools e. g. Tableau, Dashboards . Preferred qualifications Ph D in Statistics, Computer Science, Engineering, or Mathematics. Experience with big data and cloud platforms to deploy large-scale data science solutions. Experience working in industries such as retail, consumer electronics, or telecom including in a client-facing consulting or advisory capacity , and the ability to navigate intricate business organizations and manage multiple cross-functional stakeholders. Knowledge of media and the statistical algorithms typically used in marketing analytics. Ability to be adaptable and act with a sense of urgency when needed. About the job In this role, you will be a member of a new team focused on tools and methodologies to generate impact for Android's Go-to-market approach. You will help design the analytical infrastructure, and use your expertise in data analytics, statistics, and causal inference to help maximize business growth. Google’s Global Partnerships team powers extraordinary user experiences through partnerships that solve users’ needs, advance our partners’ goals, and further Google’s mission. Our team provides meaningful solutions — from helping publishers and developers grow their businesses through monetization solutions, to helping our long-term partners anticipate and navigate change in a way that delivers economic and reputational value. We build and distribute products through partnerships across a multitude of product areas, including Ads, Search & Assistant, Geo, Platforms & Ecosystems, Devices & Services, Health, Retail, Payments, Next Billion Users, and more. We collaborate across Google, ensuring a coordinated approach when engaging with our most complex and strategic partners. Responsibilities Support intricate Go-to-Market engagements to understand incrementality and effectiveness of actions on Android device sales with data science solutions. Design new methodologies, pull necessary data, build statistical and machine-learned models with accuracy, and analyze experiments, applying critical thinking throughout. Collaborate across internal teams and manage opportunities and challenges in ways that improve processes. Communicate and visualize insights to multiple levels of stakeholders with clarity, to help inform decision-making. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form. "
49,"Data Scientist, Analytics - Messenger Systems",data scientist,/rc/clk?jk=0557c18047338d85&fccid=1639254ea84748b5&vjs=3,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities we're just getting started. Messenger engagement has increased dramatically and Messenger teams are focused on continuing to increase the user base, improve user experience across existing features, but also largely focused on all the new product and feature development across the platform to engage all demographics Messaging, Status, Realtime Communication, Expression, Groups, Vo IP, Chat Heads, and Rich Attachment Types, like Selfies, Location, and Stickers. . Messenger currently has more than 1. 3 billion monthly active users, and this team focuses on the underlying systems that affect all Messenger users and enable new communication experiences both inside Messenger and across the FB family of apps. We are looking for a Data Scientist who enjoys working on complex technical problems and can connect technical concepts e. g. , mobile app performance, platform reliability to business strategy and user experience. Work cross-functionally to define problem statement, create long term product vision, and build analytical models to make recommendations Apply analytical skills to gain deep insights into Messenger’s systems and how it affects end user experience Partner with Product and Engineering teams to solve problems and identify new levers to improve product performance and user experience Influence the product roadmap and decisions made through presentation of data-based recommendations Communicate metrics, trends and other key indicators to leadership Leverage tools like R, Python, Hadoop & SQL to drive efficient analytics Advanced degree in Math/Statistics, Engineering, Physics, Computer Science, or equivalent practical experience 5+ years of experience solving analytical problems using quantitative approaches Experience initiating and drive projects to completion with minimal guidance Experience in cross-functional partnership among teams of Engineers and P Ms Experience in SQL Understanding of statistics e. g. , hypothesis testing, regressions Experience communicating the results of analyses with product and leadership teams to influence the roadmap of the product. Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions , sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb. com. "
50,"Senior Data Scientist Job, New York NY, $300,000",data scientist,/pagead/clk?mo=r&ad=-6NYlbfkN0D6Outzevkklm9u1wEqPpqtLI6IM60NTtoSPOUa_Du_DomORgL6XSWXSpHJVOFhkO-r0iLyGWkLlWDA_oElfzkQBQmLnOOptGdf6oGhR7RY6QEop4XGZ9dVrXvQYqMzfv_qBjWTVvjaaCeSCKTkvvbMSUfKmklY95EMKIMJ9CaNP9nT_KgM7RK2s9rZ_w97NwGPjYTOXemup_Tqf0X9rmI_weEY7y2uV54_G_H_L3yKsJS-qb2vysaeElhQHq4jJsnBk_if1pMI5-Z6mtDTO3YMxrl8hS6j1RdNLR83Fv2uStiFTCrGNTLAi9L7Jaf1YtsmYrkx7mC_eFJYr8ux7oMRALeWVZM0uKtjLqqI-VBeX6nClkXhvojLIfnzeC7ZQgOxgB5qrRQAunVC7tx-iNmAwkErmTml4c7fPSKyO_-FU5Y3jOCRqJG3mj_f2SJ5oNDxj1pF8k8BYJq6Uw9aZnyk0iwBlOBUUpRhbNYRzXG8a0b9JnnxZIuuDqWtitilpP1rPCdTeukqokP4walbgqLWtk2xMxQSCK3IBuQ7IbZJ2Jve_T2_ppoDp1s6UzQc8LI2e4nnGUK_mI5rhqwDamHodu0aLjNwDN5-0CJrBiJ0JNj3xwG1Ow4yXlwxZZsNg0YevHiJIdRzYsIQxZ2msT1msjhbu3aFG34=&p=6&fvj=0&vjs=3,"Role Summary Staff Risk and Investigations team are seeking a highly capable individual to help enhance the Insider Threat Program by working closely with security and development teams to solve complex risk and security challenges. What does the role involve? Accumulate and curate data needed to understand potential solutions to the problems, Communicate the insights and results of your analysis to the team Continuously evaluate changes in actors, tactics, techniques and targets to enhance threat scenarios. Can I apply? Minimum of 6 years of data science/engineering experience Strong Python programming skills with an understanding of what it takes to write high quality code Demonstrated success in managing projects and working closely with business stakeholders Awareness of latest cyber security trends and development Strong oral and written communication skills Strong interpersonal and leadership skills Ability to work in fast paced environment As a leading UK and US Cybersecurity recruitment business, we would be excited to hear from you in order that we might help you progress your career, to discuss this opportunity in more detail, please contact 020 7382 7980 or Matthew. chipchase@beechermadden. com"
51,Herbert and Florence Irving Research Data Scientists,data scientist,/rc/clk?jk=69b99d0387b0b17a&fccid=bd976cc171c690e0&vjs=3,"The Data Science Institute of Columbia University invites applications for research scientists at all levels Associate Research Scientist, Research Scientist, and Senior Research Scientist. This position is endowed and the holder will be named a Herbert and Florence Irving Research Data Scientist. A successful candidate will serve as a resident expert in data science whose research interests are specifically focused on advancing and applying data science to understanding cancer. He/she will foster collaborations with world-class faculty across Columbia’s schools, centers and institutes in order to guide, plan and execute data science research and technological innovation in cancer-related research. Columbia University is a leader in the burgeoning field of data science, with depth drawing from foundational strengths in computer science, artificial intelligence, machine learning, statistics and operations research, and the breadth of applications in every field of inquiry offered university-wide. Founded in 2012, Columbia’s Data Science Institute has 350 affiliated faculty from 18 different schools, colleges, and institutes across the University. The Institute provides research and educational opportunities in data science as applied to the biomedical sciences, the natural sciences, the social sciences, public policy, law, business, finance, media, engineering, the arts and humanities. The Institute boasts a robust industrial affiliates program with opportunities to collaborate with domestic, international and multinational organizations and strong ties with New York City’s economic development engine, including the dynamic and rapidly growing start-up community. The Data Science Institute at Columbia University promotes “Data for Good” through its mission to advance the state-of-the-art in data science; to transform all fields, professions, and sectors through the application of data science; and to ensure the responsible use of data to benefit society. We expect successful candidates to Advance and apply data science in the context of cancer research;Identify opportunities for and participate in long-term cancer related research collaborations between the Data Science Institute and the various schools at the University, the Herbert Irving Comprehensive Cancer Center and the Herbert and Florence Irving Institute for Cancer Dynamics; Contribute to and steward multi- and interdisciplinary data science research projects across the University; Support the mission of the Institute on developing the interplay between data science and cancer research, through contributions to outreach and training. Minimum Degree Required Candidates must have a Ph. D. or professional equivalent. Minimum Qualifications All candidates are expected to have a record of outstanding productivity, commensurate with years of professional experience, as evidenced by publications, grants, software innovation, technology transfers, and other academic and/or industrial measures of impact, and experience in both cancer-related and data science research. Officer of Research positions have several ranks, and candidates will be appointed at an appropriate rank given their experience Preferred Qualifications Additional Information RAPS posting date 07/21/2020 Search Closing Date Special Instructions to the Applicant Proposed Start Date EEO Statement Columbia University is an Equal Opportunity Employer / Disability / Veteran Review Begins"
52,Data Scientist,data scientist,/rc/clk?jk=7bd46878a4caf6cc&fccid=dd616958bd9ddc12&vjs=3,Position Data Scientist Location REMOTE Duration 6+ Month Contract Interview Process Video Interview 7/14 or 7/15 Start Date ASAP Top Qualifications Extensive experience working with SQL Experienced working with Python and Pycharm Experience with Azure Data Warehouse Experience working with postman Working with Databricks and digital data experience with adobe analytics data is nice to have Nice experience to have SAS R Matlab STATA Hadoop Docker Tableau If interested and qualified please send your latest resume to matthew. johnston@mondo. com! Thanks!
53,Data Scientist,data scientist,/rc/clk?jk=e5fa1ac54c20d57a&fccid=cbfe4b98d5d8dd69&vjs=3,"Company Description Quartet is a pioneering healthcare technology company striving to improve the lives of people with mental health conditions. We connect people to a personalized care team to get them the right care at the right time. Our collaborative technology platform and range of services brings together physicians, mental health providers, and insurance companies to effectively improve patient outcomes and drive down healthcare costs. Backed by $153MM in venture funding from top investors like Oak HC/FT, GV formerly Google Ventures , F-Prime Capital Partners, Polaris Partners, Centene Corporation and Echo Ventures. Quartet is headquartered in NYC and is currently operating in several markets across the United States — Pennsylvania, Washington, Northern California, New Jersey, North Carolina, Louisiana, and Illinois. About the team & opportunity As a Data Scientist at Quartet, you will work on a range of projects developing statistical analyses to study impact of Quartet interventions; predicting mental health needs among populations; building machine learning models to suggest timely and appropriate behavioral health care interventions for patients. You'll develop a deep understanding of Quartet interventions and the predictive models and algorithms. You will design and develop effective models, features, and algorithms involving multiple datasets, including user activity, Electronic Health Records EHR , admissions, discharges and transfers ADT , medical claims, pharmacy claims, and lab test claims. Leverages knowledge of computer science, machine learning, data mining and software architecture to build high-quality data products. Supports the design of Quartet products, including an entity resolution and de-duplication library for linking patients and providers data, which is collected from multiple sources; a machine learning application for the detection of opioid use disorder in patients using supervised learning and deep learning; and an optical mark recognition OMR computer vision application for transcribing paper assessments. Accountabilities Research and develop machine-learning and statistical models in Quartet Health's platform to improve software personalization and recommendations for users Drives a data-informed process for experimenting with new products to improve patient outcomes and operational efficiency. Performing ML research, exploratory data analysis, and computer-vision modeling; developing model evaluation and online maintenance methodologies, including confidence estimation, calibration, and concept drift Develop general-purpose frameworks to support machine learning applications in production. Minimum Qualifications 2-3 years experience as a data scientist. Formal training in statistics and computer science. Knowledge of mathematical fundamentals probability theory, linear algebra and statistics. Strong data transformation and extraction skills with SQL databases. Strong statistical programming skills in both Python and R. Comfort/self-sufficiency with Amazon Web Services infrastructure. Comfort/self-sufficiency with Linux command line. Comfort/self-sufficiency with git for version control. Ability to work on projects from initial to final phase- beginning by defining a problem, developing an implementation plan, and overseeing deployment and maintenance. Ability to clearly communicate across disciplines and work collaboratively. Experience with any of the following Docker, Tensor Flow, Keras, Numpy, Scipy, Pandas, Gensim, Scikit-learn, NLKT, Jupyter Employee Benefits for Quartet include Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, mental healthcare coverage of 15 free therapy sessions + unlimited copay reimbursements, medical, dental + vision coverage, generous parental leave, commuter benefits, 401K, stock option grants, gym benefits. Want to know what Quartet life is like? Click here to meet our team. Quartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/C Vs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet. Please note Quartet interview requests and job offers only originate from quartethealth. com email addresses e. g. jsmith@quartethealth. com . Quartet will also never ask for bank information e. g. account and routing number , social security numbers, passwords, or other sensitive information to be delivered via email. If you receive a scam email or wish to report a security issue involving Quartet, please notify us at security@quartethealth. com. Have someone to refer? Email talent@quartethealth. com to submit their details to us. "
54,Data Scientist,data scientist,/rc/clk?jk=a905c31dace13629&fccid=ae4c0182fd37ef13&vjs=3,"Hinge is seeking a versatile data scientist to work on product development initiatives. From assessing opportunities and results through analytical techniques to curating personalized experiences with machine learning, you will take on a central and multi-faceted role in the product development process. In turn, your work will deeply influence how millions of Hinge members connect with each other. Responsibilities Collaborate with a cross-functional team of product managers, researchers, and engineers to develop and iterate on product features Translate product objectives to data science problems and vice versa Apply statistical inference to draw rigorous conclusions from data Identify and execute on opportunities to apply machine learning to improve user experience Perform ad hoc exploratory analysis as needed Requirements 2+ years of experience as data scientist Strong knowledge of statistics and machine learning Experience with A/B testing Proficiency in SQL and Python Exposure to engineering best practices Superb communication skills Deep sense of intellectual curiosity Comfort with ambiguity Our Company Hinge is the dating app for people who want to get off dating apps. In today’s digital world, singles are so busy matching that they’re not actually connecting, in person, where it counts. Hinge is on a mission to change that. So we built an app that’s designed to be deleted. On Hinge, there are no rules, timers, or games. Instead, you’ll meet your most compatible matches and you’ll have unique conversations over what you’ve shared on your detailed profile. It’s a natural way to find a great first date. Currently, 3 out of 4 first dates lead to second dates, we’re the #1 mobile-first dating app mentioned in the New York Times wedding section, and we’re the fastest growing dating app in the US, UK, Canada, and Australia. Our Culture Authenticity Share your genuine thoughts and opinions directly. Courage Invite and deeply consider challenges and criticism. Empathy Be empathetic, communitarian and trustworthy. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. "
55,Data Scientist,data scientist,/rc/clk?jk=192724af0f1a2826&fccid=ba6506c6dbb696a1&vjs=3,"Job Data Scientist Reports to Director, Data & Insights Start Date ASAP At its core, Do Something. org is a data-driven organization. Data leads every strategy and goal-setting conversation. The Data & Insights team’s job is to analyze large and rapidly evolving data sets to detect changes in our member's behavior and the global youth population. The Data Scientist’s role at Do Something. org is to be the analytical powerhouse that fuels and informs decision making across the organization enabling us to create the most socially and civically engaged generation of young people, ever. You will deliver analyses and products that advance our understanding of drivers of growth and impact. A successful candidate will possess strong analytical and programming skills alongside a steadfast sense of curiosity, ability to communicate to technical and non-technical stakeholders and a solutions-oriented mindset. What you'll do Data Analysis and Model Building 50% Mine and synthesize large data sets to enhance the organizations understanding of membership segments, propensities, outcomes and decision points. Build models and analyses that uncover the key drivers of acquisition, engagement and retention within the context of the Do Something campaign mix and product feature development. Build statistical models to predict various outcomes of interest and forecast KP Is Deliver findings & recommendations to non-technical, cross-functional stakeholders via various visualization methods Google Analytics, Slides, Docs, Looker, notebooks etc. Assist in sizing impact of new growth opportunities Contribute to the development Do Something. org’s long term analytics strategy Data Infrastructure Strategy & Design 20% Collaborate with product management and engineering departments to understand company needs and devise possible data product solutions that scale Inform requirements for derived tables that enable analysis; Optimize these joint development efforts through appropriate database use and project design Inform requirements for monitoring to ensure our data pipelines are functional, scalable and working as expected BI/ Operational support 30% Support ad hoc requests for data & insights from business stakeholders across product, marketing, campaign and business development teams Support top level Looker reports/dashboards for non-technical staff What we're looking for Strong sense of curiosity. You have the desire to learn new skills and understand complex problems fast. Ability to explain analytical outcomes in basic terms. You understand why making data accessible to the larger organization is important, and don't want to be a gatekeeper to this pertinent information. High degree of technical proficiency 2-3 years of experience with SQL any variant is fine, though we use Postgre SQL 1-3 years of experience with R or Python Ability to write scalable, portable scripts that automate querying or generate reproducible analysis Experience with the following BI tool Tableau/Looker/Power BI Working with non-relational data Data warehousing technology and environments a plus, not a non-negotiable Team oriented with the ability to work cross-functionally and collaboratively at all levels of the organization. Well-rounded individual. You are currently learning a new skill or adopting a new hobby that has nothing to do with data science. Other Notes Data is everywhere. If you’ve worked with it at scale, apply, even if you don’t meet every single qualification, please apply! Past successful candidates have come with backgrounds in CS, statistics, mathematics, and economics, although these are not required. Application Instructions. Here at Do Something, we are committed to cultivating an equitable and inclusive environment where a diverse group of people can do their best work, and that starts with how we hire. We ask that you please remove all identifying information from your resume before you upload your application in an effort to help us remove unconscious bias from our resume review process. Identifying information includes your name, photos, Linked In URL, email and physical address. The Perks. 3 weeks vacation plus the week between Christmas and New Years plus Summer Fridays from Memorial Day to Labor Day! . And your birthday, Valentine’s day, and a half-day on Halloween. If you’re in costume. Seriously. An inclusive office environment, a gong, brownie bake-offs, and the best coworkers you could ask for. Medical and dental premiums fully covered by us. Mhmm. You read that right. Five or six? We’re losing count. different ways of making coffee. An incredibly compelling reason to wake up and make it to work every day. Do Something. org is an equal opportunity employer. Please, no calls. Do Something. org and Do Something Strategic “we”, “us” are committed to protecting and respecting your privacy. This policy explains when and why we collect personal information on candidates, and how we keep it secure. Who are we? Do Something. org is one of the largest organizations in the world focused on young people and social change. Do Something Strategic is the consulting arm of Do Something. org. Our 6 million+ members take action, online and offline, to better their communities by participating in one of our 250+ cause campaigns. How and why we collect data from you. We collect and retain data that is included in your job application. This includes your full name, contact information, and any personal information disclosed in your resume, cover letter, and other supplemental application materials. Data is collected for evaluation criteria when seeking employment at Do Something We will only use your data for what was originally intended for purposes of seeking and gaining employment at Do Something. Your data will not be “recycled” for marketing emails or to sell to third parties. How long do we retain this data? Do Something will retain candidate profiles for 3 years. Annually on August 1, candidate profiles from 3+ years prior will be entirely deleted from our system. Is your data secure? Jazz HR maintains secure job board pages. Job boards will default to HTTPS. Additionally, if we were to terminate our contract with Jazz HR our candidate data is deleted from Jazz HR’s systems in accordance with Jazz HR’s Terms of Service. Powered by Jazz HR tt10Lqb T7f"
56,Data Scientist,data scientist,/rc/clk?jk=4f747323ce4a33f2&fccid=130c68c14ef48f45&vjs=3,"The Committee of 100 C100 is seeking a Data Scientist for a full time position. The Data Scientist will work with the President to use data and informational metrics to measure the impact of C100, to help further build a brand identity, to help improve internal decision-making processes, and to raise our public profile. This position will be responsible for proactively gathering data and producing data-driven insights and developing visualizations to advance the strategic position of C100. The ideal candidatefor this position has an exceptional eye for detail, expertise as a data scientist, and a solid understanding of popular data analysis tools. He/she will work closely across C100's programs and datasets to understand objectives and identify trends in various studies. Responsibilities Responsible for researching, collecting, analyzing, storing and creating data related to C100’s programs and its dual missions Responsible for overseeing data presented across surveys, studies in order to identify meaningful results Lead and build data architecture processes and databases Perform quantitative and qualitative data analysis Audit data on a regular basis to ensure data integrity and quality Assist in preparation of data reports, board presentations, publications, marketing collaterals, and other educational materials Responsible for the maintenance, back up and security of organizational data Must have strong communication skills including capability to explain complex technical topics Must have experience with relevant data analysis software programs Qualifications Bachelor’s degree in computer science, information technology or related field; Ph D degree preferred Commitment to the mission and values of C100 3-5+ years of experience as a data scientist or analyst; experience with a data/technology firm is preferred Extensive knowledge of data analysis software programs Good problem-solving and analytical skills, ability to resolve issues Exceptional writing, proofreading and copy editing skills Strong interpersonal and presentation skills Attention to detail, and a propensity to approach problems from creative angles Outstanding prioritization and project management skills Powered by Jazz HR PR2X Et Xlzi"
57,"Data Scientist, Analytics & Inference",data scientist,/rc/clk?jk=92ec5ef35e585a2a&fccid=b9d4e9eceb3ff4c0&vjs=3,"Hello, World! Codecademy has helped tens of millions of learners upgrade their careers, build meaningful projects and gain confidence in their skills with engaging, accessible, and flexible education on programming and data skills. We provide hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they've learned with Codecademy, and we're thrilled to be working to take that impact to the next level. Codecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. Almost a decade later, we are a rapidly growing, diverse team of 100+ headquartered in So Ho, NYC. We've raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more. If you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us! WHAT YOU'LL DO As a Data Scientist focused on Analytics and Experimentation, you will work on an impactful team to analyze our millions of learners. We capture terabytes of data on how users engage with our platform. As Codecademy continues our rapid growth, we want to build a data-informed culture that uses hypothesis testing, experimentation, and exploratory analysis to guide our decision-making process. You will join a small but growing team of Data Scientists. Our work is in high-demand from all corners of Codecademy. We work on a variety of problems and have a real impact on the business and product. If you have a proven background in data and you are excited about making code education accessible, we want to hear from you! Apply exploratory data analysis and causal inference to answer complex questions about our users. Collaborate across teams to help scope out analyses, through a combination of experimentation A/B testing and quantitative user research. Design experiments and evaluate results to test and iterate on new product ideas. Perform deep dives into our data to build understanding around our business. Work with our data science and engineering teams to maintain data integrity. Mentor and consult with a cross-functional team of data scientists, engineers, and product managers. WHAT YOU'LL NEED 3+ years of industry experience in a data science, analytics, or research role. You have strong data intuition and knowledge of using data science best practices to drive impact. Expert SQL we use Redshift. Able to write clean and efficient queries on massive datasets. Applied experience with statistical programming languages R or Python preferred. Understanding of statistical methods and when to use them hypothesis testing, experiment design, sampling . Strong written and verbal communicator. Comfortable working with loosely defined research problems. WHAT WILL MAKE YOU STAND OUT Background in experimentation and measurement. We're looking to streamline our experimentation reporting framework and would love your help. A workflow involving reproducible methods and version control Github, Docker. Experience automating dashboards with business intelligence tools Looker, Tableau. Passionate about teaching the world to code. Empathy for our learners, such as a background in education or past experience using our site. Interesting questions you might work on What are some different types of patterns in user behavior? What do we do when we are unable to conduct an experiment? Can we predict whether a learner will renew their subscription? How do we improve the relevance of our course recommendations? How can we scale our existing processes? experiment reporting framework, forecasting At Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of learners with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures. "
58,Principal Data Scientist,data scientist,/rc/clk?jk=5d45b1d4d816a1f4&fccid=be3b11aa573faee7&vjs=3,"It’s a new day in health care. Combining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all. We offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability. The Clinical Products data science team helps supercharge CVS-Aetna’s clinical programs to improve member health and lower costs. Given CVS-Aetna’s size, scope, and deep set of capacities, CVS-Aetna is an unparalleled platform to make a difference in healthcare in the US and beyond. The Clinical Products team has three main goals. First, CVS-Aetna is launching a series of clinical programs that use the resources of the combined CVS-Aetna, and our team delivers the advanced analytics to support those initiatives. Second, the team develops data-driven clinical recommendations that are delivered to members, their providers, and their caregivers. Finally, we build condition-specific models that help other parts of the company e. g. pricing better predict member care journeys. About the position. A Principal Data Scientist will join the Clinical Products team and will be responsible for leading and providing technical vision for one or more likely more projects. A Principal Data Scientist will build and lead a team that develops predictive models, works with clinical experts e. g. doctors to translate their subject matter expertise into machine learning models, sources data, and presents to technical and non-technical stakeholders. A Principal Data Scientist will also lead and mentor team members. Finally, Principal Data Scientist will need to help set the team’s coherent, actionable, and inspiring technical vision…and she will need to inspire the team to execute on that vision. Fundamental Components Lead team of 2-3 Data Scientists to develop and deliver models that inform CVS-Aetna’s clinical programs. Consults with business partners and clinical leaders to understand problems and goals, understand business value, and translate into business opportunities to be captured through predictive analytics solutions. Develops and/or uses advanced algorithms and statistical predictive models and determines analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes Demonstrates superior communication skills, and ability to develop and participate in presentations and consultations to business stakeholders on analytics results and solutions Supports deployment of insights across multiple channels, i. e. , web, mobile app, email, social, call center, Apple watch Motivates team members and probes into technical detail Contributes Subject Matter Expertise SME through interactions with team members, peers and managers to exchange complex information related to areas of specialization Mentors others, and champions 360-feedback across team Background Experience 7-10 or more years of progressively complex related experience. Demonstrates proficiency in all areas of mathematical analysis methods, machine learning, statistical analyses, and predictive modeling and advanced in-depth specialization in some areas. Solid understanding of health care industry, products, and systems. Deep knowledge of advanced analytics tools and languages to analyze large data sets from multiple data sources. Strong skills to effectively communicate and negotiate across the business and in the external health care environment Demonstrates strong ability to communicate technical concepts and implications to business partners. Strong organizational, management and leadership skills. Excellent analytical and problem solving skills. Comprehensive knowledge of health care industry, products, systems, business strategies and products Clinically oriented background or professional experience is a plus. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
59,"Data Scientist, Analytics- Instagram-Family Stories",data scientist,/rc/clk?jk=233dd1634d6132ae&fccid=1639254ea84748b5&vjs=3,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities we're just getting started. The goal of the Family Stories team is to build a healthy stories ecosystem across Facebook and Instagram. We do this by supporting teams that are currently building cross-app experiences, investing in shared architecture, and exploring new product experiences that enable greater liquidity of stories. The FB Family of apps includes 3 of the 4 biggest Stories platforms in the world, including the largest one, Instagram Stories. The Family Stories team at Instagram has been tasked with building a platform that fosters sharing between you and family, friends, and community, and giving everyone the best tools to share and consume Stories regardless of the app they’re using. Stories is one of the top product priorities for Facebook and Instagram, and given how fast the product has grown, there are a lot of unanswered product questions in the space. This role is responsible for testing and iterating on new entry points for Stories a good example here is being able to cross-post to Stories from other apps like Facebook and even Whats App , as well as owning key company growth channels like Stories notifications and Stories ranking. You are directly responsible for ensuring proper goal-ing, testing, forecasting and developing insights that identify the biggest opportunities for the IG community and drive product strategy. This is a highly cross functional role and this person will be driving many product efforts across IG as well as working with DS and product teams at Facebook, Messenger, and Whats App. This person will be the first Data Scientist joining the newly formed team. The ideal candidate can shape and own a narrative and interface directly with leadership. The Family Stories team will scale in H2 2020 and H1 2021, so this is an ideal role for an IC who wants to move into a lead and then into a manager position. Developing Goal Metrics and Evaluate Product Team Performance Investigating Ad Hoc Issues and Debugging Regressions Develop Testing Plans and Validate Statistically Accurate Results Reporting to Instagram and FB Inc Stakeholders and Coordination Across the Company Strategy Analyze Data Trends to Identify Opportunities and Drive Product Strategy Size opportunities to ensure product teams are prioritizing the most impactful work Ensure product strategy has actionable measurement plans by developing goal metrics and evaluating product team performance Determine ways to use data as a strategic asset in product development Technical Execution Ensure our data and analysis are reliable and rigorous across all of our products Partner with Data Engineering to ensure the right metrics are created and validate accuracy Oversee the building/maintaining of reports, dashboards, and metrics to monitor the performance of our products Ensure we have the appropriate logging, tables and dashboards to measure product success Develop new metrics with accuracy and stability Validate metric accuracy for internal and external reporting 5+ years of hands-on data science experience Expert knowledge of SQL Knowledge of one scientific computing language such as R, Python, or Julia Experience on working with multiple cross functional partners and influence decision making based on data experience initiating and drive projects to completion with minimal guidance Analytical toolbox in probability and causal inference, especially experimental design Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions , sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb. com. "
60,"VP, Data Scientist",data scientist,/rc/clk?jk=d73e55c1fdabbf77&fccid=8e6aaeeb0e50b9c5&vjs=3,"We create a safe space for communities to thrive. Open Web formerly Spot. IM is on a mission to increase the quality of conversations online. We build technologies that create thriving and engaged communities, reduce toxicity, and increase safety improving the quality of civil discourse and supporting independent journalism. Today, Open Web works with more than 700 top-tier publishers including News Corp, Verizon Media, CBS, Fox News, Hearst, Refinery 29 and more and hosts 100 million monthly active users. Founded in 2012, Open Web has over 100 employees between New York City and Tel Aviv. We are backed by world-class investors including Insight Partners, Index Ventures, Alta IR Capital, Scale Up, and Norma Investments. Together, we are empowering individuals to take action, and demand more from one another. We're crafting technology that naturally brings people together and fosters healthy discussion—so they have more dinner-table talks and less shouting matches online. It starts with us. Let's take the web back from the trolls together. Are you in? How You'll Contribute as our VP of Data Science What Fully own the establishment of a new department from scratch whose goal is to enter new markets, accelerate business transformation, and grow revenue by using our data resources in the form you think is most effective Develop the department by using your technical knowledge in data science and your adtech industry experience to shape the overall data strategy Drive the development of engineering and analytical tools, technology, and data acquisition Hire, manage, and retain a strong, centralized team to support you in executing the plans The Skills and Experience You Bring Extensive experience of at least 10+ years in leading data science teams and complex initiatives Previous experience in digital advertising, targeting, advertising optimization products, or other related ad-tech products Deep understanding of the ad-tech ecosystem Demonstrated ability to self-motivate, guide teams, and work in executives teams while being flexible within a fast-paced and ever changing environment Advantage for hands on experience with emerging statistical methods/techniques e. g. machine learning and/or artificial intelligence The Open Web Culture We offer a dynamic and unconventional work environment that spans from NYC to Tel Aviv, bringing together a diverse group of world class and high-caliber techies, wordsmiths, entrepreneurs, and creative thinkers. We empower every individual across our global team to be a catalyst for change and strive to create a work environment where you can have the utmost autonomy over your role and projects, from start to finish. If you want to join an innovative tech company where you can challenge yourself, have the freedom to own your work, and make a lasting impact, then you have a spot within our growing community! Open Web is committed to building diverse teams and upholding an equal employment workplace that is free from discrimination. We hire amazing individuals regardless of their race, color, ancestry, religion, sex, gender identity, national origin, sexual orientation, age, citizenship, marital status, pregnancy, medical conditions, genetic information, disability, or Veteran status. Build your skills with us as you build a better web for everyone. Join us! Also, we care about your privacy! Please take a moment to review Open Web's Privacy Practice"
61,"Data Scientist, Analytics - Recruiting Products",data scientist,/rc/clk?jk=39b732bc2b8cc94e&fccid=1639254ea84748b5&vjs=3,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities we're just getting started. The mission of the Recruiting Products team is to empower Facebook to hire the best talent at scale. With thousands of recruiters using our products, millions of candidates in our database, and an intense competition for top talent, the product team is focused on helping the business achieve our recruiting goals through better operational efficiency and improved collaboration throughout the process. We build the products that help Facebook find and attract other builders. We are looking for a Data Scientist to help the team understand how the work of recruiting gets done at Facebook and how our products can make this work even better. This role works closely with the product and engineering teams, as well as leaders of our recruiting business, to identify problems and develop sustainable solutions. Successful candidates for this role will have a background in a quantitative or technical field, will have experience working with large data sets, and will have experience in influencing decision making across different teams through data. Apply your expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how our users interact with both our consumer and business products. Define problems and opportunities in a complex or ambiguous area. Partner with Product, Engineering, and Business teams to solve problems and identify new levers to improve user experience. Monitoring key product metrics, understanding root causes of changes in metrics. Influencing product teams through presentation of data-based recommendations, and clearly communicating state of business, experiment results, etc. to product teams. Advanced degree in Computer Science, Engineering, Math/Statistics, Physics, Economics, or equivalent practical experience. 7+ years experience solving analytical problems using quantitative approaches. Experience in SQL or other programming languages. Understanding of statistics e. g. , hypothesis testing, regressions, ML systems . Experience communicating the results of analyses with product and leadership teams to influence strategy. Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions , sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb. com. "
62,Data Scientist/ML Engineer,data scientist,/rc/clk?jk=e943d78b0204baf7&fccid=b4df24bc350094d0&vjs=3,"We are a premier global financial services firm operating at the forefront of technical innovation. Out team is responsible for developing creative data science solutions and initiatives that protect highly sensitive data and information from cyber security threats. You Are Someone Who Loves solving complex and challenging problems with creative technical solutions Thrives in a fast-paced, mission-critical environment Think outside the box to identify & test new sources of information that unlock new business value Is able to apply the latest technologies in machine learning, deep learning, data mining, predictive analytics, anomaly detection, etc. to correlate big datasets and real-time events Able to work effectively in a cross-functional capacity to support a global team and real-time security alerts. Technical Requirements Minimum of a Bachelors Degree in Computer Science, Mathematics, or an equivalent quantitative field Masters or Ph. D preferred 3+ years experience in a hands-on experience in a Data Science, Machine Learning, or similar role Expertise with Python Tensor Flow, Py Torch, Keras, Sci-kit learn, Py Spark, etc. Ability to write complex SQL queries Core computer science fundamentals and ability to write clean, production level code. Experience working in a high volume, real-time, distributed computing environment Exposure to other data streaming or processing platforms/frameworks like Kafka, Spark, Hudi, Elastic, Terraform, Hadoop, etc. Experience in Cyber Security, Threat Detection, or Ethical Hacking is a plus"
63,Analytics Expert or Data Scientist,data scientist,/rc/clk?jk=651cc221e994b397&fccid=1c76c3a36f6c7557&vjs=3,"Your role Are you excited to work with a global Innovation team with diverse backgrounds and talents? Can you apply your technical skills to help quantify business risks? Do you enjoy the challenge of converting a business hypothesis into an analytics approach? We’re looking for a data analyst like that who can establish relationships with business stakeholders within GIA to solve business problems innovatively as well as meet the operational analytics needs of the organization, collaborate across Group Internal Audit GIA stakeholders, providing thought leadership, and driving day-to-day implementation of our innovation strategy, advise GIA stakeholders on analytical options to address their specific needs, including discussing potential analytics and automation solutions to problems, associated costs and trade-offs, and recommendations, and providing technical mentorship to auditors across all levels, design, coordinate, and implement analytics and automation solutions to support audit and innovation initiatives, display strong communication, interpersonal, and leadership ability coupled with effective problem solving, conceptual thinking, quantitative and analytical skills Your team You will be working in the Group Internal Audit Innovation team. GIA’s role, as an independent function, is to support UBS in achieving its strategic, operational, financial and compliance objectives. We do this by assessing key processes as well as governance, risk management and the control environment within all business divisions and Corporate Center functions globally. We are independent in our work and report directly to the Chairman of the Board of Directors and the Audit Committee. We are a talent powerhouse that attracts and develops the best people by driving career growth in and outside the department. As an Data Analyst you will be a member of the global Data Analytics & Automation team, which works collaboratively with all GIA teams and management in different aspects of design and delivery of innovative and efficient solutions. In that role, you will gain insight into a wide variety of GIA's processes and will be able to influence their effectiveness. Your expertise You have Bachelor’s degree and ideally 3+ years of relevant work experience as data analyst/scientist, reporting analyst or automation analyst within the financial industry, strong analytics skills including tools such as Python, Oracle SQL, Alteryx, Tableau or equivalents experience in applying advanced analytics / data science / statistical techniques to solve business problems, such as practical applications of Natural Language Processing and Generation, Machine Learning, Clustering, and Regression analyses excellent project management skills, with the ability to manage multiple / concurrent projects, an interest and prior experience in banking operations and regulations as well a quantitative approach to problem solving an excellent verbal and written command of English with the ability to effectively communicate complex and technical topics with non-technical stakeholders LI-UBS #Dice Tech Your colleagues About us Expert advice. Wealth management. Investment banking. Asset management. Retail banking in Switzerland. And all the support functions. That's what we do. And we do it for private and institutional clients as well as corporations around the world. We are about 60,000 employees in all major financial centers, in more than 50 countries. Do you want to be one of us? Join us We're a truly global, collaborative and friendly group of people. Having a diverse, inclusive and respectful workplace is important to us. And we support your career development, internal mobility and work-life balance. If this sounds interesting, apply now. Disclaimer / Policy Statements UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce. Apply now Save"
64,"Senior Data Scientist, Analytics & Inference",data scientist,/rc/clk?jk=8f623d72576bc7d0&fccid=b9d4e9eceb3ff4c0&vjs=3,"Hello, World! Codecademy has helped tens of millions of learners upgrade their careers, build meaningful projects and gain confidence in their skills with engaging, accessible, and flexible education on programming and data skills. We provide hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they've learned with Codecademy, and we're thrilled to be working to take that impact to the next level. Codecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. Almost a decade later, we are a rapidly growing, diverse team of 100+ headquartered in So Ho, NYC. We've raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more. If you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us! WHAT YOU'LL DO As a Senior Data Scientist focused on Analytics and Experimentation, you will work on an impactful team to analyze our millions of learners. We capture terabytes of data on how users engage with our platform. As Codecademy continues our rapid growth, we want to build a data-informed culture that uses hypothesis testing, experimentation, and exploratory analysis to guide our decision-making process. You will join a small but growing team of Data Scientists. Our work is in high-demand from all corners of Codecademy. We work on a variety of problems and have a real impact on the business and product. If you have a proven background in data and you are excited about making code education accessible, we want to hear from you! Apply exploratory data analysis and causal inference to answer complex questions about our users. Collaborate across teams to help scope out analyses, through a combination of experimentation A/B testing and quantitative user research. Design experiments and evaluate results to test and iterate on new product ideas. Perform deep dives into our data to build understanding around our business. Work with our data science and engineering teams to maintain data integrity. Mentor and consult with a cross-functional team of data scientists, engineers, and product managers. WHAT YOU'LL NEED 4+ years of industry experience in a data science, analytics, or research role. You have strong data intuition and knowledge of using data science best practices to drive impact. Expert SQL we use Redshift. Able to write clean and efficient queries on massive datasets. Applied experience with statistical programming languages R or Python preferred. Understanding of statistical methods and when to use them hypothesis testing, experiment design, sampling . Strong written and verbal communicator. Comfortable working with loosely defined research problems. WHAT WILL MAKE YOU STAND OUT Background in experimentation and measurement. We're looking to streamline our experimentation reporting framework and would love your help. A workflow involving reproducible methods and version control Github, Docker. Experience automating dashboards with business intelligence tools Looker, Tableau. Passionate about teaching the world to code. Empathy for our learners, such as a background in education or past experience using our site. INTERESTING QUESTIONS YOU MIGHT WORK ON What are some different types of patterns in user behavior? What do we do when we are unable to conduct an experiment? Can we predict whether a learner will renew their subscription? How do we improve the relevance of our course recommendations? How can we scale our existing processes? experiment reporting framework, forecasting At Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of learners with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures. "
65,"Marketing Data Scientist, Professional Services",data scientist,/rc/clk?jk=2e4a82a92ec8d0ac&fccid=a5b4499d9e91a5c6&vjs=3,"Note By applying to this position your application is automatically submitted to the following locations New York, NY, USA; Chicago, IL, USA; Mountain View, CA, USA Minimum qualifications Bachelor's degree in Statistics, Computer Science, Engineering, or Mathematics, related quantitative discipline, or equivalent practical experience. Experience in data science with a focus on marketing analytics, statistical modeling, machine learning, forecasting and optimization. Advanced experience in Python and/or R, and advanced experience in data management systems. Preferred qualifications Ph D in Statistics, Computer Science, Engineering, or Mathematics. 4 years of experience in data science with a focus on marketing analytics, statistical modeling, machine learning, forecasting and optimization. Experience with big data and cloud platforms to deploy large-scale data science solutions. Experience working in industries such as retail, financial services, telecommunications and automotive including in a client-facing consulting or advisory capacity , and the ability to navigate complex business organizations and manage multiple cross-functional stakeholders. Understanding of media and the statistical algorithms typically used in marketing analytics. About the job g Tech’s Professional Services team takes a creative, collaborative, and customer-centric approach to provide foundational services and forward-looking business solutions to top advertiser and publisher customers. Through technical implementation, optimization, and key solutions, g Tech Professional Services helps customers attain their business goals while building long-term capabilities. The g Tech Professional Services g PS AMER team is a team of solution-oriented trusted advisors supporting advertisers in the Americas region. Our g PS team is looking for a Data Scientist with a deep technical and product expertise, combined with an understanding of customer’s needs and goals to solve their biggest business challenges allowing them to grow. You will use your experience in working closely with large companies and produce innovative and actionable quantitative models and analyses to address the challenges of marketing effectiveness and business growth. Google creates products and services that make the world a better place, and g Tech’s role is to help bring them to life. Our teams of trusted advisors support customers globally. Our solutions are rooted in our technical skill, product expertise, and a thorough understanding of our customers’ complex needs. Whether the answer is a bespoke solution to solve a unique problem, or a new tool that can scale across Google, everything we do aims to ensure our customers benefit from the full potential of Google products. To learn more about g Tech, check out our video. Responsibilities Lead analytics aspects of client engagements in the area of marketing effectiveness and marketing portfolio management with deep knowledge of modeling. Collaborate with clients to build an end to end machine learning framework. Engage various stakeholders, assess data readiness. Be able to scale a proof of concept to a larger solution. Work with client and internal teams to translate data and model results into tactical and strategic insights that are clear, complete, accurate, relevant, understandable and applicable to decision making and needs for varying client audiences. Co-present to and work with clients to integrate recommendations into business processes. Collaborate with Product/Engineering teams to increase and optimize capabilities, employing methods which create opportunities for scale, proactively helping to drive innovation. Develop comprehensive understanding of Google data structures, and metrics, advocating for product changes where needed. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form. "
66,Senior Data Scientist,data scientist,/rc/clk?jk=59dad2d7aefde1b3&fccid=12e41f7040f168e8&vjs=3,"About The Opportunity Grubhub is dedicated to connecting hungry diners with our wide network of restaurants across the country. Our innovative technology, easy-to-use platforms and streamlined delivery capabilities make us an industry leader today, and in the future of online food ordering. We strive to create a workplace that reflects the diversity of our customers and the communities we serve. When you join our team, you become part of a community that works together to innovate, solve problems, take risks, grow, work hard and have a ton of fun in the process! Why Work For Us We have a fast-paced environment and that is what our teams thrive on. Grubhub believes in empowering people and offering opportunities for development, as well as professional growth. We value strong, positive relationships in all areas with each other, our customers and our greater community. Want to be a part of a team of diverse collaborators in an authentically fun culture? If so, we want to talk to you and hear what’s your favorite restaurant for food delivery! More About the Role Grubhub is looking for a data scientist to join the Pricing team. As a part of Pricing, you’ll be a member of a small team of data scientists and engineers who shape and optimize how we charge our diners, shaping hundreds of millions in revenue annually. You will work closely both with financial stakeholders as well as engineers to ship models that make Grubhub more efficient with the way in which it charges customers. You’ll construct models and A/B tests as well as write code to improve our modeling capabilities. Some Challenges Youâll Tackle Prototype and ship large-scale models that help drive profit or order volume Conduct A/B experiments to validate the effectiveness of these models Work closely with financial stakeholders to construct models that drive business goals Leverage big data tools to incorporate new data sources into modeling pipelines Work closely with data scientists and backend engineers to implement and maintain your changes You Should Have MS with 5+ years of relevant industry experience or Ph D in computer science, engineering, statistics, machine learning, mathematics, or in another quantitative field Experience shipping changes to a production environment Comfortable with using git, basic Linux commands, and object-oriented programming languages Scala, Java, Python Experience performing analysis with large-scale SQL tooling Presto, Big Query, etc. Familiarity with basic AWS tools like EC2 and S3 Superior communication and data presentation skills Effective working in a fast-paced team environment Preferred Qualifications Demonstrated ability to work on cross-functional teams Experience making high-impact decisions multiple millions of dollars Ability to comprehend and debug complex systems that might cross team and tool boundaries across the company And Of Course, Perks! Flexible PTO. Grubhub employees are provided a generous amount of time to recharge their batteries. Health and Wellness. We provide programs that support your overall well-being such as generous medical benefits, employee network groups, company-wide fitness challenges, and a comfortable and casual workplace! We also support our parents by offering 8 weeks of paid parent bonding time, a 4-week returnship program, and 6-8 weeks paid medical leave. Learning and Career Growth. Your personal and professional development is a priority at Grubhub. From day one, we empower you to lead and be an active participant in your career growth. We provide continuous learning opportunities, training, and coaching and mentorship programs. Meal Perks. Who’s ready for some lunch? We provide our employees with a weekly Grubhub credit to enjoy and support local restaurants. We also offer company-wide meals several times a year to bring our Grubhub family together. Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives. Some of our popular past events include Wing-eating contests, Grubtoberfest, 5k Runs, Bring Your Child to Work Day, regular happy hours, and more! Social Impact. We believe in the importance of serving the communities that support our business. In addition, employees are given paid time off each year to support the causes that are important to them. Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. The EEO is the Law poster is available here DOL Poster. If you are applying for a job in the U. S. and need a reasonable accommodation for any part of the employment process, please send an e-mail to Talent Acquisition@grubhub. com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. CA Privacy Notice If you are a resident of the State of California and would like a copy of our CA privacy notice, please email privacy@grubhub. com. "
67,Senior Data Scientist,data scientist,/rc/clk?jk=628541a9099b6e7e&fccid=39db89979407b486&vjs=3,"Marketing Statement Metro Plus Health Plan provides the highest quality healthcare services to residents of Bronx, Brooklyn, Manhattan, Queens and Staten Island through a comprehensive list of products, including, but not limited to, New York State Medicaid Managed Care, Medicare, Child Health Plus, Exchange, Partnership in Care, Metro Plus Gold, Essential Plan, etc. As a wholly-owned subsidiary of NYC Health + Hospitals, the largest public health system in the United States, Metro Plus’ network includes over 27,000 primary care providers, specialists and participating clinics. For more than 30 years, Metro Plus has been committed to building strong relationships with its members and providers to enable New Yorkers to live their healthiest life. Position Overview The desired Senior Data Scientist candidate will lead the design, development, and management of advanced analytics solutions that innovate and help us make smarter decisions to deliver even better care to our members. This includes data exploration / data mining, model building, performance & program evaluation, and related testing and recommendations. S/he will help drive several large, cross-functional initiatives within various departments, Metro Plus at large, and other initiatives in partnership with NYC Health + Hospitals, and Home and Community Based Services. Under the Chief administrative Officer and the Head of Health Analytics, the Senior Data Scientist is expected to build / implement data science solutions at scale, and to provide thought leadership & give presentations as a Subject Matter Expert in advanced statistical techniques and mathematical analyses. S/he needs to be comfortable and experienced with leading highly complex projects involving multiple internal & external resources and tasks. The successful candidate can work both independently with little supervision or guidance & collaboratively with many stakeholders and SME’s, leading advanced data analysis and data modeling projects, from project design & iteration w/ internal clients, to the reception / processing of data and analyses and or modeling, and to the delivery of final reports/presentations, communication of results and implementation support. Teamwork is essential as this role requires collaborating with various subject matter experts, management, and other industry leaders. This position requires an analytical-minded individual who enjoys and thrives working in a fast-paced, nebulous, challenging, but rewarding environment. Comprehensive knowledge of healthcare insurance industry, products, systems, business strategies and products is highly desired. Strong understanding of Quality, Care and Utilization Management and Measurement reporting is also highly preferred, along with experience working with healthcare claims data. Job Description Based on project requirements, determine, extract and aggregate appropiate data sets. Analyze healthcare data and using quantitative methods to create actionable and timely information across the Medical Management teams, and up and down the Metro Plus business hierarchy. Create meaningful data visualizations & presentations to communicate findings and relate them back to how they create business impact. Develop sophisticated algorithms / predictive models to evaluate scenarios to make predictions of future outcomes, and or evaluate future impact of changes Apply data mining techniques to create statistical models and solutions detecting patterns, relationships across available data sets e. g. NHWS, Electronic Health Records EHR , claims data, or these data sets linked together using machine learning methodologies & tools. Build high performing predictive systems that empower cross-functional Medical Management teams and Metro Plus at large to more effectively, efficiently target care & utilization management initiatives across our membership population and provider networks Lead the design, development, and delivery of proactive linked data studies w/ internal teams and SME’s; direct or perform these linked data studies and build out studies using new linked data sets. Drive conversations with internal stakeholders and leaders on product design, data specification, model implementations Provide thought leadership on advanced statistical / mathematical analyses and related strategic insights to various internal stakeholders and key decision makers. Tests new statistical analysis methods, software and data sources for continual improvement of quantitative solutions. Drive proof of concept tests of new data, software and technologies. Collaborate with other Health Analytics team members and other departments & SME’s to understand company needs and devise possible solutions including data integrity / data governance solutions to remove joint obstacles to timely and high-quality data analytics Optimize joint development efforts through appropriate database use and project design Assures compliance with regulatory and privacy requirements during design and implementation of modeling and analysis projects. Communicate, as needed, with the NYS or NYC Departments of Health on relevant analyses Perform other duties as required Minimum Qualifications Master’s or Ph D degree in Statistics, Biostatistics, Math, Computer Science, Health Economics, Epidemiology, Healthcare Informatics, or Data Science. 3+ years’ of workingl experience with healthcare claims data. 5+ years related experience with healthcare data analytics. Strong practical experience with SQL and Pthon is required; experiences in R, Hadoop, No SQL is a plus. Strong expertise in statistical modeling techniqures such as linear regression, logistic regression, survival analysis, GLM, tree models Random Forests and GBM , cluster analysis, prinicpal components, feature creation, and validation. Strong expetise in regularization techniques Ridge, Lasso, elastic nets , variable selection techniques, feature creation trasnformation, binning, high level categorical reduction, etc. and validation hold-outs, CV, bootstrap . Experience with business intelligence platforms e. g. Tableau, Nicro Strategy, SAP is a plus. Excellent problem solving skills, critical thinking and conceptual thinking abilities Strong organizational, management and leadership skills Strong ability to communicate technical, statistical concepts and implications to non-technical teams and leaders across the business and in the external health care environment. Excellent pattern recognition and predictive modeling skills Excellent communication skills Extensive knowledge on health care industry, products, systems, business strategies and products and experience in healthcare industry is highly desired. Ability to work independentley and collabortively. Professional Competencies Integrity and Trust Customer Focus Functional/Technical skills Written/Oral Communication"
68,Quantitative Analyst,quantitative analyst,/rc/clk?jk=8e50c6c0777a983f&fccid=537b899e30af3338&vjs=3,"What is the opportunity? An Associate / Quantitative Analyst in the Central Funding / Securitized Products Analytics Group will focus on developing and implementing new, and maintaining and / or enhancing the existing models used for security financing and funding businesses What will you do? Retool the existing models against the backdrop of the IBOR transition, and gradually broaden them to cover other analytics needs Ensure that the firm’s risk reporting and capital usage computation framework for CFG is suitable for the current or future market conditions and regulations. Apply quantitative skills to design, implement, test, and roll out the valuation and risk models in the current analytical environment Build the benchmark or challenger models where deemed necessary Support the CFG business by assisting the traders, risk managers, and product controllers in understanding the models and interpreting the model outputs Support the preparation of model documentation and validation submissions, as well as tracking the model performance, per the internal policies and regulatory guidelines Review and comply with firm policies applicable to CFG business activities Escalate operational risk loss events, control deficiencies and risks that you identify to your line manager and the relevant risk and control functions on a timely basis. What do you need to succeed? Must have Post-grad degree in one of quantitative areas such as mathematics, physics, computer sciences, or engineering Solid knowledge of financial instruments and derivatives, such as bonds, IR swaps, futures, and options, as well as related pricing and risk management models. Hand-on experience on interest rate models is preferable Proficient in one programming language such as Python preferable , C#, or C++. VBA is a good to have Good communication skills in both verbal and writing Excellent analytical skills and a faster and self-motivated learner Nice to Have Broad knowledge of financial markets and regulations is good to have CFA or FRM What’s in it for you? We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual. A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation Leaders who support your development through coaching and managing opportunities Ability to make a difference and lasting impact Work in a dynamic, collaborative, progressive, and high-performing team Join our Talent Community Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you. Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc. com/careers. JOB SUMMARY City New York Address 200 Vesey Street Work Hours/Week 40 Work Environment Trading Floor Employment Type Permanent Career Level Experienced Hire/Professional Pay Type Salaried Required Travel % 0-25 Exempt/Non-Exempt Exempt People Manager No Application Deadline 11/30/2020 Platform Capital Markets Req ID 264057 Ad Code s "
69,FX Quantitative Analyst,quantitative analyst,/rc/clk?jk=0d595518a0dc60e7&fccid=5bcd1ef0a7f4fb99&vjs=3,"Promote Citi’s quantitative strategy brand through client interactions, thought leadership publications and trade idea pitches. Working closely with sales and Citi’s internal stakeholders, create sustainable and scalable currency management solutions for institutional clients. Key Responsibilities 1. Build close partnerships with FXLM Institutional Sales and Citi Velocity sales in order to promote Citi’s market leading quantitative strategy brand to our key institutional clients. 2. Producing client-driven active and passive currency risk management solutions, mainly for US, Canadian, and Latin American investors. The solutions will need to demonstrate original thinking applied to complex problems, including intelligent use of options and systematic market timing of hedge trades; 3. Creating and delivering effective presentations to clients, explaining and selling the hedging solutions or trading strategies; 4. Participating in the team’s efforts to expand and review the set of tradable currency trading strategies and portfolio allocation methods. Coming up with original ideas for new strategies and creating strategy back-tests; 5. Writing client-facing reports on new trading strategies and generic hedging solutions, writing market commentary on existing strategies and quantitative strategy tools. This is a high visibility front-office job that will offer opportunities to interact with Citi’s institutional investor clients and sales force. There will also be opportunities to work with other teams within Citi such as Investor Services, Strategy, Trading, Corporate Solutions and Intelligent Orders and contribute and learn from those interactions. Knowledge/Experience 2+ year of experience in a quantitative analysis role, preferably with direct experience developing systematic trading strategies;A good understanding of macroeconomics and drivers of exchange rates;A good understanding of the uses of financial derivatives, preferably currency forwards, swaps, and options. Skills Degree in a quantitative subject, economics or finance;Fluent and confident communication skills;Good coding skills Python, R, SQL, VBA ;Ability to do advanced quantitative analysis on large datasets. Grade All Job Level All Job Functions All Job Level All Job Functions US Time Type Full time Citi is an equal opportunity and affirmative action employer. Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity. Citigroup Inc. and its subsidiaries ""Citi” invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity . To view the ""EEO is the Law"" poster . To view the EEO is the Law Supplement . To view the EEO Policy Statement . To view the Pay Transparency Posting . "
70,Quantitative Analyst - NA,quantitative analyst,/rc/clk?jk=f55d1a2cd26490ae&fccid=ac6101b8c34bf398&vjs=3,"Quantitative Analyst, NA Quantifi is a leading provider of integrated risk, analytics and trading solutions for the global financial markets. Our clients include some of the largest global financial institutions including 5 of the 6 largest banks, 2 of the 3 largest asset managers, leading hedge funds, pension funds, insurers, brokers, clearing members and corporates. Our unique culture is defined by the ample opportunities available for personal growth. Team members are encouraged to learn, develop, discover and realise ambitions that will shape their professional lives. Our senior team includes individuals with years of experience in top-tier financial service firms including Citigroup, Goldman Sachs, JP Morgan and Bear Stearns. Our researchers actively write and publish papers and speak at industry events. The role comprises heavy research and development to keep pace with our diverse client base made up of the most sophisticated market participants. We offer exposure to every aspect of capital markets technology and research are you ready to help shape the financial technology landscape? Our ideal candidate is someone with a strong desire to learn and explore different aspects of financial research. We are seeking a talented quantitative analyst who enjoys being on the leading edge of derivatives modeling. This opportunity will provide the right individual the ability to learn from some of the top people in the finance industry. Quantifi has offices in London, New York, New Jersey, and Sydney. Responsibilities Designing, development and testing of new derivative pricing models, focusing on Python interfaces Working closely with Quantifi's product management teams Portal content assist with documentation and code samples Skills and Requirements Ph D in Computer Science, Mathematics or related quantitative field Strong programming skills in Python, C++ or similar object-oriented language essential Familiarity with compiler technique, a plus Experience working in quantitative research or computational finance, a plus Good communication skills Ability to work in a collaborative team environment Employee Benefits Health, Dental, Life, and ADD Insurance AFLAC 401k Plan Flexible Spending Plans Transit Check PTO and Holidays Annual Bonuses Long Term Disability 529 Plans"
71,Agency RMBS Quantitative Analyst,quantitative analyst,/rc/clk?jk=6354f2f7ed613529&fccid=1c2a61588e44123b&vjs=3,"Key Responsibilities Working closely with other Researchers, Data Operations, Development, and Support, Client Service, Product Management, the following lists the key areas of responsibility for this role Development and support of mortgage data transformations for the benefit of Analytics and Index business. Work with Production teams as a second line of defence. Responsibility for building monitoring system to maintain integrity of data at all levels. Own and develop custom data transformations for the benefit of wider-scope Research. Oversee timeliness and quality of data updates. Looking forward, participate in development of data-driven and behavioural collateral models. Key Behaviours Integrity Has the sustained drive and energy to deliver support service to time and quality. Partnership Values practical aspect of model development, quality of code, scalability, etc alongside with the quality of model itself. Successfully builds coalitions across different groups. Innovation Open to and willingly adopts new processes / approaches / ways of working. Seeks information/inputs from colleagues/clients. Excellence Willingly puts in the effort to ensure activities completed on time and to the quality required. Pro-active and demonstrates initiative. Prioritises activities according to business and operational need. Analysis and problem solving. Analyses issues to identify the most appropriate solutions. Utilises all available resources and toolsets to investigate and resolve problems. Candidate Profile / Key Skills Attitude of ownership and responsibility. Advanced academic degree MS level in a quantitative or technical field, such as Computer Science, Operations Research, Data Science, Engineering, Science, or Mathematics. Strong command of Python programming language is required. Experience with cloud solutions, in particular AWS S3/RDS, Glue, Sage Maker, Athena, etc is a strong plus. Industry experience with Mortgage Collateral data is a strong plus. Understanding of Securtized Products sector, trading practices and trends is a plus. Good understanding of Fixed Income models and analytics, including interest-rate, and credit is a plus. Team player, ability to learn new technology and models quickly and efficiently. Understanding of mortgage analytics is a significant plus. People are at the heart of what we do and drive the success of our business. Our colleagues thrive personally and professionally through our shared values of Integrity, Partnership, Innovation and Excellence are at the core of our culture. We embrace diversity and actively seek to attract people with unique backgrounds and perspectives. We are always looking at ways to become more agile so we meet the needs of our teams and customers. We believe that an inclusive collaborative workplace is pivotal to our success and supports the potential and growth of all colleagues at LSEG. A career with London Stock Exchange Group offers you the opportunity to be at the centre of the financial community. As well as competitive salaries and a range of attractive benefits, we maximise each employee’s potential through personal development plans, training, coaching and mentoring. Please take a moment to read this privacy notice carefully, as it describes what personal information the London Stock Exchange Group “We” may hold about you, what it’s used for, and how it’s obtained. If you would like this information to be removed from the London Stock Exchange Group HR database, please contact workday@lseg. com . If you choose to have your information removed, you will be removed as a candidate and we will not be able to progress your application for opportunities at the London Stock Exchange Group. "
72,Quantitative Analyst,quantitative analyst,/rc/clk?jk=3a5da06e2ce06a3d&fccid=4d7e317a90a915fd&vjs=3,"As part of the application process, a candidate account is required to log in and view application s . Please be sure to check email regularly for information regarding our employment process. Voya Investment Management Voya IM is the asset management business of Voya Financial, a Fortune 500 company with over 6,000 employees seeking to help clients plan, invest and protect their savings. Voya IM manages approximately $220 billion in assets across Fixed Income, Senior Loans, Equities, Multi-Asset Strategies & Solutions, Private Equity, and Real Assets. 1 Drawing on over 40 years of experience and the expertise of 250+ investment professionals, Voya IM’s capabilities span traditional products and solutions as well as those that cannot be easily replicated by an index. Voya IM’s award-winning culture is deeply rooted in a client-centric approach to helping investors meet their goals — from insurance companies, corporate and public pension funds, sovereign wealth funds, endowments and foundations, and consultants to intermediaries, and individual investors. Reliability is why our clients hire us and it is why they trust us to navigate the path ahead. Voya Investment Management Voya IM is the asset management business of Voya Financial, a Fortune 500 company with over 6,000 employees seeking to help clients plan, invest and protect their savings. Voya IM manages approximately $214 billion in assets across Fixed Income, Senior Loans, Equities, Multi-Asset Strategies & Solutions, Private Equity, and Real Assets. Drawing on over 40 years of experience and the expertise of 250+ investment professionals, Voya IM’s capabilities span traditional products and solutions as well as those that cannot be easily replicated by an index. Voya IM’s award-winning culture is deeply rooted in a client-centric approach to helping investors meet their goals — from insurance companies, corporate and public pension funds, sovereign wealth funds, endowments and foundations, and consultants to intermediaries, and individual investors. Reliability is why our clients hire us and it is why they trust us to navigate the path ahead. Voya’s quantitative equity research team develops factor-driven stock selection models for systematic investment strategies and in support of the fundamental platform. The team is directly and indirectly responsible for a combined total of approximately $31 billion in AUM as of 6/30/2020 . Key Responsibilities The analyst will work within a team of quantitative researchers and portfolio managers to assist in developing, implementing, and enhancing quantitative models and analytics used to support the Equities platform at Voya Investment Management. Our team sits at the intersection of data science and investment analysis, working closely with fundamental sector analysts for cross-fertilization of ideas. We develop investment insights through exploring datasets and analytical methods so as to systematically identify alpha opportunities in different segments of the equity market. Research areas include identifying new alpha factors, model estimation linear and nonlinear , portfolio construction, and risk management. The analyst will be expected to contribute to the shared codebase and toolkit of the research team. Combining newly available data with innovations in machine learning is key to capturing fundamental investment intuition and insights from behavioral finance in more nuanced and effective ways. This role offers a unique opportunity for the right candidate to gain in-depth understanding of “quantamental” investing and hands-on experience with the latest data science techniques applied thereto. Responsibilities include Work on innovative research projects aimed at advancing our proprietary multi-factor models through the addition of new alpha signals or better model estimation techniques, both linear and nonlinear. Explore enhancements to portfolio optimization and risk management to improve the risk-adjusted net returns of our systematic investment strategies or target specific investment outcomes for our clients. Evaluate new data sources from a range of sources for their potential to generate alpha. Help maintain and advance the team’s shared codebase, data repositories, and cloud-based technology stack. Contribute to new product development in collaboration with our client-facing and product teams. Contribute to thought leadership articles to enhance awareness of our team’s investment philosophy and capabilities. Qualifications Bachelor’s degree in a quantitative discipline such as Financial Engineering, Operations Research, Mathematics, Computer Science, Statistics, etc. Advanced degree preferred. CFA a plus. 5 years of relevant work experience in applied quantitative research, preferably investment management. Strong analytical and mathematical skills. Excellent working knowledge of econometrics and statistics. Familiarity with financial statement analysis is a plus. An understanding of a variety of machine learning techniques clustering, decision tree learning, boosting & stacking, artificial neural networks, etc. , their practical advantages and short-comings, as well as approaches to make the resultant models interpretable. Hands-on experience across broad range of modern analytic and data tools, particularly Python; numpy/pandas; machine learning packages such as XG Boost, Py Torch, Tensor Flow, Keras; and distributed data/computing tools such as Hadoop, Spark, and SQL. Solid understanding of relational and non-relational databases. Experience using one of the Big 3 cloud-based computing services Azure, AWS, Google Cloud is a plus. Experience with financial databases such as Compustat, Worldscope, Factset, Clarifi/Capital IQ, Axioma, Barra, Bloomberg is a plus. Experience in factor-based equity investing including ESG metrics is a plus. Attention to detail, curious, and self-motivated. Ability to work independently and collaboratively within the broader research team, as well as prioritize tasks. Excellent problem solving, interpersonal and communication skills. Ability to translate quantitative insights into actionable process improvements Have a passion for data science and financial markets, the curiosity to master new technologies and techniques, and a desire to drive transformational change. #LI-JS1 Critical Skills At Voya, we have identified the following critical skills which are key to success in our culture Customer Focused Passionate drive to delight our customers and offer unique solutions that deliver on their expectations. Critical Thinking Thoughtful process of analyzing data and problem solving data to reach a well-reasoned solution. Team Mentality Partnering effectively to drive our culture and execute on our common goals. Business Acumen Appreciation and understanding of the financial services industry in order to make sound business decisions. Learning Agility Openness to new ways of thinking and acquiring new skills to retain a competitive advantage. "
73,Senior Quantitative Analyst,quantitative analyst,/company/CRISIL-LIMITED/jobs/Senior-Quantitative-Analyst-aae46d74a5cc7e60?fccid=07960deccb500f33&vjs=3,"Job title Quantitative Analyst / Senior Quantitative Analyst / Lead Analyst Location New York, USA Experience6+ years Job Duties Role will require closely working with the Model Development/Model Validation/Risk Management team at one of the largest US financial firms. n Key responsibilities include helping the bank with various aspects of the Basel implementation including managing the project plan for Risk Analytics models development and evaluating/testing monthly RWA results for implementation accuracyn The candidate would be working on Basel II/III’s related projects PD/LGD/EAD Models n Understanding modelling and validation methodologiesn Developing and executing test plans to make sure the compliance with regulatory guidelines, analysing model weaknesses, benchmarking to external vendor models, documenting and reporting the results. n Qualification Masters /MBA degree in a quantitative discipline – Mathematics, Statistics, Economics, Physics, Financial Engineering, Finance. Ph D will be a plus. Skills Requiredn Good understanding of Financial Markets and Banking in the area of Risk Management; familiarity with multiple asset classes will be a plus. n Experience in model development, model validation or stress-testing Should have good understanding of regulations such as BASEL 2/2. 5/3, Dodd Frank Act. CCAR etc and its implication for banksn Good Analytical/Numerical experience demonstrated by cutting edge quantitative/statisticalanalysis projects and experience with statistical packages such as MATLAB/SAS/R will be a plusn Excellent communication skills, both written and oral About CRISIL GR&A CRISIL Global Research & Analytics GR&A is the largest and top-ranked provider of high end research and analytics services to the world's leading commercial and investment banks, insurance companies, corporations, consulting firms, private equity firms and asset management firms. CRISIL GR&A operates from research centers in Argentina, China, India and Poland, providing research support across several time zones and in multiple languages to global organizations. It has deep expertise in the areas of equity and fixed income research covering global economies, 150 global sectors and over 3000 global companies , credit analysis, exotic derivatives valuation, structured finance, risk modeling and management, actuarial analysis and business intelligence. CRISIL GR&A includes Irevna, Pipal Research and Coalition, firms which were acquired by CRISIL. For more information, please visit www. irevna. com, www. pipalresearch. com and www. coalition. com Job Type Full-time Experience Basel II/III experience do you have 2 years Required model validation OR model development 2 years Required Risk Management OR Risk Management experience do you have? 2 years Required "
74,AVP Quantitative Analyst,quantitative analyst,/rc/clk?jk=000a05cea1ea2471&fccid=5bcd1ef0a7f4fb99&vjs=3,"The Quantitative Analyst is a seasoned professional role. Applies in-depth disciplinary knowledge, contributing to the development of new techniques and the improvement of processes and work-flow for the area or function. Integrates subject matter and industry expertise within a defined area. Requires in-depth understanding of how areas collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the function and overall business. Evaluates moderately complex and variable issues with substantial potential impact, where development of an approach/taking of an action involves weighing various alternatives and balancing potentially conflicting situations using multiple sources of information. Requires good analytical skills in order to filter, prioritize and validate potentially complex and dynamic material from multiple sources. Strong communication and diplomacy skills are required. Regularly assumes informal/formal leadership role within teams. Involved in coaching and training of new recruits. Significant impact in terms of project size, geography, etc. by influencing decisions through advice, counsel and/or facilitating services to others in area of specialization. Work and performance of all teams in the area are directly affected by the performance of the individual. Responsibilities Develop analytics libraries used for pricing and risk-management Create, implement, and support quantitative models for the trading business leveraging a wide variety of mathematical and computer science methods and tools including hardware acceleration, advanced calculus, C++ including STL, C#, . NET, Java, object oriented software design, Python, kdb, Structured Query Language SQL , mathematical finance/ programming and statistics and probability Develop pricing models using numerical techniques for valuation including Monte Carlo Methods and partial differential equation solvers Collaborate closely with Traders, Structurers, and technology professionals Work in close partnership with control functions such as Legal, Compliance, Market and Credit Risk, Audit, Finance in order to ensure appropriate governance and control infrastructure Build a culture of responsible finance, good governance and supervision, expense discipline and ethics Appropriately assess risk/reward of transactions when making business decisions; and ensure that all team members understand the need to do the same, demonstrating proper consideration for the firm’s reputation. Be familiar with and adhere to Citi’s Code of Conduct and the Plan of Supervision for Global Markets and Securities Services; and ensure that all team members understand the need to do the same Adhere to all policies and procedures as defined by your role which will be communicated to you Obtain and maintain all registrations/licenses which are required for your role, within the appropriate timeframe Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency. Qualifications 5-8 years of experience in a comparable quantitative modeling or analytics role, ideally in the financial sector Must have technical/programming skills; C# . Net, SQL and C++ Exposure to Market Data; Statistics and Probability based calculations; Using probability theory to evaluate the risks of complex financial instruments, solve analytical equations and design numerical schemes to analyze complex contracts; and Software design and principles Must also possess any level of product knowledge, Investments and Quantitative Methods Consistently demonstrates clear and concise written and verbal communication skills Education Bachelor’s/University degree or equivalent experience This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required. Grade All Job Level All Job Functions All Job Level All Job Functions US Time Type Citi is an equal opportunity and affirmative action employer. Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity. Citigroup Inc. and its subsidiaries ""Citi” invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity . To view the ""EEO is the Law"" poster . To view the EEO is the Law Supplement . To view the EEO Policy Statement . To view the Pay Transparency Posting . "
75,"Counterparty Risk Analytics - Quantitative Analyst/Developer, Officer",quantitative analyst,/rc/clk?jk=c1ac6679ed5debc8&fccid=5bcd1ef0a7f4fb99&vjs=3,"The Counterparty Risk Analytics team within Citi Quantitative Risk and Stress Testing group is looking to add an Officer level Quantitative Developer. Team The Counterparty Risk Analytics team is responsible for developing and maintaining the methodologies to calculate counterparty credit risk exposures of OTC derivatives, Exchange-Traded derivatives and Security Financing Transactions. The models are used for advanced Basel regulatory capital calculations, stress testing, and internal risk management measures. As a quant developer, you will be developing state of the art codebase used for model implementation, testing and providing analysis for the business. It is a great opportunity to work on some of the most challenging problems of the team and apply new technologies to meet business needs. Key responsibilities include Design, develop, test, release, maintain and improve our model codebase for calibration, simulation and pricing across all major asset classes Work closely with model owners to streamline model implementation and testing Participate in model development and research; Perform rigorous model testing and provide explanation to model reviewers Write efficient and elegant codes; Enhance user experience of the codebase Qualifications Master’s degree from a quantitative field Computer Science, Applied Math, Computational Finance or other quantitative disciplines is required; Up to 2 years’ experience is preferred. Strong object-oriented programming skills in C++ or Python with experience in parallel computing, cloud Amazon Web Service a plus Solid background in mathematics/finance including stochastic calculus, linear algebra, numerical methods, derivative modeling Strong work ethic and a team player with excellent time management skills and ability to multi-task Great communications skills in both verbal and written Grade All Job Level All Job Functions All Job Level All Job Functions US Time Type Full time Citi is an equal opportunity and affirmative action employer. Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity. Citigroup Inc. and its subsidiaries ""Citi” invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity . To view the ""EEO is the Law"" poster . To view the EEO is the Law Supplement . To view the EEO Policy Statement . To view the Pay Transparency Posting . "
76,"Quantitative Analyst, Model Risk Management",quantitative analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0Ccyf--N9oHt5SIOArc5bruaVKv40m9oJNvW_YRRmv5mPX7fzJzbYHXO1y1tPlqpAvvPOz-qJGXT-SpKXa6owAiX52FD26inPolWzf0LsRhdtBSCMoFKeDYucpPLPN_jLVGCX34f8XOp3dlIz_CGDETFvBfkWrOHjcmtPbG4BMOPlIfeOKVW8vS-ZlS_bEybT1VlexryX2R94JdizpZO42gGUeFPGwcA2rgzwew1qYdIPzLmJzJWEdDJa6ceoUzZUrVOTt6dlYm6n_1qB_58GhhDTQekCl5y2ZmAKR7ni7oseZ0I4ivZXFYVlgM_LDiDzORFT-7BBledkFsro379OLADkW1msS1LfCKj6nXbFdqr6KQ_pwB-s3jZDaIR99Nl2B0nD_JSJa6gpAx07F89cosRcLwQ-V2_ES8fJvyrU0uTHfO-1IaZvN2Ckg0lk-Nre2j8r3awA3fidmci7Bp7R3l8rCvTalHqK8ziqNnipPp6HHt7rKtb-Psijvj6M6YHh8o9sYwVmMbIA==&p=12&fvj=1&vjs=3,"Our client, a leading Investment Bank located in New York, NY is looking for a Quantitative Analyst to join their growing Model Risk Management team. This person will focus on both market and credit risk models across asset classes, not just one specific subset. This individual will be analyzing the conceptual soundness of risk models and engines, assessing model behavior and its suitability to particular products as well as the appropriateness of the model’s output risk sensitivities. The initial focus of this position is on Va R and CCR modeling for securitized products, FI, FX, equity and credit products, and consists of the following core responsibilities · Analyzing the conceptual soundness of risk models and engines· Assessing model behavior and its suitability to particular products/structures· Assessing appropriateness of the model’s output risk sensitivities· Quantifying the materiality of suggested model improvements· Liaising with front office quants, traders, risk and finance professionals and provide guidance on model risk· Cogently reviewing analysis findings resulting in model approval or disapproval· Client is conducting phone interviews as this person will start REMOTE. Job Types Full-time, Contract Pay $600. 00 $650. 00 per day Benefits 401 k Dental Insurance Health Insurance Paid Time Off Schedule Monday to Friday Overtime Experience Risk Management 3 years Required Work Location Fully Remote Company's website https //www. missionstaffing. com/Work Remotely Yes"
77,MQA - Quantitative Analyst (Citi Investment Strategies - Equity Exotic Derivatives),quantitative analyst,/rc/clk?jk=d3831487c2208d37&fccid=5bcd1ef0a7f4fb99&vjs=3,"The Quantitative Analyst is a strategic professional who stays abreast of developments within own field and contributes to directional strategy by considering their application in own job and the business. Recognized technical authority for an area within the business. Requires basic commercial awareness. There are typically multiple people within the business that provide the same level of subject matter expertise. Developed communication and diplomacy skills are required in order to guide, influence and convince others, in particular colleagues in other areas and occasional external customers. Significant impact on the area through complex deliverables. Provides advice and counsel related to the technology or operations of the business. Work impacts an entire area, which eventually affects the overall performance and effectiveness of the sub-function/job family. Responsibilities Develop analytics libraries used for pricing and risk-management Create, implement, and support quantitative models for the trading business leveraging a wide variety of mathematical and computer science methods and tools including hardware acceleration, advanced calculus, C++ including STL, C#, . NET, Java, object oriented software design, Python, kdb, Structured Query Language SQL , mathematical finance/ programming and statistics and probability Develop pricing models using numerical techniques for valuation including Monte Carlo Methods and partial differential equation solvers Collaborate closely with Traders, Structurers, and technology professionals. Work in close partnership with control functions such as Legal, Compliance, Market and Credit Risk, Audit, Finance in order to ensure appropriate governance and control infrastructure Build a culture of responsible finance, good governance and supervision, expense discipline and ethics Appropriately assess risk/reward of transactions when making business decisions; and ensure that all team members understand the need to do the same, demonstrating proper consideration for the firm’s reputation. Be familiar with and adhere to Citi’s Code of Conduct and the Plan of Supervision for Global Markets and Securities Services; and ensure that all team members understand the need to do the same Adhere to all policies and procedures as defined by your role which will be communicated to you Obtain and maintain all registrations/licenses which are required for your role, within the appropriate timeframe Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency. Qualifications 1-6 years of experience in a comparable quantitative modeling or analytics role, ideally in the financial sector Must have technical/programming skills; C# . Net, SQL and C++ Exposure to Market Data; Statistics and Probability based calculations; Using probability theory to evaluate the risks of complex financial instruments, solve analytical equations and design numerical schemes to analyze complex contracts; and Software design and principles Must also possess any level of product knowledge, Investments and Quantitative Methods Consistently demonstrates clear and concise written and verbal communication skills Education Bachelor’s/University degree, Master’s degree preferred This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required. Grade All Job Level All Job Functions All Job Level All Job Functions US Time Type Citi is an equal opportunity and affirmative action employer. Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity. Citigroup Inc. and its subsidiaries ""Citi” invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity . To view the ""EEO is the Law"" poster . To view the EEO is the Law Supplement . To view the EEO Policy Statement . To view the Pay Transparency Posting . "
78,Principal Biostatistician,statistician,/company/Phastar/jobs/Principal-Biostatistician-b67a87489650f7ae?fccid=6c4c1b43a0cb41ab&vjs=3,"PHASTAR is a multiple award-winning, top 10, data focused CRO specialising in providing statistics, programming, data management and data science. With offices across the UK, US, Germany, Kenya and Australia, PHASTAR is a global CRO that is continuing to grow. In 2019, PHASTAR won the prestigious Queen’s Award for Enterprise, following our impressive growth from a one-person operation to a global CRO within a decade. Our unique approach to data analysis, “The PHASTAR Discipline”, has led us to build a reputation for outstanding quality. With this as our core focus, we’re looking for talented individuals who share our passion for quality and statistical expertise to join our team. THE ROLE As a senior statistician, you will hold a combined project leadership and hands-on technical statistics role. Working across a range of phases and therapeutic areas, you will lead assigned studies, ensuring they are delivered to optimal quality, on time and on budget. Key Responsibilities Manipulate, summarise and analyse clinical trial data using a variety of statistical methods Function as statistical support across multiple studies Prepare and review study documentation including protocols and statistical analysis plans Supervise work of less experienced statisticians Project manage assigned studies Maintain a positive and engaging client relationship with regards to statistical issues Communicate statistical issues across multi-disciplinary teams SKILLS AND EXPERIENCE REQUIRED Ph D or M Sc in biostatistics or related discipline· Experience with patient reported outcomes, real-world data, electronic health record data and/or claims data Experience working within a clinical trials environment CRO, pharma or academic Previous experience performing statistical analysis using SAS Excellent written and verbal communication skills Ability to communicate with a broad range of people across different functions Why PHASTAR This is a unique opportunity to join a company where hard work and fun coincide! We offer structured training and development plans, the opportunity to continue your own research, a competitive salary, an excellent benefits package, and flexible working arrangements, all within a relaxed and friendly working environment where fun is encouraged, and renowned social events are organised throughout the year. Job Type Full-time Salary $100,952. 00 $136,579. 00 per year Work Remotely Yes"
79,Statistician Programmer,statistician,/rc/clk?jk=0153ae025a6e9b31&fccid=50e211104f8cde4b&vjs=3,"Statistician Programmer position available at Healthcare Risk Advisors The Statistician Programmer provides in-depth analysis of project data using appropriate statistical methods to meet internal and external client needs. Maintains project databases, works with audit staff to ensure data collection requirements are met, and assists with development of new databases. Reports results in a variety of settings including meetings, presentations, and written reports. Position is located in the NYC office. Essential Responsibilities Hands-on statistical analysis of existing and new datasets, including descriptive statistics, regression models, and other appropriate statistical methods. Plan data collection methods for projects, and determine the sample sizes of groups to be used. Report results of statistical analyses, including information in form of graphs, charts and tables. Assist in writing abstract, articles, publications and reports of findings from analyses, as well as study design and methodology and sections for publications. Display and present data in a meaningful and easy to understand manner. Conduct quality checks of data. Provide consultations, data management and analysis. Monitor historical and current data trends. Develop automated, customized reports in Access databases. Perform other duties or special projects as required or as assigned. Education and Experience Required Masters required MS or MPH in Biostatistics, Statistics, or equivalent Minimum of 4 years in the Healthcare industry with quality/process improvement focus, with understanding of medical terminology and clinical care. Must have 4+ years of SAS programming SAS Institute base certification preferred . Develop appropriate and accurate statistical models for interpretation of existing and new datasets. Maintain and make modifications to existing databases. Prepare and update analyses based on the type of datasets being analyzed. A strong background in Access, including the ability to write modules using SQL and VBA. Excellent verbal and written communication skills; ability to speak clearly and concisely, conveying complex or technical information in a manner that others can understand, as well as ability to understand and interpret complex information from others. Advanced MS Excel skills. Proficiency with MS Office including Word, Outlook and Power Point . Healthcare Risk Advisors, Inc. , formerly FOJP Service Corporation, provides comprehensive insurance and risk management advisory services to leading academic hospitals, long-term care facilities, and social services agencies in the metropolitan New York area. Healthcare Risk Advisors employs a best-in-class approach to risk management that includes risk transfer, development of favorable insuring terms, and dedicated client services throughout the entire claims process. Our team includes accomplished insurance and brokerage professionals as well as attorneys with extensive experience in claims litigation, insurance, and contract law. HRA’s goal is to ensure that our clients have appropriate levels of coverage—as well as access to risk prevention services and resources—so they can focus their time and effort on the communities they serve. HRA offers countless career opportunities and an environment which fosters professional advancement and success. Visit www. healthcareriskadvisors. com to learn more about our company. HRA is an equal opportunity employer. "
80,Real Estate Data Analyst - 100% Remote Work,data analyst,/company/LodeStar-Software-Solutions/jobs/Real-Estate-Data-Analyst-230ca01ebb7e412c?fccid=aa3d9b437e4d5cc7&vjs=3,"Lode Star Software Solutions is looking for a data/business analyst to help develop our core product offering. He/she will be responsible for the quality control of Lode Star's platform which quotes closing costs nationwide to both title companies and mortgage lenders. The hire will be working closely with Lode Star's CEO/CTO. The position is 20-30 hours per week, fully remote, with the opportunity for a full-time job with benefits within three months. Pay is $25 per hour. Expertise with Excel specifically in creating formulas is required as well as overall technical aptitude. Previous experience in the title insurance, real estate, or mortgage industries is preferred but not required. Specifically around the calculation of closing costs typical to a home purchase or refinance. These include transfer taxes, municipal recording fees, and title insurance premiums. Lode Star was founded in 2013 and is a small but fast-growing technology company with a relaxed work environment and the option for 100% remote work is available. The company was recently named as one of the top 100 technology companies in the mortgage industry. www. housingwire. com/articles/2020-hw-tech100-mortgage-winner-lodestar-software-solutions/Job Type Part-time Pay $24. 00 $25. 00 per hour Schedule Monday to Friday COVID-19 considerations There is no in-person requirement for this job. Everything can be done remotely. Work Location Fully Remote Typical start time 9AM Typical end time 5PM Company's website www. lodestarss. com"
81,Data Analyst,data analyst,/rc/clk?jk=90bb410e5296aa5a&fccid=665f8f7bb3fcde1e&vjs=3,"Tee Public, the world's fastest growing community of independent artists & designers is seeking a Data Analyst for it's operations team including supply chain, finance and people team. In this role, you will help define what data we capture and run analysis to extract insights that help drive productivity within the teams, and will work across three different functional areas. Tee Public was founded in 2013 with the mission to create the perfect environment for artists to interact, design and sell their work. In 2019 Tee Public had over 50mn dollars in revenue and 4 billion user generated skus on the platform, that's a lot of data! Responsibilities Use data from a variety of different sources to run analysis and extract actionable insights that team members and leaders can use to grow the business. Lead meetings with stakeholders to step them through the analysis and think through how to take action based on the insights. Constantly improve your understanding of Tee Public's business both through the lens of the teams you support and the broader organization. Aggregate and prioritize data requests and implement a sprint planning and execution process. Challenge team members that are asking for analysis that won't lead to insight or won't be actionable. Execute monthly executive reporting, and effectively communicate trends through powerpoint. Work with systems and data teams to ensure relevant data is being captured and made accessible. Proactively QA data sets and diagnose data related bugs. Work with data, systems, and growth teams to resolve those bugs. Work with systems and growth teams to align on the operational plan that teams will execute, so that it drives the metrics accurately. Develop and maintain analytic content for individual contributors, managers and executives. Understand the team's objectives and determine KP Is to measure performance. Optimize team performance by helping design, execute and measure experiments related to marketing and sales tactics. Deliver strategic reporting and insights to executives on a monthly basis. Requirements Extensive experience using advanced SQL, quantitative analysis, and BI tools like Mode or Looker to influence executive-level decision making 3+ years Fluency in translating business questions and issues into prioritized functional and technical requirements, data models, analyses, and visualizations Intense curiosity and strong passion for analytical problem solving. High attention to detail Effective at coordinating work and communication across disparate groups of individuals and levels in the organization Experience executing data work using a product roadmap, sprints and user stories and a successful track record managing projects from feature definition to project deployment. Skilled at developing and exercising cross-functional influence, including a track record of being a trusted partner to business leaders and comfort with explaining and promoting best practices in interpreting and taking advantage of data. Advanced Excel skills. "
82,Data Analyst,data analyst,/rc/clk?jk=eb1c6aa226586bb7&fccid=e3977494b2ebf993&vjs=3,"Duties Summary This position is being announced under FEMA's Cadre of On-call Response/Recovery Employee CORE Program. This is a temporary appointment in the Excepted Service, not to exceed 2 years, with the option to extend based on workload and funding availability. Veterans Preference does not apply to the CORE selection process. View common definitions of terms found in this announcement. Responsibilities The Department of Homeland Security DHS is calling on those who want to help protect American interests and secure our Nation. DHS Components work collectively to prevent terrorism; secure borders and our transportation systems; protect the President and other dignitaries; enforce and administer immigration laws; safeguard cyberspace; and ensure resilience to disasters. We achieve these vital missions through a diverse workforce spanning hundreds of occupations. Make an impact; join DHS. When disaster strikes, America looks to the Federal Emergency Management Agency FEMA . Now FEMA looks to you. Join our team and use your talent to support Americans in their times of greatest need. FEMA prepares the nation for all hazards and manages Federal response and recovery efforts following any national incident. We foster innovation, reward performance and creativity, and provide challenges on a routine basis with a well-skilled, knowledgeable, high performance workforce. Please visit www. fema. gov for additional information. EMERGENCY ASSIGNMENT Every FEMA employee has regular and recurring emergency management responsibilities, though not every position requires routine deployment to disaster sites. All positions are subject to recall around the clock for emergency management operations, which may require irregular work hours, work at locations other than the official duty station, and may include duties other than those specified in the employee's official position description. Travel requirements in support of emergency operations may be extensive in nature weeks to months , with little advance notice, and may require employees to relocate to emergency sites with physically austere and operationally challenging conditions. In this position, you will serve as a Data Analyst for Region II Response Division. Typical assignments include Developing modern interactive Share Point sites and portals Developing data storage applications using Share Point or SQL Server/ASP. NET Writing code for connecting, extracting, or importing REST data services Developing interactive data products, reports, or visualizations using various programs such as Share Point, Power BI, Tableau, Excel, Power Apps, MS Flow, Excel Power Query, and Access Testing software applications for functionality and usability Using established protocols to ensure technical solutions meet agency compliance metrics Promotion Potential Positions filled under Stafford Act regulations generally do not have documented promotion potential; however, dependent on an employee’s ability to perform higher level duties, the continuing need for an employee to perform work associated with a higher level position, and administrative recommendation and approval, promotions may be earned by CORE employees. Travel Required Occasional travel Occasional non-emergency travel may be required. Supervisory status No Promotion Potential NA Job family Series 0301 Miscellaneous Administration And Program Requirements Requirements Conditions of Employment You must be a U. S. citizen to be considered for this position. You must successfully pass a background investigation. Travel will be required. You must be able to obtain and maintain a Government credit card. Selective service registration is required for males born after 12/31/59. Please review ""Other Information"" section for additional key requirements. To ensure the accomplishment of our mission, DHS requires every employee to be reliable and trustworthy. To meet those standards, all selected applicants must undergo, successfully pass, and maintain a background investigation for Public Trust as a condition of placement into this position. This may include a credit check after initial job qualifications are determined, a review of financial issues, such as delinquency in the payment of debts, child support and/or tax obligations, as well as certain criminal offenses and illegal use or possession of drugs please visit Mythbuster on Federal Hiring Policies for additional information . For more information on background investigations for Federal jobs please visit OPM Investigations. Please ensure you meet the qualification requirements described below. Qualifications The qualification requirements listed below must be met within 30 days of the closing date of the announcement. You qualify for this position at the IC-12 level starting salary $88,651. 00 if you possess the following One full year of specialized experience equivalent to the next lower grade GS-11 in the Federal Service. This experience may have been gained in the federal government, a state or local government, a non-profit organization, the private sector, or as a volunteer; however, your resume must clearly describe at least one year of specialized experience. Specialized experience for this position includes Providing analytical consultation or technical advice related to data driven solutions to customers; Performing application development and maintenance such as web design or code writing; and Troubleshooting software applications to recommend solutions. Experience refers to paid and unpaid experience, including volunteer work done through National Service programs e. g. , Peace Corps, Ameri Corps and other organizations e. g. , professional, philanthropic, religious, spiritual, community, student, social . Volunteer work helps build critical competencies, knowledge, and skills and can provide valuable training and experience that translates directly to paid employment. You will receive credit for all qualifying experience, including volunteer experience. Your application must show that you meet all requirements for this position. You may be found “not qualified” if you do not possess the minimum competencies required for the position. NOTE Qualifications are based on breadth/level of experience. In addition to describing duties performed, applicants must provide the exact dates of each period of employment from MM/DD/YY to MM/DD/YY and the number of hours worked per week if part time. As qualification determinations cannot be made when resumes do not include the required information, failure to provide this information may result in disqualification. Applicants are encouraged to use the USAJOBS Resume Builder to develop their federal resume. For a brief video on How to Create a Federal Resume, click here. NOTE If you are using the USAJOBS Resume Builder, please insert the dates of employment in MM/DD/YY format at the top of the “Duties, Accomplishments, and Related Skills” text field for each period of employment included on your resume. Current or former FEMA Reservists/DAE employees To accurately credit your experience for these intermittent positions, make sure to list the dates from MM/DD/YY to MM/DD/YY of each deployment, along with the job title and specific duties you were responsible for during each deployment. Failure to provide this information may result in disqualification. Education Additional information If you receive a conditional offer of employment for this position, you will be required to complete an Optional Form 306, Declaration for Federal Employment, and to sign and certify the accuracy of all information in your application, prior to entry on duty. False statements on any part of the application may result in withdrawal of offer of employment, dismissal after beginning work, fine, or imprisonment. DHS uses E-verify, an internet based system, to confirm the eligibility of all newly hired employees to work in the United States. Learn more about E-Verify, including your rights and responsibilities. This announcement may be used to fill one or more vacancies. STAFFORD ACT EXCEPTED SERVICE APPOINTMENTS Cadre On Call Response Employee CORE is an appointment type granted under the Robert T. Stafford Disaster Relief and Emergency Assistance Act, Section 306 b , which authorizes FEMA to appoint such temporary employees as necessary to accomplish work authorized under the Act. Appointments under this authority within FEMA are made to the excepted service and are nonpermanent in nature. All candidates must be able to deploy with little or no advance notice to anywhere in the United States and its territories for an extended period of time. Deployments may include working in excess of eight hours a day, or in excess of 40 hours per week, including weekends and holidays, and under stressful, physically demanding, and austere conditions. When activated and deployed, you serve in a federal travel status and are entitled to lodging, transportation and per diem reimbursements for authorized expenses in accordance with federal travel regulations. If selected for this position, and you have not previously completed these requirements, you are subject to 1 completion of a 2 day onboarding program at your primary duty location; and, 2 completion of a subsequent, multiple day orientation program within 90-120 days of hire, at a location to be determined. Travel associated with the orientation portion of this requirement may be at FEMA’s expense. Applying to this announcement certifies that you give permission for DHS to share your application with others in DHS for similar positions. How You Will Be Evaluated You will be evaluated for this job based on how well you meet the qualifications above. We will review your resume, supporting documentation, and your responses to the occupational questionnaire to ensure you meet the minimum qualification requirements. If you are qualified, you may be referred to a selection panel for consideration. We recommend that you preview the online questions for this announcement before you start the application process. Interview Requirement. Interviews may be required for this position. Failure to complete the interview may result in removal from further consideration. To preview questions please click here. Background checks and security clearance Security clearance Not Required Drug test required No Position sensitivity and risk Moderate Risk MR Trust determination process Required Documents Required Documents Your resume. Your responses to the job questionnaire You will be directed to the online job questionnaire once you begin the application process for this position. Are you a current or former federal employee?Submit a copy of your most recent SF-50, Notification of Personnel Action that demonstrates your eligibility for consideration, e. g. , length of time you have been in your current grade; your highest grade held; your current promotion potential and proof of permanent appointment if applying based on an interchange agreement. Examples of appropriate SF-50s include promotions, within-grade increases and accessions. Benefits Benefits A career with the U. S. Government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Learn more about federal benefits. DHS offers competitive salaries and an attractive benefits package, including health, dental, vision, life, and long-term care insurance; retirement plan; Thrift Savings Plan similar to a 401 k ; Flexible Spending Account; Employee Assistance Program; personal leave days; and paid federal holidays. Other benefits may include flexible work schedules; telework; tuition reimbursement; transportation subsidies; uniform allowance; health and wellness programs; and fitness centers. DHS is committed to employee development and offers a variety of employee training and developmental opportunities. For more information, go to the DHS Careers website and select “Benefits. ” Disabled veteran leave will be available to any Federal employee hired on or after November 5, 2016, who is a veteran with a service-connected disability rating of 30 percent or more. Review our benefits Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time, or intermittent. Contact the hiring agency for more information on the specific benefits offered. Help This job is open to The public U. S. citizens, nationals or those who owe allegiance to the U. S. Clarification from the agency This Job is open to U. S. Citizens"
83,Data Analyst,data analyst,/rc/clk?jk=1ad24eba2cc898b0&fccid=261c0771a0ed6399&vjs=3,"Job Description Job Purpose Tune Core is seeking a qualified candidate who thrives in fast-paced environments and is able to provide impactful solutions for all business needs. The ideal candidate will be responsible for building complex reports through segmentation and analysis of large datasets in a complex data environment. They will additionally be fulfilling ad-hoc data requests within tight deadlines and are preferably comfortable working with financial and marketing data. They will report to the Data Team and work in a cross-functional capacity supporting all internal departments, including, but not limited to Marketing, Product, Business Development, Tech & Operations. The tasks and responsibilities of this position include, but are not limited to Create, maintain, and improve upon new and existing reports in order to extract actionable insights and make recommendations that will drive decisions across the business Extract, transform, cleanse and synthesize data from a variety of data sources Fulfill ad-hoc requests and communicate results to stakeholders and related teams Liaise between multiple departments to develop reports relating to marketing, finance, product performance and consumer behavior across all products Validate and own accuracy and integrity of data results. Additional responsibilities will include oversight and improvement of existing ETL processes along with the in-house dev-ops team. Skills/Qualifications Experience Some coursework or experience in Accounting/Finance/Marketing is a plus 2-3 years of proven work experience as a data analyst or business data analyst Strong analytical skills with the ability to collect, organize and analyze significant amounts of data with attention to detail and accuracy Knowledge of statistics and experience using statistical packages such as Python, R, Excel, SPSS, SAS, etc. for analyzing datasets. Proficiency in SQL and in-depth knowledge of common RDBMS systems such as My SQL, SQL Server, Postgre SQL. Some amount of experience using BI tools such as Tableau, Qlikvue, Looker, Power BI etc. is desirable. Excellent communication and interpersonal skills. Should be self-motivated and be able to provide creative solutions in a rapidly changing environment. Desire to work in a small to medium business environment and improve processes, procedures and controls through redesign and/or automated solutions Knowledge of SQL, MS Excel, MS Access, RDBMS Concepts, R/Python Education BS in Mathematics, Economics, Computer Science, Information Management or Statistics Powered by Jazz HR S Cv739NHWV"
84,Data Analyst / BI Developer,data analyst,/rc/clk?jk=159e912770595041&fccid=52e389f72860d9b3&vjs=3,"Cafe Media empowers over 2,500 premium publishers, with a collective reach of over 164MM monthly unique visitors #11 largest Comscore property in May 2020 , to make a living doing what they love – producing great content – while we manage the advertising for them. From food to home to news and technology and more, we represent the influential voices who make the internet a better place. As the largest exclusive publisher network, with direct access to code-on-page and underlying datasets, we have a unique capability to build innovative products that span across publishers, advertisers and consumers. We’re looking for someone to join the Data Intelligence team to use their quantitative and technical skills to help us develop insights and reporting that make our products stronger, our processes and actions more effective and lead to stronger impact for our publishers. The ideal candidate has both strong technical skills, in SQL and dashboard building, and communication skills, in being able to effectively share findings and recommendation. Data Analyst / BI Developer Design and plan BI reports, as well as debug, monitor, and troubleshoot BI solutions. Write relational and multidimensional database queries. Form productive relationships with internal customers by listening, clarifying, and responding effectively. Provide accurate and timely answers to routine questions from customers. Act as the first line of support for data questions and issues and strive to become data subject matter expert for the company. Responsible for programs that are accurate, efficient and well documented. Work collaboratively with peers to develop quality protocols, schema, and reports per agreed timelines. Participate on project teams. Responsible for writing fundamental documentation in a clear, concise manner, adhering to standards. Skills Strong experience with programming in SQL required Experience working with Looker Command-line experience with shell scripting and software development with python and pyspark skills preferred Highly skilled in Excel Strong understanding of online media industry preferred BA/BS required Cafe Media is an equal opportunity employer Powered by Jazz HR wn F Ank K9g S"
85,"Data Analyst, Public Health Programs",data analyst,/company/Vital-Strategies/jobs/Data-Analyst-88e2b7405be4c06f?fccid=3398946381ceadd1&vjs=3,"Vital Strategies, headquartered in New York City, is an international public health organization. Our programs strengthen public health systems and address the world’s leading causes of illness, injury and death. We currently work in 73 countries, supporting data-driven decision making in government, advancing evidence-based public health policies and mounting strategic communication campaigns. Vital Strategies’ priorities are driven by the greatest potential to improve and save lives. They include non-communicable disease prevention, cardiovascular health promotion, tobacco control, road safety, obesity prevention, epidemic prevention, overdose prevention, environmental health, vital statistics systems building and multidrug-resistant tuberculosis treatment research. Our programs are primarily concentrated in low- and middle-income countries in Africa, Latin America, Asia and the Pacific; the Overdose Prevention Program is our first initiative in the U. S. Please visit our website at www. vitalstrategies. org to find out more about our work. Vital Strategies offers highly competitive compensation and comprehensive benefits. Vital welcomes and supports a diverse, inclusive work environment. As such, our commitment is to promote equal employment opportunities EEO for all applicants seeking employment. The Public Health Programs division leads several programs including but not limited to The Bloomberg Philanthropies Data for Health D4H Initiative which aims to enhance the quality and use of public health data to support decision-making on public health policies, in collaboration with numerous technical and in-country, regional and global partners. The four main programs of D4H include the Civil Registration and Vital Statistics CRVS , Data Impact, Cancer Registry and the Global Grants Program – all working across multiple countries. The Partnership for Healthy Cities, a network of 70-plus city governments in Latin America, Africa, and Asia works to improve noncommunicable disease NCD and injury prevention through high-impact policy and other interventions; the Road Safety program, part of the Bloomberg Philanthropies Global Road Safety Initiative, aims to reduce road fatalities and injuries through the support of evidence-based interventions to cities, primarily in lower and middle income countries. And the Overdose Prevention Program, Vital Strategies’ first initiative in the U. S. , works with local partners and provides technical assistance towards reforming policies, developing or enhancing innovative programs and mobilizing communities at the neighborhood, municipal, county and state levels. Vital Strategies is seeking to hire a full-time Data Analyst who will combine their analytical expertise with a passion for public health to support the PHP division with the development of financial reports and dashboards to better inform programmatic decisions regarding budget and to improve overall stewardship of grant funds. The Data Analyst will work collaboratively across several programs within the Public Health and Finance Divisions to analyze and convert existing programmatic data into powerful reports and visualizations that effectively and rapidly enable analysis for decision making. The Data Analyst will also have the opportunity to support programmatic operations with activities such as the analysis of monitoring and evaluating metrics or analysis of programmatic expenditures. Core Responsibilities Support Public Health Program and grant managers with financial and programmatic data to inform decision making and improve stewardship of grant funds. Analyze existing data sources and data flow for program management and decision-making support in the development of data models for ETL Extraction, Transformation and Load process of selected data sources Identify and interpret trends or patterns in complex data sets Create powerful and user-friendly dashboards using data table and chart visualizations to improve analytical and decision-making capabilities for program staff and division leadership Maintain and update existing budget reports and dashboards for program management and decision-making Prepare and complete ad-hoc reports and data sets Communicate results to non-statistical audiences using data visualization Responsible for regular review, and overall data quality, of various financial, operational and programmatic data sets, working with program and finance staff to resolve errors in data Meet with program staff to identify data needs and support in the creation of dashboards, and analytical tools using Net Suite and Power BI Qualifications Minimum of four-six years of related work experience Knowledge and direct experience using Power BI DAX functions, Power Query preferred, however experience with other data visualization platforms and statistical systems will be accepted Knowledge and direct experience using Net Suite preferred Experience with data management and data visualization Bachelor’s degree in a related field Interest and/or experience in grants management preferred Prior experience working in non-profit, grants, or finance sectors preferred Advanced understanding of database management system technology and architecture Technical expertise regarding data models, database design development, data mining and segmentation techniques Strong analytical skills with the ability to collect, organize, analyze and disseminate significant amounts of information with attention to detail and accuracy Ability to effectively communicate with a range of stakeholders Proven ability to collaborate well with others How to Apply Please submit a CV and cover letter, including salary expectations, through the Vital Strategies Career Center. The deadline for applications is September 10, 2020. Only shortlisted candidates will be contacted. https //chp. tbe. taleo. net/chp01/ats/careers/v2/view Requisition?org=VITASTRA&cws=37&rid=219Job Type Full-time Pay $53,389. 00 $110,473. 00 per year Schedule Monday to Friday Work Remotely Temporarily due to COVID-19"
86,Clinical Research Data Analyst,data analyst,/rc/clk?jk=197f0866b2f46aad&fccid=bd976cc171c690e0&vjs=3,"Requisition no 508179 Work type Full Time Location Medical Center School/Department Biomedical Informatics Grade Grade 105 Categories Research Lab and Non-Lab Position Summary The Department of Biomedical Informatics DBMI at Columbia University has an immediate opening for a talented and self-motivated Clinical Research Data Analyst who has strength in collaborative work in a medical/ healthcare setting and an interest in electronic health record EHR research. This individual should have a passion for advancing large scale observational research. DBMI is responsible for curating EHR data from a network of health provider organizations across the country. With the current pandemic, we are focused on reviewing and analyzing COVID data from the network. Current available position is grant-funded. Columbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population. Employees of the department are passionate, friendly and resourceful. Responsibilities Use SQL to analyze data quality checking for conformance, completeness, and plausibility of EHR data; Develop programs in Python to automatically detect, organize, quantify, display, and communicate data quality issues; Conduct research to determine future means of assessing data quality; Facilitate interaction between CUIMC and data partners and between different data partners. Minimum Qualifications Requires a bachelor’s degree or equivalent in education and experience in computer science, data science, epidemiology, statistics, or a relevant field for analyzing large data sets, plus four years of related experience Other Requirements The incumbent will work under the direction of the faculty members leading the project. He/she will work closely with the project’s team, including members of other labs within the department and external partners. The incumbent must have excellent organizational skills with attention to detail, ability to multi-task in a fast-paced environment, and work under deadlines, requires exceptional programming, communication oral and written and interpersonal skills, and experience and comfort working independently. Experience with agile development is a plus Equal Opportunity Employer / Disability / Veteran Columbia University is committed to the hiring of qualified local residents. Applications open Aug 10 2020 Eastern Daylight Time Applications close "
87,Data Analyst - AVP,data analyst,/rc/clk?jk=5fe6e1fc060e5ecc&fccid=3b98171e4a0fd997&vjs=3,"Description Job description to be developed, please work with your recruiter on this. Job description to be developed, please work with your recruiter on this. The above statements are intended to describe the general nature and level of work being performed. They are not intended to be construed as an exhaustive list of all responsibilities duties and skills required of personnel so classified. "
88,Staff Business Data Analyst,data analyst,/rc/clk?jk=cdbd7d13bdcb349c&fccid=9784ae78e9834539&vjs=3,"Overview Come join Intuit as a Staff Business Data Analyst. What you'll bring What you will bring 7+ years of experience working in finance, business or other related analytics fields Highly proficient in SQL, Tableau/Qlik Sense, and Excel. R and Python are preferred Good understanding of E2E Procurement processes and tools Coupa, Aravo, etc. Ability to tell stories with data, educates effectively, and instills confidence in recommendations, motivating others to act on them Strong analytical and problem-solving skills able to develop and use structured approaches to identify root causes and recommend resolutions can present results in meaningful terms Lead the gathering of business requirements, analysis of source systems, definition of underlying data sources and transformation requirements, documents data & process flows Design and monitor business processes, business rules, metrics and standard operating procedures as a result of improvement initiatives Ability to manage multiple projects simultaneously to meet objectives and deadlines Experience developing and gaining acceptance for proposed solutions and business cases with senior level technical and business leaders Outstanding communications skills with both technical and non-technical colleagues Strong organizational skills, time management, portfolio prioritization experience, and accountability required Technical undergraduate degree required Engineering, Computer Science, Statistics, Analytics etc. ; advanced degree preferred or equivalent experience How you will lead How you will lead Provides guidance and thought partnership to Business leaders and stakeholders on how best to harness available data in support of critical business needs and goals Partner with business owners within and outside GBS to enable decision support and key business insights through Automated Reporting Partner within the team to measure impact of business strategy and answer ad-hoc analytic questions and train users to self-serve standard reports leveraging Qlik Sense/Tableau Pursue data quality, troubleshoot data validation, and see issues to resolution Provides to business stakeholders the entrepreneurial guidance essential for appropriately interpreting and building on findings"
89,Data Analyst III (Healthcare Analytics),data analyst,/rc/clk?jk=efc15ac83d9a3603&fccid=5ece327da33e0274&vjs=3,"Position Purpose Interpret and analyze data from multiple sources including claims, provider, member, and encounters data. Identify and assess the business impact of trends Develop, maintain, and troubleshoot complex scripts and reports developed using SQL, Microsoft Excel, or other analytics tools Contribute to the planning and execution of large-scale projects with limited direction from leadership Assist in the design, testing, and implementation of process enhancements and identify opportunities for automation Identify and perform root-cause analysis of data irregularities and present findings and proposed solutions to leadership and/or customers Manage multiple, variable tasks and data review processes with limited supervision within targeted timelines and thrive in a demanding, quickly changing environment Apply expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how customers interact with analytic products Partner cross-functionally at all levels of the organization and effectively, both verbally and visually, communicate findings and insights to non-technical business partners Independently engage with customers and business partners to gather requirements and validate results Communicate and present data-driven insights and recommendations to both internal and external stakeholders, soliciting and incorporating feedback when required Provide technical guidance to junior analysts The Data Analyst Healthcare Analytics will have the opportunity to make a significant impact through the discovery, development, and implementation of leading-edge analytics that answer important business questions. The analyst will collaborate with key corporate and health plan business partners for the purpose of identifying and delivering robust reporting and analytics capabilities to drive improved business performance. Education/Experience Bachelor’s degree in business, economics, statistics, mathematics, actuarial science, public health, health informatics, healthcare administration, finance or related field or equivalent experience. Master's degree preferred. 4+ years of experience working with large databases, data verification, and data management or 2+ years of IT experience. Healthcare analytics experience preferred. Ability to integrate, validate, and reconcile data from multiple sources as well as design and construct analysis tools that extract, prepare, analyze, and store/present results to support business needs. Knowledge of query development using SQL or other coding languages. Knowledge of basic statistical, analytical, or data mining techniques including basic data modeling, trend analysis, and root-cause analysis preferred. Working knowledge of analytical tools, including R, Python, SAS, Arc GIS, QGIS, Microstrategy, Tableau, Hadoop, or related tools preferred. Working knowledge of relational databases preferred. Working knowledge of automation capabilities such as batch processes, stored procedures, scripting languages, Microstrategy, or other tools preferred. Risk Adjustment Experience in risk adjustment, clinical coding, financial reporting/analysis, or CMS/State encounters and regulatory file submissions preferred; Experience with data mining, population health, and statistical modeling preferred. This is an office-based role. Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law. "
90,Data Analyst with Governance Experience,data analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0C5IatSLh_Ak1q39eQQoPIxD737RW9NeiYGvIRXkrLjEBNGhZvRfJ822RSR1sk8CpT8TJSsm3b74huJUFjSMgzjcXRSn1CyfI9MKpFmX0EMyAV_z4N0GP4T2-XoSxnfmhT9oLgomCilqw1ofKqOHhZsaWexx3QeMBk4uWYk2WUupWytQlGoGcFx3zcSh1oudHly2S1r8kXwVNwan3FM99TKwPwDD1InhXmGAy-1G7EvJ4XCUf_vcVMVSTdRNgT9mq4Zx69aYdareVXvlFwteiCE_QdkU2d6UG7ROlDLtgctINl3cW1wuHpdYfvl0Vl4r8mn-xiRUYRj8w58RA-8rITEi-6n4xOHqOdusTYFGkOHK6UbIGoAPdAp-K8BOPLmum-_9J-XP6q1xshd2_ApTbUJo_CpDLUsNLV6IIitPV79gEQKCW6m51jQRcRjcm3QF7x1xXpeZbfkBnKgku75wFKaKN0q1R7TjYFuxUC_MHEOLIYhllg6vgab3Q77bmgZ1L13jgcYAd-NsqPTK5kPLiU1rsB3GhkYeta6whd1FlMBCTOEpTIm-KdpuxoUWks0340=&p=7&fvj=0&vjs=3,"RESPONSIBILITIES Kforce has a client that is seeking a Dn A Governance Experienced Professional in New York, NY. Summary The Consultant will serve as lead facilitator of all data and analytics governance and sub-committee meetings. This role will participate in the development of a range of data and analytics policies addressing data asset quality, security, privacy, life cycle, definitions and models. Responsibilities Engages with data and analytics governance committee, sub-committee and workgroup members and presenters on meeting agenda, materials and follow-up activities Applies practical knowledge of job area typically obtained through advanced education and work experience. Able to works independently with general supervision Responsible for influencing others within the job area through explanation of Dn A services, policies and practices REQUIREMENTS Bachelor's degree in Data Sciences, Computer Science, Mathematics, Engineering, Business Administration, or related disciplines Minimum of 2 years of experience in the field of data and analytics product/service governance, development, and/or performance management A satisfactory combination of education, training and experience Demonstrated experience developing or enforcing data and analytics policies Must have experience supporting Data and Analytics Governance framework, including preparing standard Governance meeting materials Strong communication skills with an obsessive eye towards planning and details Proficient with Microsoft Office, Visio and Share Point tools strongly desired Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. "
91,"Health Informatics Data Analyst, Bureau of Immunization",data analyst,/rc/clk?jk=47c60f693dcf5f65&fccid=4146d8487cdbf799&vjs=3,"The New York City NYC Department of Health and Mental Hygiene DOHMH Bureau of Immunization BOI Citywide Immunization Registry CIR is a highly complex central record-keeping system established by DOHMH to track the immunization status of individual New Yorkers and immunization coverage levels of the NYC population in a timely manner. The CIR is accessible to licensed health care providers, parents, and agencies authorized by DOHMH for the retrieval of immunization records for the purpose of ensuring that New Yorkers receive all recommended immunizations and are thereby protected from vaccine-preventable diseases. DUTIES WILL INCLUDE BUT NOT BE LIMITED TO Conducts research to evaluate BOI programs and policies; assists in the preparation of abstracts, articles, and presentations for local and national publications, conferences, and meetings; Carries out research and analyses to identify gaps in data completeness and accuracy of immunization reporting as well as issues with the HL7 Web Service, e. g. , spikes in message failures or connectivity problems; Runs standard and ad-hoc queries to support Centers for Disease Control and Prevention research and evaluation projects funded by supplemental grants Develops reporting tools and reporting program metrics for BOI initiatives, including the COVID-19 vaccination campaign, to meet monitoring, research, and evaluation requirements of senior management; Designs, develops and maintains database reports and mechanisms to assist senior management in improving operations of the CIR and Master Child Index systems; Acts as the lead liaison to technology contractor staff and internal technical staff for ensuring reliable day-to-day operations and security in accordance with DOHMH, state, and federal standards; Coordinates and performs systems and database support and maintenance onsite and at remote data center. Minimum Qual Requirements 1. For Assignment Level I only physical, biological and environmental sciences and public health A master's degree from an accredited college or university with a specialization in an appropriate field of physical, biological or environmental science or in public health. To be appointed to Assignment Level II and above, candidates must have 1. A doctorate degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and one year of full-time experience in a responsible supervisory, administrative or research capacity in the appropriate field of specialization; or 2. A master's degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and three years of responsible full-time research experience in the appropriate field of specialization; or 3. Education and/or experience which is equivalent to ""1"" or ""2"" above. However, all candidates must have at least a master's degree in an appropriate field of specialization and at least two years of experience described in ""2"" above. Two years as a City Research Scientist Level I can be substituted for the experience required in ""1"" and ""2"" above. NOTE Probationary Period Appointments to this position are subject to a minimum probationary period of one year. Preferred Skills Strong oral and written communication skills Strong organizational skills and ability to multi-task Ability to handle multiple and diverse work assignments in an automated environment Comprehensive data analysis experience and knowledge in research methodology Proficiency in SQL query language, R or SAS, PYTHON Linux systems skills Expertise with relational database management systems Java, JSP, Servlet, Apache and Tomcat skills Additional Information **IMPORTANT NOTES TO ALL CANDIDATES Please note If you are called for an interview you will be required to bring to your interview copies of original documentation, such as A document that establishes identity for employment eligibility, such as A Valid U. S. Passport, Permanent Resident Card/Green Card, or Driver’s license. Proof of Education according to the education requirements of the civil service title. Current Resume Proof of Address/NYC Residency dated within the last 60 days, such as Recent Utility Bill i. e. Telephone, Cable, Mobile Phone Additional documentation may be required to evaluate your qualification as outlined in this posting’s “Minimum Qualification Requirements” section. Examples of additional documentation may be, but not limited to college transcript, experience verification or professional trade licenses. If after your interview you are the selected candidate you will be contacted to schedule an on-boarding appointment. By the time of this appointment you will be asked to produce the originals of the above documents along with your original Social Security card. **LOAN FORGIVENESS The federal government provides student loan forgiveness through its Public Service Loan Forgiveness Program PSLF to all qualifying public service employees. Working with the DOHMH qualifies you as a public service employee and you may be able to take advantage of this program while working full-time and meeting the program’s other requirements. Please visit the Public Service Loan Forgiveness Program site to view the eligibility requirements https //studentaid. ed. gov/sa/repay-loans/forgiveness-cancellation/public-service ""FINAL APPOINTMENTS ARE SUBJECT TO OFFICE OF MANAGEMENT & BUDGET APPROVAL” To Apply Apply online with a cover letter to https //a127-jobs. nyc. gov/. In the Job ID search bar, enter job ID number # 442381. We appreciate the interest and thank all applicants who apply, but only those candidates under consideration will be contacted. The NYC Health Department is committed to recruiting and retaining a diverse and culturally responsive workforce. We strongly encourage people of color, people with disabilities, veterans, women, and lesbian, gay, bisexual, and transgender and gender non-conforming persons to apply. All applicants will be considered without regard to actual or perceived race, color, national origin, religion, sexual orientation, marital or parental status, disability, sex, gender identity or expression, age, prior record of arrest; or any other basis prohibited by law. NOTE This position is open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate in your resume that you would like to be considered for the position under the 55-a Program. Residency Requirement New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview. "
92,Senior Data Analyst - CRM,data analyst,/rc/clk?jk=21cbe36aaec8d01b&fccid=d440534677583830&vjs=3,"Zeta Global CRM division provides email, database and interactive marketing services worldwide to clients, the company’s services include strategic campaign planning and optimization, test design, creative development including rich media , campaign execution, web application development, research, and advanced analytics. The Analytics team is currently seeking a Senior Analyst. As a Senior Analyst, you will use a variety of data sources and software tools such as SQL, Excel and internal tools to develop reports and insights based on specification provided by clients or client analytical leads. Primary Job Responsibilities Work with digital marketing and CRM partners to understand clients’ requests, design proper metrics reports and dashboard. Handle data gathering and analysis construction with a variety of data sources and tools. Develop reports and customized analysis, data organizing and cleansing, and analysis and presentation Create easy-to-understand data visualization, presentations and provide logical insights and actionable recommendations to clients. Work with BI and technical team to prioritize and identify the opportunity to automate reports. Streamline the reporting process; develop repeatable and scalable process and coding across multiple systems and platforms Work with various businesses and technical teams to make sure the data presented are accurate and flawless. Well document the standard reporting or analysis procedures and other necessary documents. Job Requirements Minimum Bachelor’s degree in Computer Programming, Statistics, Marketing Analytics or related technical/quantitative field; Master degree a plus Minimum 3 years of professional experience in an analytical role prior experience in Agency/consulting or Email industry a plus , digital marketing, database marketing and web analytics a plus Experience working in rational database, SQL coding, SAS or R or Python programming and extensive Excel skills Experience working in BI tools, such as Tableau, Microstrategy, Jasper, etc. Experience in development, presentation and/or deployment of predictive model and segmentation is strongly preferred; knowledge of Machine Learning approaches a plus Experience working with 3rd party data providers such as Acxiom, IRI a plus Able to take broad specifications or requests from client and ask the right “questions” to help fine tune the request to help deliver reporting needs. Be able to conduct data investigation when issue arises both independently and/or working with cross functional technical resources, if required, to solve problem Good verbal and written communication skills; Attention to details, be able to identify data gaps/abnormality in output Able to plan, organize, & work on multiple projects at once #LI-MC1 About Zeta Global Zeta Global is a data-powered marketing technology company with a heritage of innovation and industry leadership. Founded in 2007 by entrepreneur David A. Steinberg and John Sculley, former CEO of Apple Inc and Pepsi-Cola, the Company combines the industry’s 3rd largest proprietary data set 2. 4B+ identities with Artificial Intelligence to unlock consumer intent, personalize experiences and help our clients drive business growth. Our technology runs on the Zeta Marketing Platform, which powers ‘end to end’ marketing programs for some of the world’s leading brands. With expertise encompassing all digital marketing channels – Email, Display, Social, Search and Mobile – Zeta orchestrates acquisition and engagement programs that deliver results that are scalable, repeatable and sustainable. Zeta Global is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, gender, ancestry, color, religion, sex, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veterans status, or any other basis protected by law. Zeta Global Recognized in Enterprise Marketing Software and Cross-Channel Campaign Management Reports by Independent Research Firm https //www. prnewswire. com/news-releases/zeta-global-opens-ai-data-labs-in-san-francisco-and-nyc-300945353. html https //www. prnewswire. com/news-releases/zeta-global-recognized-in-enterprise-marketing-software-and-cross-channel-campaign-management-reports-by-independent-research-firm-300938241. html"
93,Entry Level Data Analyst Position,data analyst,/company/jerneltechcorp/jobs/Entry-Level-Data-Analyst-Position-202a045d33f64d76?fccid=36c1c2ad2cf5af10&vjs=3,"Position Data Analyst Our growing technology firm is looking for a Data Analyst who is able to turn project requirements into custom-formatted data reports. The ideal candidate for this position is able to do complete life cycle data generation and outline critical information for each Project Manager. We also need someone who is able to analyze business procedures and recommend specific types of data that can be used to improve upon them. Requirements Candidate having Analytical skills with core experience Role involves developing queries from My SQL. Knowledge of SQL, Excel or similar applications is mandatory. Expertise in My SQL & related querying languages. Expertise in Excel and advance excel skills Should have experience in Reporting and business analytics. Must have experience in MSBI SSIS, SSRS, SSAS . SQL server and Data Warehousing Very strong experience in TSQL,SQL Store procedures,Data analysis Extensive experience to perform Data Analysis Activities Designs Data Architectures Daily Tasks Performed Write efficient T-SQL. ETL using Bulk Insert, SSIS. Performance Tuning and Optimization using native monitoring and troubleshooting tools. Perform other duties as assigned. Skills Development Program -Highly qualified trainers, Hands-on professional training. Online Interactive session. One Live project with using SDLC Agile and waterfall . On Job Support with dedicated support team. Special Technical mock with our technical experts before every interview with specific to the Job Description. **Pay and Benefits We allow candidates work on flexi terms of percentage basis or in fixed annual salary. We provide health insurance. We provide 2 weeks paid vacation. We take care of expenses for Face to Face interviews. We provide Relocation expenses for the candidate. We provide one-week accommodation in client’s place unless you have your own arrangements. F1/CPT/OPT/Green Card/ USC /H4 EAD/L2 EAD are eligible. Job Types Full-time, Contract, Commission Salary $55,000. 00 $65,000. 00 per year Benefits Dental Insurance Employee Assistance Program Employee Discount Flexible Schedule Health Insurance Life Insurance Paid Time Off Parental Leave Professional Development Assistance Referral Program Relocation Assistance Tuition Reimbursement Vision Insurance Schedule Monday to Friday Supplemental Pay Bonus Pay Commission Pay Signing Bonus Tips Education Bachelor's Required Work Location One location Work Remotely Temporarily due to COVID-19"
94,"Data Analyst II, Prime Clerk",data analyst,/rc/clk?jk=7ba3598c2b575ffa&fccid=2b6243d317042ea1&vjs=3,"Prime Clerk is the leading global provider of complex claims administration and business services. Through our proprietary technology platforms and industry experts, we provide a full suite of claims administration services, leading notice media solutions, a robust business services platform and unrivaled subject matter expertise in global corporate actions. As a division of Duff & Phelps, Prime Clerk and its affiliates employ more than 3,500 employees in over 70 offices around the world. The Data Analyst position is primarily responsible for supporting the Client Services department related to data parsing, importing, managing and analyzing of client data. The selected candidate will work as part of a high functioning team, tasked with supporting day to day operations of a busy Legal Services provider. Independently define and develop solutions, with minimal direction, for a wide range of technical problems. Participate in team meetings and other collaborations, as well as design and peer reviews. Be flexible; prepared to approach things laterally; able to think outside the box. Interface with business stakeholders, to identify or clarify requirements in order to address various problems. Responsibilities Serve as a resource for executing ad-hoc data requests, custom reports, test scenarios, data migration, data analysis and quality assurance testing in an operations environment. Develop robust spreadsheet models with clearly defined inputs, calculations, and final output. Understand, evaluate and provide documentation, including data flow mapping, for new and existing spreadsheet models. Author queries to extract data from corporate sources for data analysis & reporting. Rearrange files, directories, and subdirectories in the DOS command line. Import data files into a Microsoft SQL Server database. Characterize results of data analysis in clear and simple terms for non-technical staff. Restructure and reconcile Excel files. Develop computer programs or macros to help automate repetitive manual tasks. Develop automated and reusable routines for extracting information from various data sources. Utilize VBA in new and existing Excel-based tools to automate recurring work processes. Integrate capability in Excel-based tools to both pull and push data from/to Share Point Server. Work with data in SQL Server databases, Oracle databases, and SAS Requirements A bachelor’s or master’s degree in mathematics, computer science, physics, chemistry, engineering, finance, accounting, or economics Experience in coding, programming, macros, or scripting Flexibility and willingness to figure it out as we go along Enthusiasm and desire to learn lots of technical skills Preferred Skills Experience with SQL queries. Experience with Excel VBA macros. Experience in complex formula authoring, array functions, dynamic named ranges, pivot tables, data validation, V & H lookups and conditional formatting Experience with computer programming Java, Ruby, Python, Perl, R, SAS, C, C++, etc. Experience with DOS command line. Experience with Share Point Designer, Info Path, Web Parts and workflow creation is a PLUS In order to be considered for a position at Duff & Phelps, you must formally apply via careers. duffandphelps. jobs Duff & Phelps is committed to providing equal opportunities in employment. We will not discriminate between applications for reason of gender, gender identity, race, religion, color, nationality, ethnic origin, sexual orientation, marital status, veteran status, age or disability. "
95,Senior Data Analyst - DSP,data analyst,/rc/clk?jk=43c8363a267e4daa&fccid=d440534677583830&vjs=3,"The Programmatic Analytics and Strategy team at Zeta Global provide actionable insights to both external customers and internal teams. For external customers, analysts partner with Sales and Customer Success teams to monitor our optimization platform, design Real-time Brand Optimization surveys, introduce humanly designed segments to boost model exploration and learning, and deliver actionable insight presentations and lead marketing strategy discussions with clients. For internal teams, they provide feedback to our Product and Engineering teams on automation and model features to advance the reliability, reach, and effectiveness of programmatic campaigns. What you’ll do Own campaign performance and reporting insights while recommending opportunities for optimization. Weekly review of performance of models across revenue, performance, and pacing. Design and implement tests to evaluate model and feature performance. Work with clients to understand key business challenges and design analytic and testing plans to derive actionable intelligence. Work with clients and internal teams to create real-time brand optimization studies that align with campaign goals, but also create strong, clear signals that our models can use to optimize campaign delivery and performance. Consult with clients regarding development & alignment of marketing programs across on-line channels. Determine set-up, including testing methodologies, reporting requirements, and optimization modeling requirements, for ensuring full impact and efficiency of optimization efforts. Communicate campaign optimization results to clients to prove value against original business goals. Who you are A great communicator who is able to convey complex technical features in simple terms. Someone with an aptitude for media and strategy and able to contextually relay concepts to clients. Able to multitask and prioritize among several high-profile clients. Have a high degree of creativity, self-motivation, and drive. Eagerness to work in a startup team environment that will be rapidly changing Enthusiastic team player with a penchant for collaboration and knowledge sharing. Willingness to do whatever it takes to get the job done. Nerdy but loveable. Data driven, technical, self-starting and curious. What you need 3-5 years of working experience in a similar role. Extensive experience with SQL and Excel/pivot tables. Excellent presentation/visualization/storytelling skills with Power Point. Excellent diagnostic skills. Technical acumen to understand how to design campaigns that optimize the use of our tech platform. Professional oral and written communication skills. Client facing experience. Bachelors degree preferred in Business, Finance, Economics, Statistics, Marketing or equivalent experience. Bonus if you have Experience in a digital media analytics role Experience with VBA/Excel Macros, Tableau, Python, SAS or other data manipulation tools a plus. #LI-MC1 About Zeta Global Zeta Global is a data-powered marketing technology company with a heritage of innovation and industry leadership. Founded in 2007 by entrepreneur David A. Steinberg and John Sculley, former CEO of Apple Inc and Pepsi-Cola, the Company combines the industry’s 3rd largest proprietary data set 2. 4B+ identities with Artificial Intelligence to unlock consumer intent, personalize experiences and help our clients drive business growth. Our technology runs on the Zeta Marketing Platform, which powers ‘end to end’ marketing programs for some of the world’s leading brands. With expertise encompassing all digital marketing channels – Email, Display, Social, Search and Mobile – Zeta orchestrates acquisition and engagement programs that deliver results that are scalable, repeatable and sustainable. Zeta Global is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, gender, ancestry, color, religion, sex, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veterans status, or any other basis protected by law. Zeta Global Recognized in Enterprise Marketing Software and Cross-Channel Campaign Management Reports by Independent Research Firm https //www. prnewswire. com/news-releases/zeta-global-opens-ai-data-labs-in-san-francisco-and-nyc-300945353. html https //www. prnewswire. com/news-releases/zeta-global-recognized-in-enterprise-marketing-software-and-cross-channel-campaign-management-reports-by-independent-research-firm-300938241. html"
96,Data Analyst Health Outcomes,data analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0AfhXZLdKKEaSrE-A3kTViGcJyfCs_ENjKb6b0iXAiczLqB7mSoCRBqdcY7y0LQ3qZCA1yjofnIP_A1VsiuY13592dygRpFGWBrbBsw-qL4kL8Lyd6e7eO5a2VZf_IiqGckPnkEqFxXx41q6DpM9MaqarDE6Pp5NtZ9PZovsG1q9jh1FaGXEWaPRhSVAj3RAZ2OqsIhR2AqlrxfsIWSLVefaZTT-9ekIoosryKMjSvxfuQl7WOpO4ZlXz9H1dkVOD7g1yBZL7QAcdpfmRRTCZWd8l0ADyD1RoUW1RhJDyMG5DS_q49qwgsVwjG9BDH8NE11rMFqNVJMlCWC8SrPZSc4yHb1UpgERNMN2_auiiMtsBdehMqbM0J9_D1AGf0978caAbm-304WF-ksvM8vS9YNhDKGbnE3HXgDRZoizx4dN9W9CQ4IqWygfD_68ndBS2AwQ37ghZzo_0f1CgnJsXRnwp0vs9LYrNDOws4nhX9KgAcarZ8-8eX7QaAk91-cTNBiYLA4I7aSZJjT6HIvRsbLJjrSbcR3nwbv0Vb4h9pXC_toZWRLmOcYyuXHy0wwBOrZmi6P4mDUKPNhT5v9NP_Nq7EXZ8LpJ-c70cBKC0sOMJxRTGWtliq-4PMklh3uS03WrkufD8JP6F47Fr2oOeVp7vqbHb5rCtiEd8pFM_AxTkbmrfmMadC5hsd4hC3ppilQ-Fgk-4i74JGJc3WXpxXLzy2Hlu34BWwfKXajx0SlsBR-SYSf1wOXfaSLv-6_lmCPqjvUr7H3d5QdvIhZFBpcpHLdyz7Vk74hrpglV9pN5WWs_DENACZrckolu7cSDRxRY0WzE6ZWy8JZdLIsqHerL8VDODlKjQOK9iphRNavWsIDCFMeIeG9X28g7Pnvf0Y=&p=11&fvj=0&vjs=3,"Company Overview At Memorial Sloan Kettering MSK , we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. For the 30th year, MSK has been named a top hospital for cancer by U. S. News & World Report. We are proud to be on Becker’s Healthcare list as one of the 150 Great Places to Work in Healthcare in 2019, as well as one of Glassdoor’s Employees’ Choice Best Place to Work for 2019. We’re treating cancer, one patient at a time. Join us and make a difference every day. Job Description We Are The Plastic and Reconstructive Surgery Service is seeking a Health Outcomes Data Analyst to integrate and become part of an active research program. Ultimately, the goal of our program is to improve clinical decision making and help us refine our surgical techniques to improve clinical care. You will Conduct quantitative research including data cleaning, large-scale data analysis, logistic regression, analysis of categorical data, linear regression, etc. Utilizing large-scale state inpatient claims databases including HCUP , and analyzing impacts of the ACA on rates of breast reconstruction. This is an area of impactful translational research, focused on how health economics has impacted disparities in breast reconstruction. You are Experienced in analyzing inpatient claims databases. Knowledge of health policy, especially the ACA, and understanding of financial or economic analysis. Knowledge of either SPSS or SAS, logistic regression, analysis of categorical data, linear regression. Ideally, scientific writing and grant writing a plus. You need Bachelor's degree or Masters MPH, MHS, or similar , preferably in epidemiology, biostatistics, or health policy and management with a focus on comparative effectiveness outcomes research or equivalent experience. Advanced training in research methods preferred. Skills in quantitative research, combined with knowledge of health economics required. Benefits Competitive compensation packages | Sick Time |Generous Vacation+ 12 holidays to recharge & refuel| Internal Career Mobility & Performance Consulting | Medical, Dental, Vision, FSA & Dependent Care|403b Retirement Savings Plan Match|Tuition Reimbursement |Parental Leave & Adoption Assistance |Commuter Spending Account |Fitness Discounts &Wellness Program | Resource Networks| Life Insurance & Disability | We believe in communication, openness, and thinking beyond your 8-hour day @ MSK. It’s important to us that you have a sense of impact, community, and work/life balance to be and feel your best. Our Hiring Process You review the posting, agree it sounds like a great fit & apply ->Talent Acquisition contacts you to schedule a phone interview if your profile aligns -> after speaking with the Talent Acquisition Specialist, you will connect with the Hiring Manager by phone or video -> if your experience is a fit, you will move forward to a video call or on-site visit with the team -> post-interview feedback->ideally an offer! ->reference check & onboarding-> orientation & official welcome to MSK. #LI-POST Closing MSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision. Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. "
97,Data Analyst,data analyst,/rc/clk?jk=524041b9f0e8babd&fccid=a6dcbec0fe72d871&vjs=3,"Overview maslansky + partners is a language strategy and research consultancy that helps solve complex and high-stakes communication challenges for the world’s largest and most innovative companies, industry trade groups, and non-profits. Clients come to us to help them figure out how to stand out in the marketplace and increase sales. To win on issues and navigate crisis. What We’re Looking For We are looking for a Data Analyst who loves using data to tell stories and is able to develop a deep understanding of what we do so that we can do it even better. You must be an engaging communicator with the confidence to interact at all business levels across the organization, who can easily translate specialized concepts for any internal or external audience. You must be excited about establishing and growing a Data Analytics department within an organization. Four reasons to join the maslansky + partners team To be challenged and grow We’re looking to hire someone who is always asking the question, what’s next? We offer a variety of opportunities to help build and develop your skills and help you grow in a way that will meaningfully contribute to the trajectory of your career. To make a contribution Our clients come back to us because our team delivers great work every time. And our teams deliver great work because of who they are and how they work together. Small teams with big opportunities. No matter your job description, we encourage you to learn and participate in the broader business. If you are good, you will not get lost. . . your contribution will be recognized and rewarded. To have an impact Whether you want to help underprivileged students increase their chances of success, increase access to housing, support gender equality, reframe the debate about climate change, or pursue another passion, we actively look for opportunities to complement our corporate work with clients and pro bono opportunities with purpose. To have a good time We do serious work but never take ourselves too seriously. The best work gets done when great people like working together so that is the kind of culture we’ve created. Lots of work and lots of fun to help you stay energized and engaged. Responsibilities What You’ll Do Your role will include a number of important components from working directly with our client-facing teams to deliver compelling data insights and visualizations, to developing new data analytics-focused offerings that help our clients communicate effectively. This includes Be a subject matter expert for data analytics across the organization Work directly as a part of client-facing teams doing data analysis and visualization for client deliverables Become an expert user and advocate of our data analytics tools, manage relationships with data analytics vendors, source new leads for tools and vendors Develop internal best practices, improve current methods of data analysis, and innovate on new methods to achieve client objectives Collaborate on the backend development for a new internal tool currently in early development, and design future tools collaboratively with the Growth & Innovation team Support the continuing backend development of our proprietary PR Messaging Platform, Dynamic Response Help guide the future of Data Analytics at our company together with the Head of Operations and the Growth & Innovation team Qualifications What An Ideal Candidate Will Bring Technical skills >2 years in a similar data role, ideally in industry and agency setting A bachelor’s degree, preferably in a field requiring rigorous logical thinking such as the natural or social sciences, linguistics, computer science, philosophy, mathematics, or similar Familiarity with basic statistical concepts distributions, significance, sampling Experience constructing boolean searches, working with unstructured text, analyzing large datasets, automating processes, and simplifying complex data A good understanding of SQL and an ability to write multi-part queries Experience with at least 1 scripting and data analysis programming language Python, R a plus A strong command of Microsoft Excel pivot tables, formulas, basic macros Familiarity with at least 1 cloud technology stack a plus AWS Cloud Stack a plus Familiarity with media and social analytics platforms a plus e. g. Brandwatch, Netbase, Crimson Hexagon, Quid Someone who will excel in this role also has Intellectual curiosity Ability to reason through complex systems and relationships Comfort with ambiguity and ability to make reasoned decisions to work through it Collaborative approach and likes to learn from different disciplines and levels across the organization Comfort taking initiative and proactively offering recommendations through analysis of various data sources Experience juggling multiple priorities in a fast-paced work environment Ability to communicate complex analyses, data-heavy insights and resulting recommendations to a range of internal and external stakeholders with a simple, clear, compelling story Patience for some tedious tasks as well as critical thinking for more sophisticated projects Not just openness to, but active desire for in-the-moment feedback both constructive and appreciative Desire to mentor and coach more junior employees beyond a data analytics capacity, focusing on overall development and growth within the company and their careers"
98,"Regulatory Financial Data Analyst, Commerce and Ads, Finance",data analyst,/rc/clk?jk=7c66176ec65a0673&fccid=a5b4499d9e91a5c6&vjs=3,"Note By applying to this position your application is automatically submitted to the following locations New York, NY, USA; Mountain View, CA, USA Minimum qualifications Bachelor's degree in Computer Science, Electrical Engineering, Math, Economics, Finance, or related field, or equivalent practical experience. 4 years of experience with SQL, or time series analysis and data systems. Experience with spreadsheets and presentation software. Preferred qualifications Experience with data visualization techniques using tools such as Tableau, Python, and/or R. Knowledge of financial systems and business application, including accounting and project management experience. Ability to drive cross-functional initiatives and multi-faceted projects, and deal with ambiguity. Excellent analytical, communication, and quantitative skills. About the job Financial Analysts ensure that Google makes sound financial decisions. As a Financial Analyst, your work, whether it's modeling business scenarios or tracking performance metrics, is used by our leaders to make strategic company decisions. While working on multiple projects at a time, you are focused on the details while finding creative ways to solve big picture challenges. As a Commerce Regulatory Analyst, you will help support ongoing engagement with regulatory authorities. You will be a key member of a cross-functional team delivering regular updates to both external regulators and internal stakeholders. You will drive analyses that help build our business for the future within regulatory guidelines. In this role, you’ll have the opportunity to collaborate with teams across Europe. The name Google came from ""googol,"" a mathematical term for the number 1 followed by 100 zeros. And nobody at Google loves big numbers like the Finance team when providing in depth analysis on all manner of strategic decisions across Google products. From developing forward-thinking analysis to generating management reports to scaling our automated financial processes, the Finance organization is an important partner and advisor to the business. Responsibilities Provide management with analytical insights into business drivers, opportunities, and risks, reflecting the evolving regulatory and business requirements of our Shopping and Travel Ads businesses. Develop and maintain processes and systems, in partnership with cross-functional teams, that ensure continued regulatory compliance and support the needs of our growing business. Ensure submission of official regulatory reporting and ad-hoc requests for data insights. Partner with leaders across various organizations to provide actionable business recommendations, as well as implement methodologies to measure success. Track and report on cross-functional efforts while problem-solving and/or escalating issues as needed. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form. "
99,Senior Data Analyst,data analyst,/rc/clk?jk=d55e4d436577eea8&fccid=4b5d257051285786&vjs=3,"Job Summary We are looking for an experienced Data Analyst to join our Data Instrumentation and Governance team to partner closely with our marketing analytics organization. This role requires acute attention to detail, a strong sense of accountability, collaboration skills, and extensive hands-on experience with large datasets and industry standard marketing tools. In this role you will be working on data analysis, KPI definitions, data investigations, data lineage, instrumentation and deployment of critical marketing datasets while ensuring data quality and completeness across the complete data lifecycle At DSS, data is central to measuring all aspects of the business, and critical to its operations and growth. Data Instrumentation works closely with data stakeholders, front-end technology teams, data engineering, and back-end services for consistent collection and usage of business critical KP Is and data points. Responsibilities Understanding marketing business requirements and translating them into actionable specifications while owning end-to-end data lifecycle including how data should be tracked and measured for analysis, reporting and data quality assurance. Perform cross system data investigations and analysis to discover data inconsistencies, outlining path to remediation for development teams. Understand the systems and business processes that populate critical systems with data. Work with marketing analytic stakeholders to outline and define data definitions and apply appropriate usage. Perform data audits, identify data collection issues, suggest improvements, and help implement fixes. Work to support engineering teams tasked with ingestion of critical datasets as a subject matter expert. Create and manage key data quality health KP Is through regular metric monitoring and revise as needed as the number of datasets and critical systems expand. Work as a lead data steward hand-in-hand with the marketing product owners and managers to convey data quality needs. Document and implement data quality policies, standards, and procedures for both legacy and new data environments and platforms. Basic Qualifications 4+ years of analytics and technology experience with a focus on Advertising and Marketing Strong knowledge and experience with Python, Spark and SQL is required A strong understanding of core marketing analytics concepts including impressions, clicks, pageviews, events, sessions, users, conversions, look back windows and attribution models High familiarity with data platforms and applications such as Databricks, Jupyter, Snowflake, Redshift, Airflow Strong experience in documenting the data requirements, data strategy, data rules standardization, cleanse, and validation Experience working across marketing platforms such as Kochava, Adobe Analytics, Adobe Tag Manager, Flashtalking, Google Ads, Campaign Manager, DV360, Bing Ads, Facebook, Datorama, Segment, Alooma, Blue Kai and Salesforce Marketing Cloud Exceptional interpersonal skills and written communication skills Strong analytical and technical skills to troubleshoot issues, analyze the cause, quickly come-up with the possible solution s , document the changes, and communicate to the change to the organization Ability to evaluate risks and provide recommendations / solutions in a timely manner BA or BS in a quantitative field Business, Math/Statistics, Economics, CS, Engineering, or similar is desired Preferred Qualifications Experience in technology industry, knowledge of data and product is a plus. "
100,Lead Data Analyst - Infrastructure Engineering,data analyst,/rc/clk?jk=af76d0cb50586708&fccid=b4048be2884af072&vjs=3,"TECHNOLOGY SERVICES GROUP TSG is the central infrastructure services group with a proven track record of innovating to help BNY Mellon and its customers have most reliable, nimble and cost-effective solutions in the financial services market place. BNY Mellon App Engine technology is leading edge with industry first Pas S/Iaa S fully integrated solution, combined with enterprise standards for developers. TSG is building next generation scalable, efficient data center as a service using the latest innovation infrastructure. Building on top of the cloud deployments in BNY Mellon, we are aggressively automating and bridging the green field cloud with the existing virtual computing environments. This role within TSG Engineering is responsible for assisting with the design, creation and design of executive-facing reporting across all facets of the organization as well as continuous service improvement through process engineering and business analytics. The successful candidate will be challenged to solve complex problems related to process optimization, data visualization and organizational strategy to support the requirements of senior leadership as well as users across the organization. Key responsibilities include Conducts analyses and reporting in support of general business operations, special projects/initiatives and/or operational planning activities. Document technical processes via logical data flow and workflow and flow charts to show process gaps and improvements. Analyses and synthesizes the data and compiles it into informational and decision-seeking reports, analyses and/or presentations for senior management. Communicates effectively with senior managers to contribute to strategy development and documentation. Lead Data Analyst->> Analyzes application requirements and develops conceptual, logical and first-cut physical database designs data models . Creates associated data model documentation such as entity and attribute definitions and formats. Assists in logical data designs to deliver stable and flexible high performance data solutions. Investigates and corrects data discrepancies by reconciling faulty codes. Provides data element naming consistent with standards and conventions and ensures that data dictionaries are maintained across multiple database environments mainframe, distributed systems . Ensures data content/quality by planning and conducting moderately complex data warehouse system tests, monitoring test results and taking required corrective action. Acts as a liaison to data owners to establish necessary data stewardship responsibilities accountability for a particular data element/verifying accuracy of the data element before loading it into the database and procedures. Analyzes and designs data models, logical databases and relational database definitions using both forward and backward engineering techniques. Seeks opportunities to promote data sharing, and to reduce redundant data processes within the corporation by identifying common structures across application areas. Contributes to the achievement of related teams' objectives. Qualifications Bachelor's degree in computer science or a related discipline, or equivalent work experience is required. Four to six 4-6 years of experience in data modeling, data warehousing, data entity analysis, logical and relational database design, or an equivalent combination of education and work experience is required. Experience in the securities or financial services industry is a plus. Strong analytical and data manipulation skills; advanced in Microsoft Excel Data Models, Pivot Tables, nested formulas and Power Point. Experience with automated self-service reporting tools e. g. Qlik, Tableau . Exceptional verbal and written communication skills. Self-starter, desire to learn, ability to prioritize, multi-task and meet deadlines. One to two 1-2 years of relevant industry experience is preferred. MBA or advanced degree in applied statistics, data science, economics is preferred. Python for data analysis and visualization is preferred. Knowledge in SQL and PL/SQL procedures to extract, transform, and load data from various sources is preferred. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums. Primary Location United States-New Jersey-Jersey City Internal Jobcode 45092 Job Information Technology Organization Technology Services Group-HR06725 Requisition Number 2008697"
101,Data Analyst I-MSH/SSVS/ELM,data analyst,/rc/clk?jk=ac686e9464b5498b&fccid=c007936ceb766fe5&vjs=3,"Strength Through Diversity Ground breaking science. Advancing medicine. Healing made personal. Roles & Responsibilities The Data Analyst will be responsible for gathering and analyzing healthcare data from multiple sources to extract trends and business insights. The primary responsibility for this position is to fully support the ongoing healthcare business intelligence and data analytics objectives of the Division. The Data Analyst will enjoy working with healthcare data and have a lively curiosity about analytical tools. The Data Analyst would also push the boundaries of what technology can do to empower our clinicians and our administrators to enhance clinical operations while providing them with data they need to grow the business. Specific responsibilities Include Pulls and integrate data from disparate sources. Analyzes, presents and explains information in accessible and actionable way that helps informs practice operations and informs business growth. Develops, monitors, and distribute analytic reports on a recurring basis. This includes monitoring data from various databases and creating monthly, quarterly and ad-hoc reports that captures physician KPI’s data and other metrics. Build models and data visualizations i. e. tables, graphs, etc. as required; lead regular stakeholder meetings. Summarizes large volumes of encounter data in analytical reports for management review and use Reviews and ensures accuracy of all ad-hoc and automated reports and corrects discrepancies. Uses Excel, EPIC, BI system and other appropriate software to generate reports based on input/direction from management and other report users. Collaborate with management and internal teams to implement and evaluate improvements. Requirements Bachelor's degree in health services research, statistics, or economics OR minimum four 4 years of data analyst or related experience, including proficiency with analytical software. At least 1-year experience Data Mining complex sets and formulating custom reports. Experience performing data analysis from EPIC, Claims, Rosters, etc. in a health care or health plan organization, required. Expert knowledge of Excel and strong hands-on familiarity with Epic Formal informatics training a plus. Strong analytic and reporting skills required. Effective oral, written and interpersonal communication skills required. Strength Through Diversity The Mount Sinai Health System believes that diversity is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize medicine together. We work hard to acquire and retain the best people, and to create a welcoming, nurturing work environment where you can develop professionally. We share the belief that all employees, regardless of job title or expertise, can make an impact on quality patient care. Explore more about this opportunity and how you can help us write a new chapter in our story! Who We Are Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve. Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospital campuses, including Mount Sinai Beth Israel, Mount Sinai Beth Israel Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West formerly Mount Sinai Roosevelt , Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai. The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. EOE Minorities/Women/Disabled/Veterans"
102,eCommerce Data Analyst,data analyst,/rc/clk?jk=76d5bff104ea7e20&fccid=b66bfc184e5a8902&vjs=3,"About Fair Harbor Fair Harbor makes ultra-comfortable beachwear made from recycled plastic bottles. Founded in 2014 by siblings Jake and Caroline Danehy, Fair Harbor was started with the goal of helping promote the mitigation of single use plastics and keeping waste out of our oceans. Here at Fair Harbor we create products for you to enjoy the places we love, while protecting those places too. About The Role This role will be responsible for managing and maintaining all Fair Harbor Data. This position will be the first of its kind at Fair Harbor and as such, the person in this role must be able and excited to take full responsibility for building out and maintaining internal tracking and reporting procedures. It is imperative that the person in this position is analytical, detail oriented and a critical thinker. The person in this position will be required to query, analyze and report on data related to all of Fair Harbor's Key Performance Indicators, including marketing attribution, Lifetime Value and demographic selling trends, among many others. Key Responsibilities Work day to day in Fair Harbor's Data warehouse to splice data and analyze trends Conduct analyses to accurately attribute sales through different marketing channels Create and maintain various reports to be presented to the executive team on a regular basis Construct detailed models anayalizing and projecting channel CPA's, customer LTV's and marketing attribution Build detailed product selling reports to analyze selling data in numerous contexts Position Requirements Expert in excel and data analysis Proficient in SQL, with the ability to query and manipulate all data contained within the Fair Harbor Data Warehouse Extremely fast thinking, creative and detail oriented Has general knowledge of e Commerce and how our business works Motivated and enthusiastic; a great team player who is also able to work independently and own their function What We Offer Competitive Compensation Health and Dental Benefits Advancement Opportunities Open PTO Policy 10 Company Holidays annually Holiday Calendar https //docs. google. com/spreadsheets/d/1Xr4Zn A Nw T Qj3m X Pott M Egt T4Ygwqn YRK-l A4Gp Zdu28/edit#gid=0 Weekly out of office team activities *currently on hold due to COVID Company Culture Fair Harbor Family We believe in turning our employees and customers into family Sustainability We believe in leaving the world better than we found it and using our products as a platform to promote the mitigation of plastic waste. We regularly engage in company cleanups to spread awareness and educate about the impact of plastic waste Fun We believe in letting loose, enjoying the special moments when we are off duty and building products that help us and our customers enjoy our time to the fullest Collaboration We trust and respect one another, which contributes to our collaborative work environment"
103,Data Analyst - Answers,data analyst,/rc/clk?jk=2ff8ae9550408ce4&fccid=2358f3d9df359353&vjs=3,"New York We are The Wix Answers team. Wix Answers is a customer support platform developed and built in-house to support our over 190 million users. Recently, Wix started selling its customer support platform to SM Bs and enterprise companies that want to create better customer experiences for their brands. Wix Answers is just one piece of the innovative, cloud-based web development products that has made Wix’s value more than $5 billion. We’re looking for a Data Analyst to help us start our business intelligence function and create our first marketing sales data model within Wix Answers. You are A Data Analyst with 2-5 years’ experience working with and modeling various types of data sets. You’re motivated by solving business problems through data, thinking through marketing attribution cases, and being an internal leader who can shape strategy through data. As a Data Analyst you will Partner with the Marketing and Sales teams to create our revenue model, and arm internal stakeholders with the insights and information to make data driven decisions. Develop an understanding of Wix Answers’ business to support marketing and sales teams. Assist in the management of Wix Answers’ data pipeline Fivetran, Snowflake . Create and manage the sale and marketing data model using Look ML Looker . Lead the creation and management of our multi-touch attribution models. Train internal stakeholders on using Looker reports. Consistently bring data to the forefront of our conversations. Work with internal developers to create an internal event structure for web properties. "
104,Data Analyst,data analyst,/company/NuSources/jobs/Data-Analyst-f5c9ae07984b44fb?fccid=cea282df01e4f50c&vjs=3,"Title Data Analyst with Data Scanning and Data Profiling experience Terms Contract 6 – 12+ months renewable Location NYC office. Remote for now, onsite when city reopens. Responsibilities · Responsible for determining systems requirements for new or modified database application programs· Creates the system specifications. · Responsible for the development, testing and implementation of efficient, cost effective application solutions. · Will receive general direction from the Manager, work closely with business analysts to identify and specify complex business requirements and processes. · May coordinate the activities of the section with the client area and other IT sections e. g. , database, operations, technical support . · Work in conjunction with the data architect/modeler on the data warehouse reporting solution. · Possess expertise in writing and tuning, view, stored procedures, and functions. Strong problem-solving, interpersonal, written and oral communication skills with demonstrated ability to simultaneously adjust to changing priorities, successfully completing multiple tasks. · Prepares deliverables such as source-to-target mappings; transformation rules documentation Qualifications · Experience and knowledge in Be Cubic or ASG-DI / Rochade required. · 7-10 years of relevant work experience required. · Must have Data Scanning, Data Profiling, Data Cataloging experience. · SQL Server, DB2, Oracle. · Must have knowledge of lineage and financial services industry, technical metadata layer. Job Types Full-time, Contract Schedule 8 Hour Shift Monday to Friday Experience Data Profiling 3 years Required Data Cataloging 3 years Required Data Scanning 3 years Required Be Cubic or ASG-DI / Rochade 3 years Required SQL Server, DB2 or Oracle 3 years Required Location New York, NY 10018 Required Language English Required Work authorization United States Required Contract Length 7 11 months1 year More than 1 year Varies Full Time Opportunity Yes Work Location One location"
105,Senior Data Analyst,data analyst,/rc/clk?jk=37286a7c41279f5e&fccid=1794d21df18d629b&vjs=3,"mindbodygreen is looking for a Senior Data Analyst to own analytics, strategic insights and data infrastructure for a fast-growing and dynamic direct-to-consumer e Commerce & digital media business. In this role, you will uncover deep insights to inform business, marketing and content strategy. This position reports directly to the CFO. Key Responsibilities Own analysis on customer lifecycles, lifetime value, publishing efficiency, SEO, ROAS, and more; present findings/recommendations to leadership team on a regular basis Analyze and understand onsite user behavior to inform product strategy in partnership with UX/marketing teams Develop and manage all reporting and analysis efforts for e Commerce DTC supplements, online classes , mbg editorial and leadership teams Own entire analytics engineering workflow, including using DBT for analytics engineering and Amazon Redshift for warehousing Develop KP Is and regular reporting to understand mindbodygreen audience, email, content, marketing customer acquisition costs, retention, lifetime value , and sales performance Manage data requests from teams across the org and develop existing Looker model; train colleagues regularly to institute a self-service data culture About Your Experience and Skill Set You have 2-4 years of experience working in an analytical/data-centric role in a fast-paced, environment preferably at an e Commerce or direct-to-consumer company You are comfortable working with large data sets and are fluent in SQL; familiarity with Python or another scripting language a plus You are capable with Looker, have developed in Look ML, and have built Explores, Looks and Dashboards You have hands on data modeling experience with DBT, including developing models, materializing compound tables, and working with macros Experience with building A/B tests and reporting performance a plus About You You’re intellectually curious, unafraid to ask questions, and eager to dive in to solve ambiguous problems to drive the direction of the company You have a passion for efficiency – a track record of automating manual processes and bringing together multiple data sources in a scrappy, scalable way Your communication skills are excellent, and you enjoy teaching others You’re interested in not just providing data, but understanding the “so what”, distilling data into actionable insights from both a right and left brain perspective You’re able to prioritize competing tasks and think strategically in a fast-paced, startup environment A tech, health & wellness enthusiast, you are looking to work for a mission-based company Perks include Working with an incredibly talented, smart, and ambitious team Exposure to senior leadership and the ability to work with every team across the org Opportunity for career and professional growth Free access to all mindbodygreen online classes Health Insurance, 401k, Commuter Benefits, and more via Justworks Can work anywhere!"
106,CRM Data Analyst,data analyst,/rc/clk?jk=0a678ed896f11c8a&fccid=ea5981f4317746da&vjs=3,"Veronica Beard is an elevated American ready-to-wear brand that strikes the balance between cool and classic. The brand provides a modern perspective on iconic wardrobe pieces and delivers a lifestyle offering that has expanded to include jeans and shoes. Thoughtfully designed with feminine silhouettes and refined tailoring, the collection is intuitively built for real life – empowering her from day to night, work to weekend, and everywhere in between. The purpose of this role is to work cross functionally across the organization to identify and develop omni channel insights of the Veronica Beard customer. This role will leverage data to provide insights and identify opportunities to further develop segmentation and targeting initiatives to maximize customer lifetime value. Candidate will have the opportunity to provide data-led optimizations to dive acquisition and retention campaigns across channels Responsibilities Manage CRM data architecture and partner cross-functionally with Retail, Digital Marketing and Ecommerce teams to build, maintain and optimize all CRM data pipelines. Continuously monitor CRM data hygiene inputs from data warehouse/data sharing tools and outputs to end users, ensuring fluctuations do not fall outside of standard deviations. Own data activation roadmap and requirements to advance real-time, cross-channel and personalization use-cases. Support team in building comprehensive reports and dashboards of all relevant CRM KP Is to track and socialize performance across individual and evergreen/lifecycle campaigns. Employ data-driven insights and an iterative approach to optimize campaign builds, content modules, segmentation and testing constructs. Work cross functionally with all DTC partners to develop an in-depth understanding of the customer journey at Veronica Beard and identify churn behaviors that allow for us to implement strategies to maximize customer retention Provide Sr. Leadership Team & DTC business partners with insights on the health of our customer base by using advanced analytical techniques and making recommendations for optimizations to assist with revenue and conversion goals. Requirements 2+ years of professional analytics experience is a must; CRM, direct/retention marketing or data analytics Strong analytics skills and working knowledge of data visualization/reporting tools Chartio, Looker, Tableau, etc. BS or MS degree in computer science, mathematics, economics, statistics, engineering, or a related field is required Experience working SQL and Business Intelligence tools i. e. Chartio is preferred Must be proficient in Excel & Google Analytics Self-motivated, results driven, highly organized, have strong attention to detail & excellent communication skills A self-starter personality that can thrive in the fast-paced environment of a rapidly growing company. Ability to work well in high-pressure, short-deadline situations. "
107,"Data Analyst, STARS",data analyst,/rc/clk?jk=14d07d14e9e78fc2&fccid=402a1c36c3f1508f&vjs=3,"Hi, we're Oscar. We're hiring a Data Analyst to join our Value Capture team in our New York office. Oscar is a technology-driven, consumer-focused health insurance startup founded in 2012 and headquartered in New York City. Our goal is to make health insurance simple, transparent, and human. We need your help to do so. About the role This position would create and further automate the predictive Medicare Advantage Stars dashboard. Additionally, it would create HEDIS metrics and other Star measures as needed for real-time tracking of current performance. You will also support the Data analysis needed for forecasting future performance trends and behaviors, provider cut point setting, and high priority member identification that is needed to refine improvement campaigns. You will report into the Director of Stars and be part of the Stars Improvement team that is responsible for ensuring that we partner with our Members to improve their health and achieve at least a 4 Star rating. You will report into the Director of Stars Responsibilities Build dashboards to monitor progress of Value Capture programs and escalate issues with key stakeholders Deliver comprehensive analytics and forecasting/trend modeling tools based on complex data sets to inform Star interventions Recommend solutions for business problems based on analysis of data and business processes Analyze data that will drive business intelligence and enterprise Star strategy Design and operationalize strategically data driven risk stratified member engagement campaigns and monitor their success Requirements Bachelors in Business, Data Analytics, Health Administration or related field Working knowledge of CMS Star Rating System including HEDIS, CAHPS, HOS, PDE, and administrative measures 5+ years of experience in Stars related Data work 3+ years of SQL Experience Life at Oscar At Oscar, being an Equal Opportunity Employer means more than upholding discrimination-free hiring practices. It means that we cultivate an environment where people can be their most authentic selves and find both belonging and support. We're on a mission to change health care an experience made whole by our unique backgrounds and perspectives. We encourage our members to care for their whole selves, and we encourage our employees to do the same with comprehensive medical benefits, generous paid-time off, paid parental leave, retirement plans, company social events, stocked kitchens, wellness programs, and volunteer opportunities. Reasonable Accommodation Oscar applicants are considered solely based on their qualifications, without regard to applicant's disability or need for accommodation. Any Oscar applicant who requires reasonable accommodations during the application process should contact the Oscar Benefits Team accommodations@hioscar. com to make the need for an accommodation known. Pay Transparency Policy Oscar ensures that you won't be discharged or discriminated against based on whether you've inquired about, discussed, or disclosed your pay. Read the full policy here. "
108,Business / Data Analyst,data analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0DNoILxOnWIsoHfQE7VRgrjw9NlRoJFeLDlYBIP8lxdz3xvFd0TYpQsIl18Tz9ZV7lqWj8NMa180lSnFvuvZzzeBm-pzeUESm_HiParFdlTOtTkdHbWyKIB0b_oeMEx6az5iOlv9RqmyQmDJgeTc2l_HyXvcs1Fn1mloe0WzZCO6wsIxrwXTijIv15uLYcQhqtDt8XOW1S3ZLUrOWIp4LJmlYqBG_fcfvQGjyNTFOsrUGML98t_gELAnEvB3N1hdKJ1sizZ3cbYBWgkOvqaeXoXBjS3hspT31LB6H8Ye08xPTcZzbVfJc5DWgtYQ20Neu_avooVmhuY7yzNbpcBPslgm8LBvtikrR6MKRFkn6zYL85169BniuGomtW6aHZ5FvXIMse8xDEwvAyZApjhfGFgUQCKxUgRDuTIXMtNja2ob6NtFCDllhMtUCaHBx23hVoRUa29WT-s_Q==&p=3&fvj=0&vjs=3,"Details Trigyn's direct government client has an immediate need for Business / Data Analyst in Long Island City, NY. Description Business / Data Analyst needed for completion of work in more timely manner. Skills Strong analytic and writing skills Ability to think critically, juggle multiple competing priorities, and respond to requests with a fast turnaround time. Assignment tasks Responds to HUD monitor requests Performs analysis of mold and leak data Provides general administrative support Responds to Independent Data Analyst Requests NEW YORK CITY HOUSING AUTHORITY RESIDENTS STRONGLY ENCOURAGED TO APPLY! For Immediate Response call 732-876-7621, or send your resume to Recruiter CS@Trigyn. com Trigyn-8181-Non IT TRIGYN TECHNOLOGIES, INC. is an EQUAL OPPORTUNITY EMPLOYER and has been in business for 30 years. TRIGYN is an ISO 9001 2015, ISO 27001 2013 ISMS and CMMI Level 5 certified company. "
109,Market Data Analyst,data analyst,/rc/clk?jk=6b98b81cd82abe3a&fccid=98ffee493c9fe0eb&vjs=3,"Location This is a remote position. S4 Market Data is a market data management consultancy that helps financial services companies with vendor management and administrative services. We are based in Miami, FL, and service financial institutions across the U. S. A Summary We are looking for an experienced Market Data Analyst with experience in dealing with the business areas, accounting/finance, legal, and other departments of our client's organization in order to ensure the proper management and administration of financial Market Data services. The Market Data Analyst will provide support by effectively performing a wide variety of detail-oriented, multi-step and collaborative analytics, administrative, and organizational duties. The role requires the ability to mentor and manage Market Data Administrators. Much of the work is self-appointed and requires a high degree of professional independence, initiative, and self-discipline. This role will require managing various Market Data services across our client organization and potentially working closely with various front office business units and other Market Data Team members. Responsibilities Handle all day-to-day market data vendor and administrative inquiries for both new or existing client s Market Data vendors and services. Maintain an updated and current inventory, invoicing, reporting, and contracts of all Market Data that the client s is subscribed to. Reconcile invoices and validate monthly allocations/expenses. Work closely with finance/accounting to review reports interpreting spend and usage trends for the client s . Maintain reports on costs and identify ways to consolidate spend. Interact with various client s stakeholders; technology, legal, accounting/finance, human resources, and the various front office business areas and managers. Perform business development and other duties as required by the manager s and founder s . Conducts regular meetings with the Market Data Administrators to ensure all tasks are being completed and deadlines met. Job Requirements Market Data industry experience with a proven track record in MD and Business Management Relevant work experience in consultancy, market research or financial services is preferred Key Competencies Experience working with Market Data vendors such as Bloomberg, Fact Set, Exchanges NYSE, ICE, etc. . FISD Certified is a plus. Product knowledge of the key Market Data vendors, including content and application functionality Awareness of compliance and governance issues pertaining to the licensing of Market Data Excellent communication and project management skills Experience in working closely with senior stakeholders Collaborative team player with the ability to work independently Advanced Microsoft Excel and Access skills along with Power BI are required. Knowledge of FITS, MDSL and INFO Match inventory systems is a plus. Exceptional analytical abilities, including the interpretation of large data sets and deciphering the findings into clear messages and visuals Effective written and verbal communication with internal and external stakeholders The ability to develop an understanding of individual client needs and industry trends An entrepreneurial and creative mind-set. Display a high level of time management skills in order to manage multiple and elaborate requests simultaneously Have high energy and be a self-starter with the ability to work independently and as part of a team. 1. Fill out the form goo. gl/forms/Ru L Fu9tpve2l O Nl93 2. Click Apply Powered by Jazz HR Ihh Ctw4r4H"
110,Senior Business Data Analyst,data analyst,/company/HGcomply/jobs/Senior-Business-Data-Analyst-7329bbd3c9e1759d?fccid=aa61a20407663147&vjs=3,"SENIOR BUSINESS ANALYST – FINANCIAL SERVICES***PLEASE READ THE ENTIRE AD BEFORE APPLYING***are actively seeking a seasoned Senior Business Data Analyst with extensive experience working within the financial services industry. This position is located in lower Manhattan, New York, NY but will initially require the ideal candidate to work remotely due to the active global pandemic ALL APPLICANTS REQUIRE TO BE LOCAL Please review the following requirements carefully · 8+ year extensive seasoned Business Data Analysis experience within the financial services industry!· Extensive experience in working on Data Lineage projects is required· Extensive experience writing BRD’s· Must have strong experience in UAT plans and executing test scripts!· Strong Process Improvement experience· This is a straightforward Business Analyst role – We are not seeking any Technical Business Analysts!· All applicants must be local to NYC and be able to work under W2. NO CORP TO CORP!· Must be a strong independent worker that is able to pick up quickly without much handholding JOB DETAILS & FUNCTIONS · Location New York, NY· Duration 12 Month Project· Hourly W2 Pay Rate $95. 00/hr· Client Global Investment Bank· Description Qualified candidate will have the following roles and responsibilities 1 Develop deal specific models, reports and user tools according to legal documents and clients specifications using in house systems and other supported applications for standard and routine deals. 2 Troubleshoot, resolve issues client issues at all levels for moderately complex financial structures and run and/or maintain deal models on active standard or routine deals. 3 Analyze moderately complex finance structures and legal documentation, including amendments. 4 Liaise directly with internal client services, business partners, clients and other third parties to resolve deal specific issues related to the deal model or the on boarding of the deal. 5 Runs or maintains deal models using proprietary systems on an on-going basis for routine standard deals, including uploading/inputting of data, analytical review for validation and/or reconcilement, investigation and resolution of discrepancies, while meeting required timelines and quality standards. 6 Investigation and resolution of cases raised while meeting required turnaround times and quality standards for client service/admin teams. 7 Review, update, modify, and test any required changes to the complex financial models. 8 Work closely with Client Services to interpret and identify needed changes according to document guidelines, calculations and responsibilities that are clear and within system capabilities. 9 Perform quality control according to the internal procedures. 10 Manage risk and ensure satisfactory audits working with Team Leader to assist with the audit examination and the audit response process. May train and coach other team members on systems, modeling and user tools. 11 Solve technical issues. and 12 High degree of interactions with both internal and external partners, staff members, business partners, clients, other third parties to ensure own tasks and teams deals have no financial impact or loss. Job Types Full-time, Temporary Pay $90. 00 $95. 00 per hour Schedule Monday to Friday Supplemental Pay Signing Bonus Experience UAT 5 years Required extensive Financial Services work 6 years Required test scripts 5 years Required Data Lineage 4 years Required extensive Business Data Analysis 6 years Required Application Question Are you local to NYC?Work Remotely Temporarily due to COVID-19"
111,Quality Data Analyst,data analyst,/rc/clk?jk=f1cdeb7b2df3e451&fccid=bf8d0f5b45d49fcf&vjs=3,"Provides support and analytical insight for Quality Incentive measures, HEDIS measures, and Quality Improvement initiatives. Monitors internal performance against benchmarks through analysis. Participates in the identification, development, management, and monitoring of quality improvement initiatives. Collaborates with Education staff and makes recommendations for areas of focus in training of assessors and care managers, based on analysis of performance trends. Researches and identifies technical/operational problems surrounding systems/applications; communicates/refers complex and unresolved problems to management, Business Intelligence & Analytics BIA , and/or IT. Conducts ad hoc analyses to help identify operational gaps in care; drafts presentations, reports, publications, etc. regarding results of analyses. Communicates results of data analysis to non-technical audiences. Participates in prioritization of departmental goals based on identification of operational gaps in care. Participates in establishing data quality specifications and designs. Coordinates and supports integrated data systems for analyzing and validating information. Identifies and makes recommendations for reporting re-designs and platforms for reporting e. g. automating a manual Excel file using macros, developing a Micro Strategy dashboard to replace manually updated Excel dashboards, moving data storage from Excel to Access, etc. , as needed. Trains staff on use of new/updated systems and related topics. Assists Quality management team with database and department reports. Conducts operations review and analysis of processes and procedures, issues report of findings and implements approved changes as required. Identifies and recommends software needs and applications to accomplish required reporting. Retrieves, compiles, reviews and ensures accuracy of data from databases; researches and corrects discrepancies, as needed. Analyzes data from internal and external sources. Identifies and resolves data quality issues before reports are generated. Works with staff to correct data entry errors. Analyzes data, identifies trends, reoccurring problems, statistically significant findings and prepares reports/summaries for management review. Acts as a liaison between Quality Management, CHOICE Clinical Operations, and BIA. Reviews and identifies trends and variances in data and reports. Researches findings and determines appropriateness of elevating identified issues to leadership for further review/evaluation/action. Monitors and maintains files by ensuring that files are current and of relevant nature. Analyzes and corrects error reports to ensure timely and accurate data; develops corrective actions to prevent errors where possible. Participates in special projects and performs other duties, as needed. "
112,Research Data Analyst - Chinese & English speaking,data analyst,/rc/clk?jk=8147253b1d170d74&fccid=d7365da89e9fe3bc&vjs=3,"Fast-growing Life Science data company is looking for Chinese-speaking research data analysts. Ideal candidates will speak Chinese fluently, demonstrate expertise working with tools such as Excel or Google Sheets and have an analytical mindset. Our SAAS platform comprises of healthcare providers, organizations and related claims, clinical trials, publications, and other assets. As a Research Data Analyst, you will be responsible for researching and collecting data on leading healthcare professionals to add to our proprietary database. Candidate must be curious, motivated, show strong attention to detail and have an interest in developing their knowledge of the life sciences sector and the work we do to ultimately improve patients’ lives. This is a contract position working remote in the US 30 hours a week for a four month commitment, paying $25 per hour. After the 4 month period, if this person demonstrates the right skillset and passion for our business they will be considered for a full-time position with H1 Insights. Responsibilities Gather data on healthcare professionals in both Chinese and English language Translate English terms to Chinese where they are not otherwise available Learning and staying current with all guidelines and procedures for researching and collecting data Perform peer quality review on other team members collection work Collaboration with team members across the US and internationally Qualifications Fluent Chinese reading skills required Bachelor's Degree reqred Prior work experience required for recently graduates a track record of internship work required About H1Insights H1 is the first company to arm healthcare and life science companies with on-demand, live insights from across the data universe to accelerate the discovery and development of therapies to fight diseases. The company provides real-time data to support the end-to-end therapeutic development process from fundraising to product development to product launch, helping companies make smarter scientific decisions. Working with medical affairs and strategy teams who span all phases of the development lifecycle, H1 provides the complete picture of institutions, experts, scholarly content, markets, competitors and new opportunities through research grounded in actual data and clinical findings. "
113,Research Data Analyst - Japanese & English language skills,data analyst,/rc/clk?jk=967a9123fa27f43d&fccid=d7365da89e9fe3bc&vjs=3,"Fast-growing Life Science data company is looking for Japanese-speaking research data analysts. Ideal candidates will be able to read Japanese fluently, demonstrate expertise working with tools such as Excel or Google Sheets and have an analytical mindset. Our SAAS platform comprises of healthcare providers, organizations and related claims, clinical trials, publications, and other assets. As a Research Data Analyst, you will be responsible for researching and collecting data on leading healthcare professionals to add to our proprietary database. Candidate must be curious, motivated, show strong attention to detail and have an interest in developing their knowledge of the life sciences sector and the work we do to ultimately improve patients’ lives. This is a contract position working remote in the US 30 hours a week for a four month commitment, paying $25 per hour. After the 4 month period, if this person demonstrates the right skillset and passion for our business they will be considered for a full-time position with H1 Insights. Responsibilities Gather data on healthcare professionals in both Japanese and English language Translate English terms to Japanese where they are not otherwise available Learning and staying current with all guidelines and procedures for researching and collecting data Perform peer quality review on other team members collection work Collaboration with team members across the US and internationally Qualifications Fluent Japanese reading skills required Bachelor's Degree required Prior work experience required for recently graduates a track record of internship work required About H1Insights H1 is the first company to arm healthcare and life science companies with on-demand, live insights from across the data universe to accelerate the discovery and development of therapies to fight diseases. The company provides real-time data to support the end-to-end therapeutic development process from fundraising to product development to product launch, helping companies make smarter scientific decisions. Working with medical affairs and strategy teams who span all phases of the development lifecycle, H1 provides the complete picture of institutions, experts, scholarly content, markets, competitors and new opportunities through research grounded in actual data and clinical findings. "
114,Healthcare Quality Assurance Data Analyst,data analyst,"/company/Bridging-Access-to-Care,-Inc/jobs/Healthcare-Quality-Assurance-Data-Analyst-063cfe59c666426b?fccid=7a7e64dbedc24129&vjs=3","Brooklyn based nonprofit seeks Healthcare QA/Data Analyst with a bachelor’s degree in health related field from an accredited college or university with 5 years’ experience. Must be able to be proficient in reporting, data analysis with direct experience in health analytics, modify existing reports, test report for accuracy and enhance reports as necessary, participate in performance improvement initiative. This candidate will assist with updating EHR system to reflect changes in healthcare landscape for example fee schedule diagnosis, insurances, procedures, etc. Must be computer literate Word, Excel, etc. and have good writing and communications skills. Job Type Full-time Pay $65,000. 00 per hour Benefits Dental Insurance Health Insurance Life Insurance Paid Time Off Retirement Plan Vision Insurance Schedule Monday to Friday Experience microsoft office 1 year Preferred Data Analysis Skills 1 year Preferred Quality Assurance/quality improvement 1 year Preferred SQL 1 year Preferred EHR Systems 1 year Preferred Education Bachelor's Required "
115,Senior Data Analyst,data analyst,/rc/clk?jk=7370d4fa214b00bd&fccid=3001b6ad1dad0a92&vjs=3,"At VMLY&R, we create connected brands. We resist the usual ways of seeing, doing and thinking — harnessing creativity, technology and culture — to reimagine the entire connected consumer experience. Our goal? To create work that becomes part of people’s lives, to drive value for our clients and, in the best of cases, to impact the world. The Senior Analyst supports internal VMLY&R audiences and client stakeholders by pulling and analyzing data and consumer behavior to provide actionable insights and recommendations. S he helps analyze available data to determine the appropriate next steps for providing the most relevant information to stakeholders for business and marketing decisions. As part of the WPP Group, VMLY&R offers one of the best overall compensation packages in the business. Required Skills Synthesize and interpret marketing data into a narrative of business performance and actionable insights in deliverables of various formats Create measurement frameworks that identify KP Is and diagnostic measures for marketing programs Enable behavior tracking on various digital platforms Extract data from various digital tools Manage data accuracy and integrity for client marketing programs Plan, implement and manage marketing optimization programs e. g. A/B testing Create and design business and marketing dashboards that align to client program objectives and various audiences Implement, collect, and analyze natural language data across social media platforms Additional responsibilities as assigned Required Experience 6-8 years of related experience working in marketing analytics Demonstrated ability to connect paid media tools, CRM, and Social Monitoring tools with site analytics tools to gain integrated online marketing insights Deep experience in data analysis and report generation through tools such as Excel, Tableau, Power BI, SPSS etc. Expert knowledge in digital metrics, dimensions and KP Is Experience working independently, self-managing, and managing others on projects to deliver results supporting multiple clients and stakeholders Hands-on experience with web analytics platform s Adobe Analytics, Google Analytics, etc. Proficiency in Microsoft Excel Experience with multivariate and experimental test platform s Adobe Target, Optimizely, Visual Website Optimizer, etc. Experience with predictive analytics and CRM applications a plus VMLY&R is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, gender identity, sexual orientation, national origin, disability status, protected veteran status, or any other characteristic protected by law. "
116,"Data Analyst, Center for Health Equity and Community Wellness/Bureau of Equitable Health Systems",data analyst,/rc/clk?jk=6ca1a16b772bf940&fccid=4146d8487cdbf799&vjs=3,"THIS IS A PART-TIME PUBLIC HEALTH EPIDEMIOLOGIST II 35 HOURS/WEEK Be a change agent and join the Bureau of Equitable Health Systems BEHS , a bureau in the NYC Department of Health and Mental Hygiene. BEHS partners with health and social care institutions in New York City to ensure that every New Yorker receives, in an equitable fashion, the care and resources they need to be healthy and maintain wellness across their lifespan. BEHS utilizes a wide variety of data sources, including but not limited to Medicaid claims data and clearinghouse data accounting for the majority of outpatient claims in New York City. BEHS maintains a large data warehouse that receives multiple data streams that include summarized measures on the quality of ambulatory health care, processes in care delivery, and population health indicators. Health professionals assess these data to respond to public health needs, and to conduct program evaluation and research. The Data Analyst is being recruited to produce and analyze public health data from a variety of sources, including claims data, in the context of population health and health system utilization, to explain and track trends. The Data Analyst will work with a dynamic, cross-disciplinary team that leverages multiple data sources to inform health systems planning and policy. The Data Analyst will be in the Health Systems Planning and Policy unit. This person will be responsible for conducting data analysis on health services utilization and creating dashboards to meet programmatic goals. The Data Analyst will also be responsible for preparing data summaries to inform leadership at the bureau and divisional level. Job Responsibilities Conduct studies and time-sensitive investigations that address emergent public health issues involving chronic disease prevention and health equity. Lead a project to monitor the volume of hospital use for each block in NYC and identify clusters of chronic disease e. g. asthma, CVD, diabetes, heart disease . Respond to requests for information from health professionals and the general public to support program interventions e. g. integrated pest management, diabetes prevention that reduce chronic health disparities across NYC neighborhoods. Monitor clinical surveillance measures and identify population health trends using SPARCS hospital and New York State Medicaid health service utilization data. Implement epidemiological analyses to determine sociodemographic risk factors and inform DOHMH leadership, payers, and providers of opportunities to achieve significant reductions in medical costs and preventable hospital use in places and population cohorts. Manage claims-related projects and initiate queries based on technical specifications gathered from BEHS staff and other stakeholders in the agency to meet bureau, divisional and agency program monitoring and evaluation goals. Develop data visualization and dashboards primarily in Tableau and Arc GIS and provide technical assistance to internal staff and external stakeholders. Database management, integrating and analyzing different data sources, and supervising diagnostics and quality assurance on multiple data sources using SAS/SQL/R. Present findings within the bureau and across the agency as necessary; recommends appropriate actions or responses. Minimum Qual Requirements 1. A master’s degree in epidemiology or in public health with a minimum of 12 graduate credits in epidemiology from an accredited college or university; or 2. A baccalaureate degree from an accredited college or university, supplemented by the successful completion of PRINCIPLES OF EPIDEMIOLOGY #3030-G or #SS1000 , a course for health professionals, given by the Centers for Disease Control, U. S. Public Service and one year of satisfactory full-time experience as a health professional in a position which requires data collection and the reading and interpretation of medical charts and medical information in support of surveillance and epidemiologic investigations; or 3. A baccalaureate degree from an accredited college or university and two years of satisfactory full-time experience as described in ""2"" above. Preferred Skills Master’s degree in statistics, public health or related field with at least two 2 years of full-time working experience. Strong analytic and statistical skills with experience in manipulating and analyzing data using SAS, R, Tableau, and SQL preferred. Experience using the Salient tool to extract New York State Medicaid medical and prescription claims data. Experience using SPARCS to extract New York State hospital utilization claims data. Familiarity with medical billing procedures. Familiarity with geographical information software such as Arc Map preferred. Competency in Microsoft Office Word, Excel, and Power Point . Expertise in epidemiological data analysis, research, and program evaluation methodologies. Excellent written and oral communication skills. Ability to work collaboratively in a cross-disciplinary team environment. Highly organized and detail oriented. Willingness to adapt to new data structures and tackle novel problems creatively. Working knowledge of or interest in a range of public health topics. Comfortable with shifting deadlines and priorities. Additional Information **IMPORTANT NOTES TO ALL CANDIDATES Please note If you are called for an interview you will be required to bring to your interview copies of original documentation, such as A document that establishes identity for employment eligibility, such as A Valid U. S. Passport, Permanent Resident Card/Green Card, or Driver’s license. Proof of Education according to the education requirements of the civil service title. Current Resume Proof of Address/NYC Residency dated within the last 60 days, such as Recent Utility Bill i. e. Telephone, Cable, Mobile Phone Additional documentation may be required to evaluate your qualification as outlined in this posting’s “Minimum Qualification Requirements” section. Examples of additional documentation may be, but not limited to college transcript, experience verification or professional trade licenses. If after your interview you are the selected candidate you will be contacted to schedule an on-boarding appointment. By the time of this appointment you will be asked to produce the originals of the above documents along with your original Social Security card. **LOAN FORGIVENESS The federal government provides student loan forgiveness through its Public Service Loan Forgiveness Program PSLF to all qualifying public service employees. Working with the DOHMH qualifies you as a public service employee and you may be able to take advantage of this program while working full-time and meeting the program’s other requirements. Please visit the Public Service Loan Forgiveness Program site to view the eligibility requirements https //studentaid. ed. gov/sa/repay-loans/forgiveness-cancellation/public-service ""FINAL APPOINTMENTS ARE SUBJECT TO OFFICE OF MANAGEMENT & BUDGET APPROVAL” To Apply Apply online with a cover letter to https //a127-jobs. nyc. gov/. In the Job ID search bar, enter job ID number # 442244. We appreciate the interest and thank all applicants who apply, but only those candidates under consideration will be contacted. The NYC Health Department is committed to recruiting and retaining a diverse and culturally responsive workforce. We strongly encourage people of color, people with disabilities, veterans, women, and lesbian, gay, bisexual, and transgender and gender non-conforming persons to apply. All applicants will be considered without regard to actual or perceived race, color, national origin, religion, sexual orientation, marital or parental status, disability, sex, gender identity or expression, age, prior record of arrest; or any other basis prohibited by law. NOTE This position is open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate in your resume that you would like to be considered for the position under the 55-a Program. Residency Requirement New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview. "
117,Data Analyst,data analyst,/company/Montefiore-Medical-Center/jobs/Data-Analyst-7c4232ad87c7cd74?fccid=d1e1c9b793a7214a&vjs=3,"Montefiore Health System MHS is a premier academic medical center and the University Hospital for Albert Einstein College of Medicine. Combining a population health perspective that focuses on the health needs of communities with nationally recognized clinical excellence, MHS delivers coordinated, compassionate, science-driven care where, when and how patients need it most. MHS consists of six hospitals and an extended care facility with a total of 2,059 beds, and state-of-the-art primary and specialty care provided through a network of more than 150 locations across the region, including the largest school health program in the nation and a home health program. The Division of Critical Care Medicine is renowned for both its clinical excellence and expertise in providing intensive care to acutely ill medical and surgical patients in the hospital and its research contributions to improving the care and outcomes of patients with acute critical care illnesses. We have an exciting opportunity to join our Critical Care team as a Data Manager and Analyst. The principal function of this position is to provide database and analytical support to the Chief of Critical Care and Director of Critical Care Research to support the clinical operations and research projects in the division. Job Responsibilities Job responsibilities include the abstraction and merger of data from electronic health records and hospital administrative databases, generation of reports and statistical analysis to track and measure various clinical functions, development of clinical and research databases, statistical assistance to research investigators for investigations, maintenance of databases, and production of standing and ad hoc reports as required. Bachelors or Masters degree in the relevant field of biostatistics or epidemiology is desired. Knowledge of SQL and statistical programs such as STATA, SAS, R is helpful. Job Type Full-time Pay $60,000. 00 per year Benefits 401 k Matching Dental Insurance Disability Insurance Flexible Spending Account Health Insurance Life Insurance Paid Time Off Tuition Reimbursement Vision Insurance Schedule 8 Hour Shift Day shift Monday to Friday Work Location One location Benefit Conditions Only full-time employees eligible Work Remotely No"
118,Data Analyst,data analyst,/rc/clk?jk=4f848b9c2a2ea0aa&fccid=6e7de84a2f525ed9&vjs=3,"Bodhala’s groundbreaking legal technology solutions empower teams to analyze, interpret and optimize outside counsel spend — trailblazing a new era of legal market intelligence through AI and machine learning. With legal departments being asked to do more with less, Bodhala takes the ambiguity out of legal spend and empowers business leaders to make strategic decisions and justify those decisions with confidence. Headquartered in the Financial District of Manhattan, Bodhala tripled its headcount last year and is positioned for continued fast-paced growth through 2020, with locations across the United States and in Europe. This position will be remote until at least September 2020, but candidates should expect to eventually be onsite in some capacity at our NYC HQ. Reporting to the Senior Product Data Analyst, the Data Analyst is responsible for assisting and leading several data QA projects on our existing data stores, using data to develop new platform features, and enhancing and uploading incoming datasets. This is an exciting opportunity to be part of building something from the ground up and put your own mark on the company. Responsibilities Helping curate the industry’s leading source-of-truth data set Implementing quality assurance processes with the data engineering team Leading efforts to extend and expand data set Becoming a domain expert in legal analytics Participating in machine learning projects — labeling data and engineering features Communicating insights and helping to structure data roadmap Qualifications Bachelor’s degree in a quantitative field is required, data science and analytics related degrees are preferred Advanced degree in data science, law, or related fields a plus0-2 years of experience in a data analytics role Advanced knowledge of Excel array formulas, index/match, text manipulation, etc. Intermediate knowledge of SQL array formulas, subqueries , experience with Postgre SQL a plus Knowledge of Python Pandas, SQL Alchemy, Beautiful Soup, etc. a plus Ability to operate in highly ambiguous situations Highly organized, detail-oriented, highly accurate and able to manage multiple priorities Bodhala is an equal opportunity employer. We value diversity. We don’t discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, marital status, veteran status, disability status, or socioeconomic status. "
119,Data Analyst,data analyst,/rc/clk?jk=782834daf85c18bc&fccid=e17fb816c4dc2df9&vjs=3,"Position Data Analyst Department People Role Overview We’re looking for someone to support the data team with gathering, refining and presenting actionable data. The Data Analyst will report to the Senior Manager of People Data & Systems, and will play an important role in developing data strategy across all facets of the organization. An ideal candidate will have a keen eye for detail and a passion for problem solving. Please note this opportunity is contract to hire What you’ll get to do here Clean, Analyze and Report Data to help drive decisions across the organization Figure out what questions are being asked and figure out if those questions can be answered by data Determine technical issues with collecting and analyzing data, and design reports Identify new sources of data and methods to improve data collection, analysis, and reporting Collect, analyze, and report data to meet business needs. Distinguish trends and patterns Report data in a comprehensive and repeatable way Be a thought partner in the data strategy for Lincoln Center You’ll be a fit if you 2-4 years of relevant work experience Possess Strong Excel Skills Have experience collecting, analyzing, and reporting data to meet business needs. Strong attention to detail Have a background in computer science, mathematics, statistics, or data science You’ll be a great fit if you Possess Strong Microsoft Access Skills and have familiarity developing or working with databases Have experience developing reports with Power BI Familiarity with HRIS systems ADP Workforce Now for additional points Have working experience as an analyst in in either a HR or finance department What is Lincoln Center for the Performing Arts? Lincoln Center for the Performing Arts LCPA is the world’s leading performing arts center in the heart of New York City. In these times of heightened anxiety and vigilance, we are holding on to the important role the arts play in our lives they nourish our hearts and minds, teach us valuable lessons and critical skills, and help us create community. We are resolved not to lose sight of what connects us, and we hope you will consider joining our talented, diverse team. We are 1. The manager of the Lincoln Center Campus. We are part of 16 acres of activity and one of eleven amazing resident organizations 2. A leading Arts Presenter. We curate a number of series showcasing music, dance, and theater 3. An Education Hub. We have reached 20 million students, educators, principals, and community members Who are our people? Lincoln Center is a diverse team of dreamers, collaborators, and entrepreneurs who use unique platforms in the heart of New York City and beyond to advocate for the transformative impact of artistic experiences. Lincoln Center People imagine and create in concert with this mission by founding President John D. Rockefeller III ""The arts are not for the privileged few, but for the many. Their place is not on the periphery of daily life, but at its center. "" Lincoln Center welcomes applicants from all sectors who agilely solve problems, show up as they are, and can't stop innovating. What’s the news? https //www. broadwayworld. com/article/Lincoln-Center-Announces-Lincoln-Center-At-Home-Featuring-Performances-Classes-and-More-20200324 Who is our President and CEO? Henry Timms is the co-founder of #Giving Tuesday and the co-author of New Power. Join us! It is the policy of Lincoln Center to ensure equal employment opportunity without discrimination or harassment on the basis of race, creed, color, national origin, sex, age, religion, disability, marital or civil partnership/union status, familial or caregiver status, alienage or citizenship status, sexual orientation, gender identity or expression, pregnancy, military or veteran status, genetic information, predisposition, or carrier status, unemployment status, domestic violence, sexual violence, or stalking victim status, or any other characteristic protected by federal, state, or local law. LCPA is committed to creating a dynamic work environment"
120,Jr. Data Analyst - remote during pandemic,data analyst,/company/Utility-Data-Analytics/jobs/Junior-Data-Analyst-15ad9e4c1f4f2573?fccid=6c14cc8992df0c1f&vjs=3,"Junior Business and Data Analyst Be part of the team which has delivered over $450 Million dollars in refunds and savings to clients, and join us as we march toward One Billion dollars!Utili Save is seeking qualified candidates for Junior Business and Data Analyst position. This is a business critical hands-on position, and the ideal candidate should have demonstrated ability to work with large amounts of data, analyze complex laws and regulations. Identify billing recapture opportunities and solutions as well as conduct extensive on-site inspections to supplement the analytics to be undertaken. This position interfaces directly with customers and utility companies, so strong communication, presentation, and client management skills are also essential. Familiarity with utilities, especially utilities’ rates, tariffs, and billing practices would be particularly valuable. Responsibilities· Using Utili Save’s proprietary software applications, conduct financial, utility, and business analysis of clients’ operations and initiate actions to obtain cost savings for clients. · Interface with clients, utility companies, tax authorities, regulatory authorities, and government agencies to obtain appropriate information and successfully implement savings opportunities. · Seek smart and creative manner in which to apply complex utility tariff codes against our clients’ usage and practices to identify new savings. · Match details in complex utility tariff codes against our clients’ usage and practices to identify savings/refund opportunities. · Collaborate with our IT group in implementing tools to improve our clients’ accounts review process. Requirements· 1-3 years work experience in one or more of the following areas rates modeling, energy policies, financial, cost analysis, auditing. · College Degree in Mathematics, Statistics, Accounting, Business, or Energy related majors preferred. · Strong analytical, cognitive, and computation skills. · Detail-oriented. · Able to independently solve problems and think outside the box. · Valid Driver’s License. Additional Information· Candidates will receive the necessary training to understand our methodology and analytical techniques. · Our company is growing, and this position has significant growth potential for an aggressive self-starter. · Competitive salary commensurate with experience; excellent health, dental and vision benefits, 401K with company match; 3 weeks paid time off to start, plus 9 holidays. · We are located in NYC on West 27th Street, between 6th and 7th Avenues. About Utili Save, LLC Utili Save has been an industry leader in utility bill auditing and energy efficiency services for over 26 years. Our clients are large organizations with significant energy and utility usage, such as hospitals, nursing homes, commercial and residential buildings, universities, hotels and government facilities. By utilizing our unique and unmatched knowledge of utility tariff structures, we have successfully recouped over $450 million dollars in savings to our clients. We are growing rapidly as we expand our business, and we seek talented professionals to join our team. We are looking for candidates who will play a key role in our forensic utility bill analysis, and who will help our clients to manage their utility expenditure more efficiently. Job Type Full-time Benefits 401 k 401 k Matching Dental Insurance Employee Discount Flexible Spending Account Health Insurance Paid Time Off Vision Insurance Schedule Monday to Friday Supplemental Pay Commission Pay Experience Data Analysis 1 year Preferred Company's website https //www. utilisave. com/Benefit Conditions Waiting period may apply Work Remotely Temporarily due to COVID-19"
121,Data Analyst,data analyst,/rc/clk?jk=0ba4a3b735614972&fccid=fb55aa512078e608&vjs=3,"Company Overview Health disparities among New Yorkers are large, persistent and increasing. Public Health Solutions PHS exists to change that trajectory, and support vulnerable New York City families in achieving optimal health and building pathways to reach their potential. As the largest public health nonprofit serving New York City, we improve health outcomes and help communities thrive by providing services directly to vulnerable low-income families, supporting community-based organizations through our long-standing public-private partnerships, and bridging the gap between healthcare and community services. We focus on a wide range of public health issues including food and nutrition, health insurance, maternal and child health, sexual and reproductive health, tobacco control, and HIV/AIDS. To learn more about our work, please visit healthsolutions. org. Position Summary Public Health Solutions PHS is seeking a temporary full-time Data Analyst, who will report to the Director of Quality and Evaluation for the Division of Neighborhood Health. This position is funded through December 2020, with possible option to extend, contingent upon securing additional funding. Specifically, the Data Analyst will Gather data from Neighborhood Health programs and/or community partners, populate tables, create data visualizations, and present those data Maintain Division-wide monitoring report in Power BI Use and visualize data to proactively identify program and/or data quality issues, develop programmatic solutions, and implement those solutions Contextualize analysis with community-level data and/or literature Perform data analysis in Microsoft Excel for various stakeholders Develop, administer, enter data from, and analyze data from surveys Write data summaries, including interpretation and/or recommendations Support development of external-facing products e. g. health briefs, reports, journal articles Fulfill data requests from other parts of the organization Other duties as assigned by Neighborhood Health Leadership team Qualifications Bachelor’s Degree from four year college or equivalent Minimum one year experience Experience using data for program management and/or evaluation Fluency with Microsoft Excel specifically, VLOOKUP and Pivot Tables Experience visualizing data so that it is easily understood by stakeholders Experience telling stories with data, in written and/or oral communication Experience effectively collaborate with people of different backgrounds Experience using data to identify issues and come up with recommendations Experience building reports and dashboards in Power BI"
122,Data Analyst,data analyst,/rc/clk?jk=78830d177bd30c0d&fccid=9f8fb49912899420&vjs=3,"About us Cityblock Health is the first tech-driven provider for communities with complex needs, bringing better care to where it's needed most, block by block. Founded in 2017 on the premise that ""health is local"" and based in Brooklyn, we are backed by Alphabet's Sidewalk Labs and some of the top healthcare investors in the country. Our mission is to improve the health of underserved communities, one block at a time. Importantly, our solutions are designed specifically for Medicaid and lower-income Medicare beneficiaries, and we meet our members where they are—bringing care into the home and neighborhoods through our community-based care teams. In close collaboration with community-based organizations, local providers, and leading health plans, we are reorganizing the health system to focus on what matters to our members. Equipped with world-class, custom care delivery technology, we deliver personalized primary care, behavioral health, and social services to deliver a radically better experience of care for every member and community we serve. Over the next year, we'll grow quickly to bring better care to many more members and their communities. We will get started in new markets, each with their own operating structure and care teams, and continue to grow in the communities where we are working already. To do this, we need people who, like us, believe that everyone should have good care for what matters to them, in their community. Our work is grounded in a belief in the power of a diverse community. To deliver a radically better experience of care to our members and advance equity in the communities we serve, we strive to make our own team diverse and inclusive. Our ways of working are characterized by creativity, collaboration, and mutual learning that comes from bridging together a community from diverse backgrounds and perspectives. We strive to ensure that every person on the Cityblock team, and every Cityblock member, feels like they belong, are valued, respected, and celebrated as a part of our community. Our Values Aim for Understanding Be All In Bring Your Whole Self Lean Into Discomfort Put Members First About the Role We are seeking a data analyst to design and build analytic solutions that will inform business decisions, drive operations, measure our performance and demonstrate our value as a company. Healthcare data is complicated, so you must be excited about digging deep into complex data, and coming up with interesting, actionable insights to communicate to the team. Behind every data element is a real person, with real problems that Cityblock is working hard to address, and the Data team is responsible for delivering accurate and timely information to steer these efforts. If you like hard problems, have experience pulling insight out of complex structured and unstructured data sources, and are an amazing teammate, we want to hear from you! Requirements for the Role You have 2+ years of experience in a highly operational data analysis role. You are a strong SQL programmer, with experience navigating and cleaning messy data at scale. Experience with DBT is a bonus. Experience with business intelligence back-end infrastructure Looker/Look ML/Tableau Experience building scalable metrics You can storytell using data How We Define Success We are in tight sync with Market Operations teams on required analytic needs and are on track to deliver on time. We have complete and accurate weekly, monthly and quarterly reports for our healthcare plan partners On-time delivery of ad hoc requests We have built a self-service reporting infrastructure, allowing Market teams to create and maintain their own high quality, scalable reports using front-end tools that we manage Nice to Have, But Not Required Experience in a high growth technology company. Comfort across a range of data analysis tools such as Python or R You have exposure to healthcare data in at least one domain claims processing, EHR data pulls, HIE feeds, prescriptions, etc and standard clinical metrics and taxonomies HEDIS, STARS, HCC, etc Experience with Github Experience with dynamic data pipelines such as via Py Spark or Apache Spark What We'd Like From You A resume and/or Linked In profile A short cover letter, please! Cityblock values diversity as a core tenet of the work we do and the populations we serve. We are an equal opportunity employer, indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic. We do not accept unsolicited resumes from outside recruiters/placement agencies. Cityblock will not pay fees associated with resumes presented through unsolicited means. "
123,Data Analyst,data analyst,/rc/clk?jk=4bbc85aea6303455&fccid=b941067378d4d8e8&vjs=3,"Fern Health is pioneering digital care for people with musculoskeletal MSK pain helping them avoid surgeries, return to work, and get back to doing the things they love. Want to collaborate with engineers, product managers, data scientists, and clinicians developing innovative digital healthcare solutions using cutting-edge technology? Love to geek out on data visualization & storytelling? If you answered “Yes” to all of the above, you should join our fast-growing team at Fern! We are looking for a mission-driven Data Analyst to join our growing Data Science team and help us translate healthcare data into insights. This role will provide data-driven insight across multiple disciplines to help answer key questions, define and track critical business metrics, and communicate findings to enable data-driven decision making. As a Data Analyst, you will . . . Work alongside some of the most talented technologists out there as part of a tight, high-performing team of product designers, engineers, data scientists, and clinicians. Dive deep into our growing database to produce analyses for the Product, Clinical, and Marketing teams and report key findings. Troubleshoot issues and provide support for analytics data sources and tools. Create dynamic and interactive reports or dashboards for internal and external use. Maintain comprehensive and up-to-date documentation, and ensure proper data governance. Collaborate with stakeholders to understand analytics needs, set priorities, and work on multiple projects simultaneously. We’d love to talk more if you have… A BS/MS in Computer Science, Engineering, Statistics or related field Minimum two years of experience as a Data Analyst or similar role Experience using SQL to query multidimensional and complex datasets from data warehouses e. g. AWS Redshift, Snowflake, Postgre SQL, Google Big Query A deep understanding of data visualization techniques and appropriate use cases Proficiency with statistical testing, modeling, and analysis using Python/R Experience providing self-service analytics using BI/data visualization tools e. g. Tableau, Looker, Power BI A methodical approach to solving problems with excellent attention to detail Bonus points for. . . Work or internship experience at a technology company Experience using Business Intelligence tools eg. Looker Experience with predictive analytics and machine learning Familiarity with behavioral analytics tools e. g. Mixpanel, Amplitude, Google Analytics Familiarity with data ingest and ETL pipelines AWS Lambda, Snowpipe, Snowplow "
124,Associate Data Analyst,data analyst,/rc/clk?jk=c8733773c01604c4&fccid=2f7cb114b730b276&vjs=3,"Medscape, a division of Web MD, develops and hosts physician portals and related mobile applications that make it easier for physicians and healthcare professionals to access clinical reference sources, stay abreast of the latest clinical information, learn about new treatment options, earn continuing medical education credits and communicate with peers. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Position Overview Web MD is currently seeking a driven Associate Data Analyst and Report developer, for our Business Intelligence division. This person will be working with Vertica and business analysis tools including Omniture Site Catalyst and Tableau to support their data analysis and exploration. This person will work as part of a team of seasoned analysts and developers with opportunities for collaboration and mentoring in using SQL in a big data environment and employing the latest in analytic database capabilities. This position will be responsible to support various business functions specifically to understand their initiatives, identify reporting needs, vet out requirements, design and develop reports Responsibilities Support various business units across the organization, Promotional Marketing, Membership, and Editorial Teams by developing and delivering ad-hoc and standard reporting using both SQL and Enterprise reporting tools Perform and execute advanced analytic functions in Vertica and Tableau to deliver on complex marketing sciences use cases Provide historical performance, trending and benchmarking analysis Investigate changes in traffic, engagements, emails, ads, retention, and click through and assist in identifying root causes of monetizable engagements Create and maintain reporting documentation and user guides Other duties and responsibilities as assigned include Insightful, easy to understand reports that meet business needs Delivery of trend analysis for website, registration and marketing activities Provide daily/weekly/monthly reports and insights into changing patterns with an understanding of business initiatives that may be driving the changes Deliver timely accurate reports Provide recommendation on report automation and business process improvement Qualifications Bachelor’s degree in Data Science, Database Design, Computer Science, Statistics or a related discipline or at least 4 years of additional experience in report writing and reporting applications. 2+ years’ experience in report writing and reporting applications 2+ years’ experience with databases such as Vertica, SQL Server, Oracle, and strong SQL skills. Must be able to write SQL queries from scratch 2+ years of practical experience with Excel. Must be proficient with advanced analytic functions such as vlookup, named ranges, pivot tables, formulas, etc. Ability to work through problems and reach a workable solution Self-motivated, proactive and self-sufficient with the ability to motivate peers and drive intelligent, creative, and action-oriented results Ability to understand data relationships, provide insights and deliver results Experience with web analytics tools such as Omniture preferred and Google Analytics Experience with reporting and analyzing web metrics Experience with Tableau Experience with Vertica, SQL, Tableau and/or Adobe Analytics is preferred"
125,"Data Analyst, Center on Immigration and Justice (CIJ)",data analyst,/rc/clk?jk=d0a05cb37f434397&fccid=c84fc6b7ec7ae769&vjs=3,"Are you eager to roll up your sleeves and harness data to drive policy change? Do you enjoy sifting through complex datasets to illuminate trends and insights? Do you see yourself working for a values-driven organization with a vision to tackle the most pressing injustices of our day? We are looking to hire a bright, hard-working, and creative individual with strong data management skills and a demonstrated commitment to immigrant's rights. The Data Analyst will assist with analysis and reporting needs for Vera’s Center on Immigration and Justice CIJ , working across its current projects and future Vera initiatives. Who we are Founded in 1961, The Vera Institute is an independent, non-partisan, nonprofit organization that combines expertise in research, technical assistance, and demonstration projects to assist leaders in government and civil society examine justice policy and practice, and improve the systems people rely on for justice and safety. We study problems that impede human dignity and justice. We pilot solutions that are at once transformative and achievable. We engage diverse communities in informed debate. And we harness the power of evidence to drive effective policy and practice What we’re doing We are helping to build a movement–among government leaders, advocates, and the immigration legal services community–towards universal legal representation for immigrants facing deportation. In the face of stepped-up immigration enforcement, millions of non-citizens are at risk of extended detention and permanent separation from their families and communities. Vera’s Center on Immigration and Justice CIJ partners with government, non-profit partners, and communities to improve government systems that affect immigrants and their families. CIJ administers several nationwide legal services programs for immigrants facing deportation, develops and implements pilot programs, provides technical assistance, and conducts independent research and evaluation. That’s where you come in The Data Analyst will support the Center’s programmatic efforts through regular monitoring and reporting of federal government and subcontractor data. CIJ manages several proprietary databases that run on AWS and Caspio and uses SQL, R, and Python to manage data. This is an opportunity to help shape an innovative national research and policy agenda as part of a dedicated team of experts working to improve access to justice for non-citizens. Vera seeks to hire a Data Analyst to work on various data management projects with its Center on Immigration and Justice CIJ . In collaboration with other Data Analysts, this position will involve work across several projects, such as the Unaccompanied Children’s Program UCP , a program to increase legal representation for immigrant children facing deportation without a parent or legal guardian. The position may cover additional duties for the Legal Orientation Program for Custodians LOPC , which educates the custodians of unaccompanied children about their rights and the immigration court process. About the role As a Data Analyst, you will report to a member of the research team and work in close collaboration with other Vera staff on ongoing database management, monitoring, reporting, and analysis projects. You’ll support the team by taking ownership of ongoing monitoring and reporting tasks involving large data sets. Other principal responsibilities will include Supporting research staff by preparing large datasets for analysis, including merging, cleaning, and recoding data; Providing insights into program performance through summary statistics and performance indicators; Producing timely reports on Vera projects for team members and stakeholders; Improving recurring reporting processes by optimizing code and producing subsequent documentation; Coordinating database management tasks such as participating in new database design, modifying existing databases, and communicating with outside engineers and subcontractors; Developing codebooks and delivering user trainings through webinars and database guides; Building and maintaining interactive dashboards; Documenting and correcting data quality issues; Working with supervisors to prioritize program needs; Assisting on other projects and tasks as assigned. About you You’re committed to improving issues affecting immigrants in the United States. Applicants with personal experiences with the immigration system are especially encouraged to apply. You’re just getting started in your career and have 1 – 2 years of professional or internship experience working with large datasets and preparing data for analysis. You have a real enthusiasm for working with data. You are comfortable writing queries in SQL, R, and/or Python, or have a solid foundation coding in other programming languages used to manipulate data. Experience working collaboratively using tools like Git/Git Hub is a plus. You have exceptional attention to detail, strong problem-solving ability and logical reasoning skills, and the ability to detect anomalies in data. You’re able to work on multiple projects effectively and efficiently, both independently and collaboratively with a team. This position involves working with secure data that may require government security clearance. That clearance is restricted to U. S. citizens and citizens of countries that are party to collective defense agreements with the U. S. The list of those countries is detailed on this webpage. An additional requirement of that clearance is residence in the United States for at least three of the last five years. How to apply Please submit cover letter and resume. Applications will be considered on a rolling basis until position is filled. Online submission in PDF format is preferred. Applications with no cover letter attached will not be considered. The cover letter should address your interest in CIJ and this position. However, if necessary, materials may be mailed or faxed to ATTN Human Resources / CIJ Data Analyst Recruitment Vera Institute of Justice 34 35th St, Suite 4-2A Brooklyn, NY 11232 Fax 212 941-9407 Please use only one method online, mail or fax of submission. No phone calls, please. Only applicants selected for interviews will be contacted. Vera is an equal opportunity/affirmative action employer. All qualified applicants will be considered for employment without unlawful discrimination based on race, color, creed, national origin, sex, age, disability, marital status, sexual orientation, military status, prior record of arrest or conviction, citizenship status, current employment status, or caregiver status. Vera works to advance justice, particularly racial justice, in an increasingly multicultural country and globally connected world. We value diverse experiences, including with regard to educational background and justice system contact, and depend on a diverse staff to carry out our mission. For more information about Vera and CIJ’s work, please visit www. vera. org. Powered by Jazz HR M1w Ln77k XT"
126,International Data Analyst,data analyst,/rc/clk?jk=f3b5cbe551ba7847&fccid=718f6f4dd822068c&vjs=3,"At Nova Credit, we’re on a mission to make financial access easier, no matter where you’re from or where you go. We’ve built a platform that enables lenders to access a single, predictive cross-border credit database to help underwrite immigrants and other global citizens. In effect, we are the first global infrastructure for financial identity to seamlessly move around the world. We’re passionate about financial inclusion, and committed to building a diverse and fulfilling work culture. In International Data & Analytics, you will play an essential role in researching, designing and building Nova Credit’s global data and analysis strategy. You will report to Nova Credit’s Head of Risk and Analytics. You will serve as a conduit to bring key functions together, including data partnerships, risk & analytics, product & engineering, and customer success to develop and implement Nova Credit’s core data systems. You will work with customer-facing teams to analytically demonstrate the strength of Nova Credit’s products and services. Ultimately, your role is to ensure that Nova Credit’s products deliver high quality predictive risk signals. Every initiative you work on will be critical for the company’s success. Responsibilities Risk Analytics Leverage and integrate data streams to deliver reporting and analytic insights with a focus on providing end users with superior credit, risk, affordability and underwriting strategies Data & Analytics Strategy Research and contribute to the formulation of Nova Credit's long-term data strategy with global credit bureaus, alternative data providers and evolving predictive data sources Business Support Develop a robust customer facing knowledge base and set of tools to communicate the intricacies of Nova Credit’s platform, bureau integrations and data assets. Support Business Development and Success teams in product expansion, inquiry and resolution work streams. Qualifications 7+ years experience working with credit and bank transaction data, consumer credit risk analytics and strategy development MBA/MS degree; relevant Master’s preferred e. g. Economics, Statistics Intellectual curiosity is essential along with creativity and technical expertise as you work with data from diverse sources to develop powerful insights Excellent interpersonal skills including technical communications in order to develop and manage multiple external relationships as well as coordinate resources cross-functionally with multiple internal teams to deliver a consistently excellent client experience. Experience in consumer financial services. Should have a capacity to develop simple industry-standard analyses and tools Passion for enabling underserved consumers Everyone is welcome at Nova Credit. We are an equal opportunity employer where our diversity and inclusion are central pillars to our company strategy. We look for applicants who understand, embrace and thrive in a multicultural and increasingly globalized world. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. "
127,"Data Analyst, Fixed Income",data analyst,/rc/clk?jk=84d8d40a7f7c775b&fccid=0140e51652b12787&vjs=3,"Description Primary Responsibilities Building pre/post-trade analytics application for Fixed Income products with an emphasis on e Trading/Algo metrics. Supporting and enhancing existing data analysis applications across a variety of technologies Java, Python Work with desk quant, trader, and tech team to product key KPI to drive E-Trading market share, improve Hit Rates, and optimize liquidity sourcing methods. Support and enhance current Qlik platform for Business Intelligent Reporting Support initiatives for data integrity and normalization Qualifications Education BS/MS in Computer Science, Mathematics, or Financial Engineering Required Background Able to communicate effectively with the desk. Currently in Data analysis role with data mining experience a must Strong attention to detail Ability to collaborate effectively and work as part of a team Knowledge of basic bond math a plus Knowledge of AWS Sagemaker or Databrick a plus Knowledge of E-Trading Market Structure a plus Knowledge of KDB/Q a plus Knowledge of Python/Machine Learning API is a plus Primary Location US-New York Job Information Technology Organization Corporate Schedule Full-time Employee Status Regular Job Level Non-Management Job Posting Jul 23, 2020, 10 56 23 AM"
128,Salesforce Data Analyst,data analyst,/rc/clk?jk=3477dbee63544250&fccid=a575b7c5c50b587a&vjs=3,"A pioneer in K–12 education since 2000, Amplify is leading the way in next-generation curriculum and assessment. Our captivating core and supplemental programs in ELA, math, and science engage all students in rigorous learning and inspire them to think deeply, creatively, and for themselves. Our formative assessment products turn data into practical instructional support to help students at every skill level build a strong foundation in early reading and math. Our programs provide teachers with powerful tools that help them understand and respond to the needs of all their students. Today, Amplify serves five million students in all 50 states. The Salesforce Data Analyst will be responsible for the fidelity and accuracy of the data in the Salesforce system as well as the maintenance of data, processes, profiles and architecture of our Salesforce. com solution. The analyst will work closely with other CRM Team members and business subject matter experts to process data hygiene requests, update requirements and business processes specifications, and aid in large projects. The analyst will also work closely with IT development teams CRM and Enterprise Data Mart on projects intended to improve user experience and operational functionality; they must also work closely with SME’s to reconcile sales data between CRM and corporate systems of record. In addition, the analyst may assist in developing proposals for new policies and procedures, metrics, and for documenting process flows. Responsibilities Specialize in collecting, organizing, and analyzing data from various systems Develop an in-depth understanding of the interrelationships of data and multiple data domains Create and maintain thorough documentation artifacts Identify and document data migration paths and processes Demonstrate commitment to providing customer-focused quality service Develop and perform standard queries to ensure data quality, identify data inconsistencies, and missing data then resolve as needed Standardize data naming, data definitions, and modeling Perform data extraction, storage, manipulation, processing, and analysis Collect, profile, collate, and map appropriate data for usage in new or existing solutions as well as for ongoing data analysis activities Interpret customers’ functional and information needs and turn them into functional or data requirements, process models, etc. Perform other duties as deemed relevant based upon experience Basic Qualifications Bachelor’s Degree or equivalent with 1 or more years of experience related to this position 1-3 years of demonstrated experience in data analysis, data manipulation, and decision support Knowledge and application of relational database concepts Experience or exposure to Extract/Transform/Load ETL and/or Business Intelligence Greater than 1 year working in SQL Experience completing technical design via iterative mockups Several years experience using MS Office Suite esp. Word and Excel Preferred Qualifications High-level knowledge of enterprise IT organizational, business, and technical environments High aptitude in analytical and logical thinking Demonstrated experience with data models and data mapping Experience with ETL tools and procedures any tool Prior experience working in the Cloud Ability to perform ad-hoc analysis and maintain clear and comprehensive documentation of decision-making, data preparation, and processes Openness to learning new skills and software as necessary Demonstrated ability to manage simultaneous work flows while maintaining quality output and meeting deadlines Demonstrated ability to learn new and exciting technologies with little assistance Strong verbal and written communication skills with demonstrated ability to communicate at different levels of the organization Demonstrated ability to produce highly detailed, comprehensive documentation artifacts Client-focused attitude Ability to analyze processes and recommend improvements as needed We celebrate diversity and are committed to creating an inclusive environment for all employees. To that end, we seek to recruit, develop and retain the most talented people from a diverse candidate pool. Amplify is an Equal Opportunity Employer of Minorities, Females, Protected Veterans and Individuals with Disabilities. This position may be funded, in whole or in part, through American Recovery & Reinvestment Act funds. Amplify Education, Inc. is an E-Verify participant. "
129,FREELANCE MARKETING DATA ANALYST,data analyst,/rc/clk?jk=d71b6eb3cf9d33fb&fccid=ecfa9a8d5cc12928&vjs=3,"New York Doremus is looking for a Freelance Marketing Data Analyst with a technical background to immediately join our team for a period of 2-3 weeks, with a probability of extension. The analyst needs to be an Excel virtuoso, intimately familiar with paid social data Twitter, Facebook, Instagram, Pinterest , as well as web/Google Analytics reporting. Responsibilities Design and develop a comprehensive report presentation output in Power Point Manipulate, clean and finalize data structures in Excel Discover, analyze, and quantify trends and correlations Qualifications Master knowledge of Excel Experience with manipulating and analyzing quantitative marketing data Knowledge of Doubleclick, Twitter Ads, Facebook Ads Strong initiative and willingness to approach work with a sense of urgency Strong academic profile with an emphasis on quantitative coursework e. g. , degree in Mathematics, Statistics, Economics, Econometrics, Market Research, or similar fields Requirements Experience developing campaign reporting Master level experience with Excel3+ years in marketing analytics with a heavy emphasis on analysis and insights generation Experience with Paid Social data Facebook, Twitter, Instagram "
130,Senior Data Analyst,data analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0C5IatSLh_Ak1q39eQQoPIxD737RW9NeiYGvIRXkrLjEBNGhZvRfJ822RSR1sk8CpRmMFLf0mj6oZSsqWb6_XHqqsUe7XCEJksArEINM0vQrJ81TFJczxHa5vq3Q7fs9ABJu3LA6wLkzDKY88cqy1xC-kvVHmJeve4cgjqCHf4UAcDUTUm9ikeZQr-Fot3XhdFrABYFfIX4B5Fjecrk_fp9GqOfYCKEoyBNn04WqDeCB7TXoYGXiDtr9BPh9EMG79wewLSHL4WF2-xDn7ORTgMa5_q3MBT77Yon2bfFM9N2poPwrgnkrPxweyeKPmlVcvh_2dA2P3TQ7KUfngHp1heWbiMpyXkilsZFG8sqe-8e07Tl-1Yx-HXdBMXt9As1_VBXfuTEXPmYM5fWnS7cIuPI9J5JAOaTj7gVh6UXK3bdV61mq8OisahIVwUgLv-jNjuE5r-EATRa-FYylauPEmZVdb30zxqiJQffLRZksd4qi2pkyNMFNlmZDHiHgmfe1MQo7i7LFYMwwqRLAsgmLbB6ro3ONXTDD2g7luK7dzR658nF2SZugzVdiS5JmMvAhOM=&p=2&fvj=0&vjs=3,"RESPONSIBILITIES Kforce has a client that is seeking a Senior Data Analyst in New York, NY. REQUIREMENTS Bachelor's degree with 7+ years of work experience in data analysis and visualization Minimum 4 years of experience working with healthcare data and helping businesses make better data driven decisions Minimum 4 years of strong troubleshooting and problem-solving skills Experience in preparing and conducting extensive business analysis and studies, process assessments, data analysis and cost benefit analysis in an effort to support data driven strategies Experience with various data platforms Proficiency in PL/SQL coding Willingness to participate in team on-call rotation, and perform off hours work as needed Strong technical and project management/process leadership skills Excellent troubleshooting and problem-solving skills Knowledge of healthcare industry systems and applications is a plus Ability to work within a diverse, technically oriented, team setting is a must Demonstrated ability to take ownership of incidents, coordinate effective response, and drive improvements through root cause analysis and lessons learned The ability to quickly absorb new changes in computer technologies is necessary Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. "
131,Senior Data Analyst,data analyst,/rc/clk?jk=7285b2a78b6cccd9&fccid=4146d8487cdbf799&vjs=3,"The Bureau of Equitable Health Systems BEHS within the Center for Health Equity and Community Wellness at the NYC Department of Health & Mental Hygiene DOHMH seeks a Senior Data Analyst City Research Scientist III to work within the Health Care Access & Policy HCAP unit. During this period of change in the health care system, particularly amidst the COVID-19 pandemic, primary care remains a key anchor to advance population health, but the landscape is undergoing rapid change. HCAP drives policy, programming, and research that maximizes health insurance coverage and increases access to affordable, high quality, and coordinated primary care with a strong focus on health equity. HCAP has a dedicated staff working to improve access to quality, culturally appropriate primary care for New Yorkers, particularly those who are at risk for poor health outcomes, by ensuring that on-the ground programmatic work, policy, and data activities inform one another. HCAP staff utilizes multiple data sources and policy expertise to analyze the health system in NYC and identify barriers to obtaining insurance coverage and accessing primary care services. This includes a focus on health care safety net components such as Federally Qualified Health Centers FQH Cs , health insurance including public programs such as Medicaid and the Children's Health Insurance Program , federal shortage designations Health Professional Shortage Areas, Medically Underserved Areas, Medically Underserved Populations , and health care system utilization and efficiency. DUTIES WILL INCLUDE BUT NOT BE LIMITED TO -Analyze health system and hospital data related to the COVID-19 response. -Identify research questions and activities related to the use of convenient care sites for COVID-19 testing. -Create table shells and figures to summarize COVID-19 metrics. -Write code to automate daily COVID-19 reporting of metrics sent out to various other staff involved in the COVID-19 response. -Provide leadership for data analysis, research, and surveillance activities. -Analyze existing data to support planning and policy development. -Develop analysis plans, data-driven reports, and presentations. -Conduct spatial analyses, including producing maps. -Support database management and evaluation activities. -Coordinate projects and research activities. Assist with preparing scientific manuscripts for publication. -Work collaboratively with internal and external groups to align data collection and data integration. -Prepare reports, briefs, and presentations on research findings and recommendations while appropriately handling sensitive and confidential data. -Present findings within the bureau and across the agency, as necessary. -Assist on special projects as needed. -Additional duties as assigned. Minimum Qual Requirements 1. For Assignment Level I only physical, biological and environmental sciences and public health A master's degree from an accredited college or university with a specialization in an appropriate field of physical, biological or environmental science or in public health. To be appointed to Assignment Level II and above, candidates must have 1. A doctorate degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and one year of full-time experience in a responsible supervisory, administrative or research capacity in the appropriate field of specialization; or 2. A master's degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and three years of responsible full-time research experience in the appropriate field of specialization; or 3. Education and/or experience which is equivalent to ""1"" or ""2"" above. However, all candidates must have at least a master's degree in an appropriate field of specialization and at least two years of experience described in ""2"" above. Two years as a City Research Scientist Level I can be substituted for the experience required in ""1"" and ""2"" above. NOTE Probationary Period Appointments to this position are subject to a minimum probationary period of one year. Preferred Skills Strong research and analytical skills Excellent project management skills Experience with formulating research questions, statistical inference, hypothesis testing, and data visualization/data exploration Expertise in manipulating and analyzing data using statistical software such as SAS, SUDAAN, or R Experience in quantitative research and analysis in epidemiology, public health, population health, and/or health services, including experience with large clinical, administrative, or survey datasets Expertise in analysis of complex surveys such as the Community Health Survey, American Community Survey Public Use Microdata Sample, MEPS, BRFSS, and NAMCS. Experience with administrative data such as SPARCS and Medicaid claims databases a plus Experience using GIS software such as QGIS or Arc Map Experience analyzing spatial data Ability to independently conduct epidemiological and population-health analyses Excellent written and oral communication skills and proficiency in Microsoft Word, Excel, Outlook, and Power Point Ability to communicate complex research findings to wide range of audiences. Some knowledge of health-related topics such as primary and preventive care, health care systems, health care reform, public health insurance programs such as Medicaid, and/or access to health insurance and care for low-income and other vulnerable populations Successful experience working under strict deadlines and fulfilling ad hoc requests. Additional Information **IMPORTANT NOTES TO ALL CANDIDATES Please note If you are called for an interview you will be required to bring to your interview copies of original documentation, such as A document that establishes identity for employment eligibility, such as A Valid U. S. Passport, Permanent Resident Card/Green Card, or Driver’s license. Proof of Education according to the education requirements of the civil service title. Current Resume Proof of Address/NYC Residency dated within the last 60 days, such as Recent Utility Bill i. e. Telephone, Cable, Mobile Phone Additional documentation may be required to evaluate your qualification as outlined in this posting’s “Minimum Qualification Requirements” section. Examples of additional documentation may be, but not limited to college transcript, experience verification or professional trade licenses. If after your interview you are the selected candidate you will be contacted to schedule an on-boarding appointment. By the time of this appointment you will be asked to produce the originals of the above documents along with your original Social Security card. **LOAN FORGIVENESS The federal government provides student loan forgiveness through its Public Service Loan Forgiveness Program PSLF to all qualifying public service employees. Working with the DOHMH qualifies you as a public service employee and you may be able to take advantage of this program while working full-time and meeting the program’s other requirements. Please visit the Public Service Loan Forgiveness Program site to view the eligibility requirements https //studentaid. ed. gov/sa/repay-loans/forgiveness-cancellation/public-service ""FINAL APPOINTMENTS ARE SUBJECT TO OFFICE OF MANAGEMENT & BUDGET APPROVAL” To Apply Apply online with a cover letter to https //a127-jobs. nyc. gov/. In the Job ID search bar, enter job ID number # 441952. We appreciate the interest and thank all applicants who apply, but only those candidates under consideration will be contacted. The NYC Health Department is committed to recruiting and retaining a diverse and culturally responsive workforce. We strongly encourage people of color, people with disabilities, veterans, women, and lesbian, gay, bisexual, and transgender and gender non-conforming persons to apply. All applicants will be considered without regard to actual or perceived race, color, national origin, religion, sexual orientation, marital or parental status, disability, sex, gender identity or expression, age, prior record of arrest; or any other basis prohibited by law. NOTE This position is open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate in your resume that you would like to be considered for the position under the 55-a Program. Residency Requirement New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview. "
132,Lead Data Analyst,data analyst,/rc/clk?jk=2addd2fe19a886ef&fccid=5549c6b9723b4956&vjs=3,"At Rockstar Games, we create the games we would want to play ourselves. A career at Rockstar is about being part of a team working on some of the most creatively rewarding, large-scale projects to be found in any entertainment medium. You would be welcomed to a friendly, inclusive environment where you can learn, and collaborate with some of the most talented people in the industry. Rockstar New York is seeking a Lead Data Analyst to help build a cutting-edge game analytics platform and tools to better understand our players and enhance their experience in our games. This is a full-time permanent position based out of Rockstar's unique game studio in the heart of downtown Manhattan. WHAT WE DO The Rockstar Analytics team provide insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making. We partner with multiple departments across the company to design and implement data and pipelines. We collaborate as a global team to develop cutting-edge data pipelines, data products, data models, reports, analyses, and machine learning applications. RESPONSIBILITIES Lead the development of consumer-ready recommendations, insights, and reporting to Analytics team leadership, live producers, product managers, and partner groups. Effectively translate statistical findings into actionable recommendations for senior leaders on game and business teams. Lead the research and development of state-of-the-art data mining and reporting solutions to understand game design, system performance, and key business behaviors, such as player engagement. Drive needle moving improvements via the exploration of data, dashboards, statistical analysis, and predictive modeling. Combine qualitative and quantitative driven insights with game knowledge to recommend potential avenues to the game and business teams. Lead the awareness and training of data and tools. Facilitate the analytical needs of the business, collaborate in the design of diagnostic solutions, and deliver exceptional data products and insights. Lead key strategic project initiatives, develop comprehensive project plans, solution design, timelines, resource plans, communication, and other pertinent documentation. Provide technical leadership and direction to define solutions. Lead scoping and business priority-setting and develop functional specifications. Lead the recruitment process of best in class Analysts. Coach talented analysts to realize their proven potential. QUALIFICATIONS 3+ years in team leadership and management. 8+ years in an Analytics or similar role in the video game, marketing, finance, forensics, or technology fields required. Bachelor's degree in a quantitative field Statistics, Applied Mathematics, Operational Research, Business Intelligence or any other relevant field . Extensive knowledge of data analysis techniques, languages, software and systems R, Python, Tableau required. Extensive knowledge in SQL or a SQL-like language required. Experience with Hadoop. Passion for Rockstar Games and our titles. SKILLS Ability to develop and maintain good relations and communicate with people at all hierarchical levels. Strong problem-solving skills. Ability to reconcile technical and business perspectives. Autonomy and entrepreneurship. Strong mentoring abilities. Strong team spirit. PLUSES Please note that these are desirable skills and are not required to apply for the position. Experience with Snowflake, an asset. Graduate degree MBA, M Sc or Master's, PHD , an asset. Game industry experience strongly desired. HOW TO APPLY Please apply with a resume and cover-letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process. Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities. If you've got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, or race. "
133,Data Analyst - Business Intelligence (Lead Data Analyst),data analyst,/rc/clk?jk=5e63410ef13e7a2b&fccid=b4048be2884af072&vjs=3,"The Bank of New York Mellon is a provider of investment services and investment management and we are a world-renowned leader in each. When combined, the power of our extensive capabilities can help drive your success in markets around the world. BNY Mellon is the corporate brand of The Bank of New York Mellon Corporation. BNY Mellon Technology's mission is to provide our business partners with technology based solutions that enhance their ability to be successful through world-class software solutions maintained on a stable and secure infrastructure, and to provide our employees with the tools and means to enhance their professional qualifications and careers. Our Team TSG Technology Services Group provides reliable, resilient, next-generation enterprise technology infrastructure and support that enables BNY Mellon employees to deliver the whole firm to their colleagues and clients around the world. We are continuously improving automation and bridging the gap between cost, customer needs, and innovative tools to ensure the best possible experience. TSG Production Services team is dedicated to ensuring that our technology environments are best in class. Our Production Services team runs the systems that keep BNY Mellon running. Ourmission is to deliver a full IT service management lifecycle across all run-the-bank applications and infrastructure support with the purpose of providing a controlled, secure, and predictable production environment. Our team members use their technical skills and business knowledge to bring forth solutions that will take our company into the future. If you are a collaborative continual learner with a global mindset and a desire to contribute to our company’s top priorities, this is the place for you. The Role As an integral member of our Operations Solutions team, we are seeking a Data Analyst to implement and support business intelligence solutions, maximize operational business productivity through information availability, provide support and the information needed for optimizing business processes. You must have a successful track record of developing robust BI solutions, primarily using Microsoft SQL Server technologies. Key Responsibilities Complete project responsibilities including design, coding, unit testing and documentation in a timely manner and at required level of quality Gather business requirements and translate them into effective BI solutions that make business decisions efficient, actionable, trustworthy, and timely. Define and implement data quality processes, monitor and detect data quality issues Design, build and implement ETL scripts using Microsoft SQL Server Integration Services Design, build and implement BI reports using SQL Server Reporting Services Optimize complex stored procedures, scripts, functions and other BI components for best performance Provide support to the business community for self-service BI Thrive in a team environment and able to work independently as well Lead Data Analyst->> Analyzes application requirements and develops conceptual, logical and first-cut physical database designs data models . Creates associated data model documentation such as entity and attribute definitions and formats. Assists in logical data designs to deliver stable and flexible high performance data solutions. Investigates and corrects data discrepancies by reconciling faulty codes. Provides data element naming consistent with standards and conventions and ensures that data dictionaries are maintained across multiple database environments mainframe, distributed systems . Ensures data content/quality by planning and conducting moderately complex data warehouse system tests, monitoring test results and taking required corrective action. Acts as a liaison to data owners to establish necessary data stewardship responsibilities accountability for a particular data element/verifying accuracy of the data element before loading it into the database and procedures. Analyzes and designs data models, logical databases and relational database definitions using both forward and backward engineering techniques. Seeks opportunities to promote data sharing, and to reduce redundant data processes within the corporation by identifying common structures across application areas. Contributes to the achievement of related teams' objectives. Bachelor's degree in computer science or a related discipline, or equivalent work experience required. 4-6 years of experience in data modeling, data warehousing, data entity analysis, logical and relational database design, or an equivalent combination of education and work experience required, experience in the securities or financial services industry is a plus. Qualifications Bachelor's degree in computer science or a related discipline, or equivalent work experience required. 4-6 years of experience in data modeling, data warehousing, data entity analysis, logical and relational database design, or an equivalent combination of education and work experience required, experience in the securities or financial services industry is a plus. Required Experience with Microsoft SQL Server 2016 -2019, including Microsoft SSAS, SSRS, and SSIS 5+ years of experience in T-SQL, MSSQL 2016+ complex Stored Procedures, scripts and Functions 3+ years of ETL experience using SSIS other ETL tools Pentaho, Talend, etc. a plus Candidate must demonstrate strong software development skills and be able to communicate well with our team and external consultants Ability to analyze and resolve technical issues and problems and to identify different design alternatives. Ability to communicate complex technical details to team, management and non-technical people Excellent data profiling experience Excellent data modelling experience CA Erwin a plus Excellent data normalization experience All forms 1NF-6NF, BCNF, DV & Anchor Experience with BI data visualization tools is desired SSRS, Power BI Preferred Experience with BIML/BIML Script Experience with Microsoft MDM and DQS Experience with Data Vault 2. 0 Experience with Biml Flex Knowledge of Service Desk/ITIL processes and KPI's Any formal training or experience with Data Vault 2. 0 a Agile development methods BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums. Primary Location United States-Pennsylvania-Pittsburgh Internal Jobcode 45092 Job Information Technology Organization Technology Services Group-HR06725 Requisition Number 2006905"
134,FAA Business Management Data Analyst,data analyst,/rc/clk?jk=b440adcc8688ec2c&fccid=0c39fb2c91742dcf&vjs=3,"FAA Business Management Data Analyst Job Number 3154961 POSTING DATE Jul 22, 2020 PRIMARY LOCATION Americas-United States of America-New York-Purchase JOB Wealth Management EMPLOYMENT TYPE Full Time JOB LEVEL Analyst DESCRIPTION Morgan Stanley “MS” is a global financial services firm that conducts its business through three principal business segments—Institutional Securities, Wealth Management, and Asset Management. Wealth Management provides comprehensive financial advice and services to its clients including brokerage, investment advisory, financial and wealth planning, credit & lending, deposits & cash management, annuities, insurance, retirement and trust services. The Financial Advisor Associate FAA Talent Management group drives the overall field strategy relative to the sourcing and professional development of new Financial Advisor Associates for Morgan Stanley Wealth Management. The FAA Training Program encompasses a variety of pathways to becoming a Financial Advisor at Morgan Stanley Wealth Management and is central to the Firm’s long-term sustainability and revenue growth. POSITION SUMMARY The FAA Business Management Analytics and Technology Lead will be responsible for driving data, analytics and technology initiatives for the broader FAA Program. This person will report to the Head of Business Management for FAA Talent Management and serve as the liaison between various departments including, but not limited to, Human Resources, Finance, Technology, The Office of Business Management and Diversity teams as it relates to all specific requests and deliverables. ESSENTIAL JOB FUNCTIONS Work with the larger FAA team and strategic partners to understand and prioritize data, metrics and technology needs. Assist in the definition of project scope and objectives. Facilitate meetings with different stakeholders to gather business requirements. Create and manage a detailed work plan for each project that identifies and sequences the activities needed to successfully complete them. Develop a schedule that effectively allocates resources to each project. Work on ad hoc projects aimed to evaluate business processes, uncover areas for improvement, and develop and implement solutions Conduct strategic analysis to better inform program decisions Creation of presentation materials for Senior Management Manage Data, Analytics and Technology Squad QUALIFICATIONS Bachelor’s degree with strong academic record Minimum 3-5 years’ experience in business reporting, analytics or other project management roles Strong project management, organization, communication, and analytical skills Advanced knowledge of Microsoft Office especially Excel required. Familiarity with other data analytics and presentation tools a plus Active listening and questioning skills; essential in gathering data and requirements Strong relationship management skills with ability to build a sustainable relationship with stakeholders Practical experience generating process documentation and reports Experience with Tableau is a plus The ability to influence stakeholders and work closely with them to determine acceptable solutions Knowledge of Salesforce or other CRM platforms preferred"
135,Senior Securities Data Analyst,data analyst,/rc/clk?jk=d495729ea9b88fb2&fccid=4983b5e32f963a34&vjs=3,"Summary The Neuberger Berman Operations Team is responsible for all aspects of operational support associated with our high net worth and institutional asset management teams. Responsibilities would include the set-up, maintenance and exception monitoring of security master across all products and ensuring data quality in the trading and accounting platforms across the firm as well as the timeliness, completeness & accuracy of securities pricing/valuation data. The candidate should be able to perform the necessary facilitation, analysis and design tasks related to the development of Master Reference Data & Pricing solutions for equities, fixed income and derivatives. This candidate will also work directly with portfolio managers, traders, IT, and other operations groups on security data and pricing matters. Responsibilities Responsible for set up, maintenance and review of securities in trading and accounting systems. Manage and maintain pricing data across all product types and internal systems. Analyze exception reports detailing price discrepancies and variances over a defined tolerance. Work with internal sources and external vendors on price verification. Analyze current business practices and procedures and define and implement processes and procedures to ensure data integrity. Help to define and document policies and procedures, including the designing and documenting of process flows across multiple functions within Operations. Identify and manage the resolution of data quality issues, recommending appropriate corrective action. Assist in setting up Service Level Agreements with business partners, scheduling the work to meet expectations. Responsible for producing quality and quantity metrics measuring performance. Timely escalation of issues and resolution to Senior Management. Requirements Bachelor degree, B. S or B. A. 3-5 years of experience in Financial Services / Asset Management Operations, especially around security data. Knowledge of Equity, Fixed Income, Derivatives and Leveraged Loans. Understanding of data management and data platforms. Exceptional analytical, conceptual and problem-solving abilities. Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a concise and objective manner to both technical and business audiences. Excel knowledge a must. Excellent team player able to work with virtual and global cross-functional teams Proficiency with Microsoft Word, Power Point and advanced proficiency with Microsoft Excel Experience with Bloomberg preferred; experience in platforms like Aladdin and Charles River a plus. Effective time management skills; strong interpersonal skills. Ability to multi-task and function well independently in a dynamic, changing environment. Neuberger Berman is an equal opportunity/affirmative action employer. The Firm and its affiliates do not discriminate in employment because of race, creed, national origin, religion, age, color, sex, marital status, sexual orientation, gender identity, disability, citizenship status or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact onlineaccommodations@nb. com . Learn about the Applicant Privacy Notice . "
136,Data Analyst,data analyst,/rc/clk?jk=8656231195af22c1&fccid=42baa9286dfcea6d&vjs=3,"R2Net is a diamond bridal jewelry company with a technology soul and one of the Signet Banners. The Data Analyst assists the Business Intelligence Manager by preparing, organizing, and maintaining the company’s data collection. The tasks completed in this role allow the team to improve the quality and consistency of the data used to make informed decisions and process improvements critical to the success of the company. ESSENTIAL RESPONSIBILITIES Analyze internal data architecture and data integrations to identify inefficiencies and to assist BI Manager in making best practice recommendations for modeling business data and BI solutions Actively engage with the operations team to understand the data requirements, data flows, data entities and data relationships Maintain existing data collection and analysis systems in support of the business Develop data reports or templates for internal clients Contribute to the design and implementation of analytical and data visualization tools Work on special projects and assignments, as needed REQUIREMENTS AND QUALIFICATIONS Excellent work ethic, including a desire to learn continuously. Excellent written and verbal communication skills. Ability to work both independently and as part of a team in a multi-task environment with attention to detail. Ability to keep many competing priorities in order and to learn quickly in a self-directed manner Background in a shared services environment doing reporting and/or analytics a plus Ability to proactively investigate data and troubleshoot errors autonomously SKILLS Python or programming experience with the ability to learn Python quickly Query languages SQL, Graph QL, Jaql, or equivalent a plus Java Script, HTML a plus EDUCATION and/or EXPERIENCE Bachelor’s degree from an accredited college or university is preferred Studies in computer science, statistics, or related fields is a plus PERKS Choose your own Tech Tuition reimbursement, learning groups, and training Generous PTO program"
137,"Junior Target Data Analyst, CDP North America",data analyst,/rc/clk?jk=b8450121da501f20&fccid=ccffb6f5de1d2e11&vjs=3,"Job Purpose and Background Are you passionate about using data to drive change? Do you have excellent data analysis skills alongside the ability and desire to work with users of data and to communicate well? CDP’s Data Analytics Team is looking for a skilled, enthusiastic, and ambitious data analyst who is passionate about using data to drive companies, investors, and governments to build a thriving economy which works for people and planet. You will work primarily on a project to analyze and improve the quality of corporate target data reported to CDP. This piece of work supports key projects with several key partners of CDP. Additionally, you will be involved in developing robust progress tracking methodologies to as-sess a variety of corporate and subnational climate actions. Working with data disclosed to CDP, you will deliver value-adding Analysis and Insight. This Analysis and Insight helps inter-nal and external stakeholders make better actionable decisions. The Data Analytic team works to improve the usability of the data disclosed to CDP. The team develop and apply robust and transparent methods for data cleaning and modelling. Key responsibilities Delivering vital data product projects across CDP and other stakeholders such as Cleaned Corporate Targets Dataset. Delivering exciting new Data products, according to Stakeholder requirements. Delivering analytics across CDP and other stakeholders. Support colleagues, investors, NG Os, and other data consumers in the use of our data, data products and make better decisions from them. Data requirement gathering and data testing for key tools. Required skills and experience At least 1 years of data analysis experience using Python/R. Experience of data mining and cleaning approaches, using basic Python and R li-braries such as pandas, dplyr and tidyr. Experience of data visualization skills using Power BI or similar BI tools. Experience of SQL or T-SQL for querying relational databases Great data wrangling skills. Experienced in data cleaning and statistical modelling. Good written and oral communication skills. Strong numeracy and good statistical skills. Excellent problem-solving skills. Desired skills and experience Familiarity with Git Hub, Linux, Shell scripting bash . A good understanding of GHG accounting methods and sustainability data. Knowledge of the financial system and capital markets. An awareness of environmental issues, particularly as they relate to our core themes of Climate Change, Deforestation and Water Security. This is a full-time 12-month contract position based at CDP’s New York office as part of the Data & Insight team. Interested applicants must be eligible to work legally in the United States. Salary and benefits Competitive non-profit salary, 24 days annual vacation, non-contributory 401K plan, flexible working opportunities and excellent others benefits. Working in one of the most successful and fastest growing initiatives driving climate action within the corporate sector; Exciting and challenging tasks in a dynamic, international, innovative and highly motivated team. Before you apply We will only use the information you provide to process your application. For more details on how we use your information, see our applicants privacy notice. By emailing us your CV and covering letter, you are permitting CDP to use the information you have provided for recruitment purposes. To apply Please email your CV and cover letter describing how you meet the required skills and experience, to recruitment. usa@cdp. net with ‘Junior Target Data Analyst , Your First name, Last name’ in the subject by 11pm EST by July 31st 2020. CDP North America, Inc. is an equal opportunity employer. CDP North America, Inc. is committed to fostering, cultivating and preserving a culture of diversity and inclusion. "
138,Data Analyst,data analyst,/rc/clk?jk=63beca8bf5e6ffff&fccid=cf9b9ed5c1a95462&vjs=3,"Data Analyst EOE STATEMENT The Council on Accreditation provides equal employment opportunities EEO to all employees and applicants. COA does not offer sponsorship for any visas required for employment. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. If you are a qualified individual with a disability or a disabled veteran, and unable or limited in your ability to use or access this site as a result of your disability, you may request a reasonable accommodation by calling 1- 866 262-8088 or by visiting http //coanet. org/about/contact/. POSITION SUMMARY The Data Analyst creates actionable narratives from structured and unstructured data, helps drive a culture which incorporates data into all levels of decision-making. Situated within the Business Intelligence division, this position serves a critical role in developing meaning and insight from an array of data sources Demographic and performance data related to COA's accredited organizations, volunteers, and the accreditation process Marketing and sales data from Google Analytics, Pardot, and Salesforce Publicly-available data primarily from government entities This role will also work intensively on COA's emerging human services benchmarking program. This novel, forward-looking program will provide vital benchmark data on measures of organizational health and sustainability to COA's network and the national human services community. SPECIFIC DUTIES/RESPONSIBILITIES Data Environment Develop a deep understanding of COA's data model and continually identify improvements to Salesforce, Pardot, Google Analytics, and related assets Become proficient in COA's reporting environment, including native Salesforce reporting and advanced reporting methods Assist in the launch of Power BI throughout the organization, including training staff, creating dashboards, and providing customer support to staff Identify sources of third-party data for integration into COA's data model Benchmarking Program Assist in the development, launch, and ongoing management of a benchmarking program to define performance targets across several measures of organizational health and sustainability Assist in providing customer support to MOA-related questions both internally and externally Craft reports and presentations to share this data back to accredited organizations, volunteers, and the human services community Reports and Analyses Answer ad-hoc report requests from staff and organizations within a timely manner Maintain and continually enhance operations dashboards for COA staff Meet regularly with staff to review reports/analyses, process their meaning, and make recommendations regarding improvements or strategic direction Design and publish reports to clients and volunteers which transform demographic, performance, and accreditation process data into actionable insights Write concise narrative summaries, including recommended actions, for all complex analyses Integrate statistical testing into analyses as appropriate Conduct market research and other analyses using public data Performance Measurement Assist in the continued development of a robust performance measurement system Work with senior leadership to define KP Is, reporting methodologies, and dashboards Execute regular reporting and work with leadership to process and operationalize this data Other duties as assigned QUALITY EXPECTATIONS Practices reflect COA's mission and are responsible, flexible, reliable, and dependable Shows respect for each person's individuality and preferences and the cultural/ethnic diversity of COA's stakeholders Demonstrates a commitment to high quality and responsive service Contributes to a positive and collaborative work environment Maintains organized, efficient, and effective work habits Demonstrates a commitment to learning and improvement by pursuing professional growth Protects the confidentiality of data obtained by the nature of COA's work POSITION REQUIREMENTS EDUCATION AND PRIOR EXPERIENCE Bachelor of Science or Arts degree required; Master’s degree preferred At least 2 years of experience in data reporting, analysis, or evaluation required At least 2 years of experience with/in a human services organization SKILLS, KNOWLEDGE AND ABILITIES Required Experience with Salesforce Experience with BI tools e. g. Tableau, Power BI, Google Analytics, etc. High level of proficiency with Microsoft Excel Pivot Tables, charts, and advanced formulas like index/match and array formulas Self-driven desire to improve technical/analytical skills and continually improve understanding of COA’s accreditation process Ability to successfully communicate technical details to a non-technical audience Knowledge of data visualization best practices Basic understanding of statistical analysis Strong professional skills Ability to manage competing priorities, multi-task, and meet deadlines Strong communications skills Solution oriented problem-solving Commitment to the values of COA’s accreditation process Ability to work with minimal supervision Preferred Experience with statistical reporting tools e. g. , SPSS, Minitab, Mathematica, etc. Experience with Pardot, Google Analytics, or similar systems FULL-TIME/PART-TIME Full-Time ABOUT THE ORGANIZATION The Council on Accreditation COA is an independent nonprofit international accreditor of community-based social service and behavioral health organizations. Founded in 1977 and originally known as an accrediting body for children's and family services agencies, COA now accredits or is in the process of accrediting over 2,000 organizations or programs that serve more than 7 million individuals and families each year. COA’s mission is to partner with human service organizations worldwide to improve service delivery outcomes by developing, applying, and promoting accreditation standards. The Council on Accreditation COA offers competitive salaries with one of the most comprehensive benefits packages for an organization of our size and scope, including health and dental insurance, a voluntary vision plan, a 401 k plan, Employee Assistance Program, Short Term Disability, Long Term Disability, Life and other insurances. In addition, we offer many opportunities for professional growth and development, including an annual Professional Development Plan and tuition reimbursement. Whether you’re fresh out of college or an experienced professional, the Council on Accreditation COA offers incredible learning opportunities and growth. COA is one of the leading international accrediting bodies for social services. We are committed to customer service and have a strong culture of innovation and teamwork. Help change the world and make a difference by joining the COA team. "
139,Data Analyst with ASG & Lineage experience,data analyst,/company/Spica-Computers-LLC/jobs/Data-Analyst-Asg-Lineage-Experience-457e8d1acb1b1dc7?fccid=42f8b5cb2d387c85&vjs=3,"Interview Mode phone/Skype Visa USC, GC, TN, EAD/GC, H4/EAD, CPT/EAD Only Description Data cataloging & data profiling need at least 1 of these REQUIREDSQL Server, DB2, Oracle need at least 1 of these REQUIRED Must have hands on experience working with data lineage at a high level REQUIRED Must have financial industry background that is recent- REQUIRED Metadata layer experience REQUIRED Candidates must have ASG Becubic or ASG Rochade experience- REQUIREDIBM ISG/Watson, Informatica IMM or ECD experience is a PLUS Job Type Contract Salary $60. 00 $65. 00 per hour Work Remotely Temporarily due to COVID-19"
140,Global Cards Solutions Data Analyst,data analyst,/rc/clk?jk=e1e3c5038c1b3725&fccid=9da8868d3ef232e1&vjs=3,"Overview Support the data analytics function within Global Cards Solutions department. Responsible for working on both operations and product analytics. Main responsibilities include reporting, maintaining dashboards, ad-hoc analysis and projects. Assist management in developing segmentation, campaign design and operational analysis. Duties performed by this role support the creation of member-centric offers and products, along with the improvement of cards operations. Responsibilities Perform research and analysis derived from all internal and external information sources, including the UNFCU Data Warehouse, Cleartrend, TMG Dynamic Reports, First Data Reports, Visa Vue and Gemalto Allynis. Assist management with dashboard development and monitoring KP Is that track the profitability of cards. Maintain monthly and quarterly reporting. Ensure accuracy and timeliness. Support the execution of marketing campaigns by assisting with analytics, including list creation and tracking. Support budgeting analytics, including forecasting and an understanding of cost drivers. Assist with segmentation, combining both demographic and transactional data. Use internally generated data, including call center data, to identify pain points and improve the member experience. Develop, document, and maintain internal procedures related to card data. Monitor operational KP Is and find opportunities to automate manual processes. Work with internal and external stakeholders for ad-hoc analysis, reporting, process improvement and projects. Report and monitor competitive market data. Qualifications TYPE & AMOUNT OF EXPERIENCE Bachelor’s degree and 3-5 years of experience working with data Financial institution experience is preferred TECHNICAL COMPETENCIES Strong analytical skills with ability to translate large volumes of data into meaningful information and actions Experience working with large data sets using Excel, SQL or Python Experience in Tableau and knowledge of best practice data visualization techniques Knowledge of database infrastructure and data relationships Experience in cohort analysis, segmentation, deciles and campaign evaluation Development of campaign design, performance evaluation and testing strategies Exceptional MS Office skills Strong data manipulation and database application skills Project management skills BEHAVIORAL COMPETENCIES Strong problem solving skills Strong interpersonal, verbal and written communication skills Excellent service orientation Quality conscious and detail oriented Ability to multi-task and prioritize workload to manage multiple projects Ability to work independently WORK ENVIRONMENT/CONDITIONS Standard office conditions In addition to any specific job requirements in connection with Bank Secrecy Act and/or OFAC BSA , employee must i be aware of BSA matters commensurate with the position; ii report any suspicious activity to the manager or compliance department; and iii satisfactorily complete any required BSA training. "
141,Senior Data Analyst,data analyst,/rc/clk?jk=c4399695df4f70db&fccid=dc55ba6e7c37067b&vjs=3,"THIS IS BLUE APRON Chefs around the world wear blue aprons when learning to cook, and for us it has become a symbol of lifelong learning. Our highly motivated group of food enthusiasts are inspired to work on complex business problems with creativity and passion always looking to learn something new. We take preparation seriously, embrace a spirit of excellence, and put in the extra effort in everything we do. We are thrilled to be part of our customers' lives, cooking and creating experiences in their home kitchens. WHO'S IN THE KITCHEN Blue Apron's Analytics and Business Insights team develops critical metrics and actionable insights on our rapidly expanding customer base and the broader meal kit marketplace. This team also powers business decisions around complex supply chain and logistical challenges. WHAT'S ON THE MENU The Senior Data Analyst will drive efforts to iteratively improve Blue Apron's growth and retention marketing strategies. This will include efforts to size risk and opportunity, diligently test new marketing and promotional strategies, understand and improve attribution and payback models, create tools and dashboards for business partners, and work directly with senior management to implement new strategies. Candidates will be motivated self-starters who are comfortable navigating large datasets, building and/or interpreting predictive models, partnering with technical teams building digital products, coordinating with internal and external non-technical partners and understanding customer demand signals. They will be intimately familiar with survey and A/B testing methodology and interpretation. They will be comfortable presenting findings through both verbal and written formats. Candidates will feel confident in their abilities to influence decision making at the highest levels, understand requirements in working with cross functional teams, and be confident in presenting to individuals of all levels throughout the company. Key responsibilities include A/B testing marketing and promotional strategies Support company executives in understanding, forecasting, and managing demand in an emerging industry Improving Blue Apron's testing environment while supporting an aggressive marketing roadmap Translating data into actionable insights Identify, prioritize, structure, and analyze ad hoc analyses about the business's demand, customers, marketing efficacy, and business operations to answer questions and integrate insights into the forecasting process Provides customer profiling, segmentation analysis, survey design and analysis Devises, socializes, and garners support for practical and actionable recommendations for optimization NECESSARY INGREDIENTS Strong analytical, organizational, decision making, and presentation skills with experience communicating on an executive level A deep understanding of A/B testing methodologies, and forecasting Interest and experience in communicating advanced analytics concepts to non-technical executives 3+ years of hands-on experience working with very large datasets, including statistical analyses, data visualization, data mining, and data cleansing/transformation Experience with predictive modeling and demand forecasting, demonstrating knowledge of multiple statistical methodologies as well as an understanding of financial and operational impacts of the forecast Experience with performance and/or digital marketing Coding skills, including Advanced data manipulation skills in SQL Analytics tools experience such as Pandas, R, SQL a plus Experience performing analysis with large datasets Data visualization such as Tableau, Looker Demonstrated ability to multitask and prioritize diverse tasks with a proven ability to meet hard deadlines Ability to thrive in an unstructured, fast-moving, and constantly evolving environment Blue Apron provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, marital status or veteran status. In addition to federal law requirements, Blue Apron complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
142,Senior Pharmacy Data Analyst,data analyst,/rc/clk?jk=43605b03f7d53752&fccid=ec5a2dfcc7a0c05d&vjs=3,"ABOUT RIGHTWAY At Rightway we are committed to creating a diverse environment and are proud to be an equal opportunity employer. We hire for the best talent and actively seek diversity of thought processes, beliefs, background and education. Rightway fosters an inclusive culture where differences are celebrated to drive the best business decisions possible. We are committed to equal opportunity and fairness regardless of race, color, religion, sex, gender identity, sexual orientation, nation of origin, ancestry, age, physical or mental disability, country of citizenship, medical condition, marital or domestic partner status, family status, family care status, military or veteran status or any other basis protected by local, state or federal laws. Rightway Healthcare was founded in early 2017 by a team of dedicated healthcare, business and technology leaders. Rightway is a technology platform that provides employees and their families with the support, information and advocacy they need to be better healthcare consumers. We combine a high-tech consumer portal with a high-touch dedicated concierge environment to provide consumers with the same level of support and guidance they would receive if they had a doctor in the family. We are focused on personalization, implementation, advocacy and results. Rightway works with employers from 100 – 25,000 employees to provide an ecosystem of healthcare navigation for employees. We are committed to generating high member satisfaction scores, industry leading engagement and a quantifiable ROI for every employer. Our approach is rooted in analytics and guided by a bottom-up understanding of a population. Rightway is privately financed, having raised over $30mm to date from investors such as Thrive Capital. It is headquartered out of New York City, with new offices in Hudson Yards. We also have a remote office in Miami, FL. ABOUT THE ROLE We are seeking a full-time Senior Reporting Analyst to join our new pharmacy benefit management project, called Rightway Rx. You will collaborate with software engineers and data scientists to build out infrastructure and data pipelines atop pharmaceutical claims data. We view this role as a critical, high-leverage role and strongly believe that this role is central to our ability to become a disruptive technology company that improves patient health while lowering medical costs. RESPONSIBILITIES Develop an overall understanding of the business and clinical operations at Rightway Rx, our pharmacy benefit management project. Use SQL to develop analytics infrastructure that generates insight from pharmaceutical data Develop a visually stunning, interactive Tableau report that demonstrates Rightway Rx's value to our clients Interact with coworkers across departments to communicate progress and outcomes of planned work QUALIFICATIONS You have more than 4 years of industry experience working directly with pharmacy claims data using SQL. You have experience with either Medispan or First Data Bank products to analyze pharmacy claims data. You have considerable expertise using Tableau to create data visualizations and reports You are excited to work in a fast-paced start-up environment with a wide range of job responsibilities You care about Rightway's mission about improving the healthcare experience of our users"
143,"Data Analyst, Monitoring, Evaluation and Learning",data analyst,/rc/clk?jk=65b8f2e058a483c2&fccid=91852703289ad7d4&vjs=3,"For over 75 years, Episcopal Relief & Development has been working together with supporters and partners for lasting change around the world. Each year the organization facilitates healthier, more fulfilling lives for more than 3 million people struggling with hunger, poverty, disaster and disease. Inspired by Jesus’ words in Matthew 25, Episcopal Relief & Development leverages the expertise and resources of Anglican and other partners to deliver measurable and sustainable change in three signature program areas Women, Children and Climate. Data Analyst, Monitoring, Evaluation and Learning Remote – US Based The Data Analyst, Monitoring, Evaluation and Learning MEL ensures quantitative and qualitative data systems are fit for purpose and accurately capture results-based program management indicators. You will clean, analyze, aggregate and synthesize the data to inform project improvement, decision making, and reporting that supports Episcopal Relief & Development’s strategic goals and core values. As Data Analyst, MEL you will • Analyze, aggregate and synthesize qualitative and quantitative monitoring and evaluation data from community development projects; perform functions at field and multi-country levels, using various required results frameworks Create visualizations of analyzed and aggregated data and findings Develop a suite of filters around use of underlying data Support periodic implementation of data reviews by staff Tailor and provide relevant and/or historical data overviews of program results for use in proposals, major grants and fundraising materials Execute data management and integrity, contribute to regular systems review and optimization, and ensure data is clean, accurate and complete Ensure that data warehousing and storage capacity is in place and applied survey data is reconfigured into clear, visualizable and trackable indicators Systematically audit data points, testing their veracity, and clean as needed Transfer externally collected data to Episcopal Relief & Development’s proprietary systems Improve measurement and indicators based on data review and analyses to ensure intended information is accurately caputured Strengthen Episcopal Relief & Development’s MEL capacity building and learning by contributing to a stronger data analysis and use function and through incorporating relevant best practices, new methodologies and data management tools as they emerge Work as a member of a dynamic Strategic Learning/MEL team in the Program Department You Are • A confident communicator with the ability to explain technical concepts and information to multicultural audiences with varied levels of monitoring and evaluation skills An enthusiastic team member who is energized by data and its use, able to collaborate effectively with field data collectors, program managers and database developers, and effectively optimizes workflows and automates repetitive processes Detail oriented with strong organizational and project management skills and adept at meeting deadlines in a fast-paced environment Able and willing to work some Eastern standard time business hours, travel domestically and internationally intermittently and work a varied schedule including occasional evenings and weekends You Have • A shared commitment to Episcopal Relief & Development’s values, principles and philosophy An Advanced Degree in statistics, economics/econometrics, computer science, mathematics or equivalent quantitative area such as MPA with International Development concentration A minimum of five 5 years of qualitative and quantitative data management and statistical analyses experience, preferably in an international NGO context Proven experience in field based mobile data collection and system requirements Demonstrated experience in statistical testing, data mining, data visualization best practices and a solid understanding of prevalent database design and communication between data systems Demonstrated proficiency with qualitative assessment including focus groups and in- depth interviews; experience with most significant change story methodology a plus A proven ability to synthesize data and communicate results clearly through multiple formats and data visualizations Strong working knowledge of and proficiency with SQL, Excel, SPSS, STATA, and/or Epi Info as well as with N Vivo or other qualitative data analysis and mixed method analysis software Experience working with Tableau, Looker, Salesforce, Arc GIS or other visualization tools a plus Legal authorization to work in the US without sponsorship from Episcopal Relief & Development How To Apply Email resume and cover letter to careers@episcopalrelief. org with the subject line “Data Analyst, MEL. ” For more information, visit our website at www. episcopalrelief. org. Generous benefits package offered. Episcopal Relief & Development provides equal employment opportunities EEO to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, sexual orientation or any other legally protected status. Episcopal Relief & Development offers reasonable accommodations in the hiring and employment process for individuals with disabilities. If you need assistance in the application or hiring process to accommodate a disability, you may request an accommodation at any time. "
144,Business Data Analyst,data analyst,/pagead/clk?mo=r&ad=-6NYlbfkN0CQRQ3eiV4YWjrRS1ho7HVQ9JO8v6Fb3eU0yDOJbdOiErHG5GQKjhLaylYkHZ6ibcwakwrOWgHhuGm7juIQfE1tsT6RkR3o_MZBmjUHuUqsB8IfhLmU4gX417FNPBUHK9MdGCmI-2JldHH6xiKX5-kwUp4aQgg5HEpHejfysShUPS2ryP0sO8NaBDI2O4AFMYRX4fT0m8WMxXy78oo9iXfKPP5XAyIdDN-eyzMX9N_Ev8jg0AU7YRQgPSPmTdJP-IZMVUWSru1DAz6e9BQE1Kfx_NfaJyizTGMall-9ro4ffFSfn3HH9fniExuJ9iNhiUynQM06wugQK8g7ttmjDEHjNbp4jeiJY4QEnU7-HzWzvTP28-rJ4ZO1vTpzrTbh7ofJfZwyGTCQAoug9EOv2iiOWbfAKQx_YVck936Lpcn95SJCWWp3z4NDjO0bo34pOpC0FQSx1sQc0ggO_Nlcb52CRXZpQ4QDt70joQwUsekwGIhH01zl_tzBGajSrUyG-swnnv0y7_hXEQfVg5K7MXY78cruTsbTzSsaxWzhQa5pLIreR9Z6htaBkbZ-1Lyxzboj_M5NqtUoXujea0HnIX64xTeTrGnE4AeqZ21f3rU0oQ==&p=9&fvj=0&vjs=3,"Staffmark Workforce Solutions offers this exciting contract opportunity at a global leader in electronics, mobile devices and appliances located in Plano, TX. Organization ME Mobile Experience /Io T Biz Ops & Finance planning If you have ambitions to be a part of a Best in Class organization, this company is the place to be. From planning to execution, we handle the day to day sales for all of our partners. Being well rounded is critical on this team as you will work with Product leaders, Vendor partners, Supply Chain, Customers and Sales teams. The dynamic culture at this company offers both great challenge and great reward. In this role, you will be responsible for understanding the financial operations and processes for the Electronic America’s business units. You will provide assistance with financial reporting and analysis. You will participate in process evaluation, ad hoc reporting, and various financial duties. You will build & maintain various intelligence databases, and produce regular reports regarding key developments in Mobile Accessary and Watch industry. In this role you are an individual contributor that works under limited supervision and considered a seasoned and experienced professional with adequate understanding of own job area. Essential Duties & Responsibilities • Responsible for building and maintaining sales and forecast database • Responsible for building weekly reports using the database • Able to lead weekly conference calls with customers • Responsible for financial process systems and reconcile any differences • Applies advanced Microsoft skills; primarily in Excel and Powerpoint • Performs work within company and regulatory guidelines Background/Experience required • Strong database management- need to be very attentive to details • Prior experience in research analysis and/or business analysis is preferred • Prior experience in finance and/or supply chain management is preferred • Advance excel skills- VBA coding for macro is a plus • knowledgeable in basic finance processes • Fluency English, written and speaking is required Necessary Skills Develop and maintain excellent working relationships with all assigned levels with the customer and within and outside the company. Plan, organize, and prioritize multiple complex assignments and projects. Read and interpret detailed and complex customer requirements. Demonstrated competency in both oral and written communication modes for both internal and external personnel at various levels, especially in the sales and marketing, logistical, or financial areas of clients, prospects, and SEA. Work independently and in a team environment in order to achieve personal and team goals and complete assignments within established time frames. Demonstrated excellence in time management skills and follow up to ensure meeting on time deliverables. The ability to accept tasks and problems/situations that differ and require creativity to search for solutions among learned consequences. Only broad and general guidelines exist for solving problems. The ability to utilize implied knowledge of task alternatives and to make spontaneous decisions using past experience and the guidance of successful experiences of others. Interact with all levels within the organization and has frequent external contacts. The position will require a blend of analytical skills, strategic thinking, and organizational influence. Key success factors in this role include strong project management skills, attention to detail, statistics/finance acumen, enthusiasm, a self-starter attitude, practicality, agility, collaboration and communication. Staffmark talent working with this client receive competitive compensation and a great benefits package including medical, dental, vision, 401K and Paid Time Off plus more! About Staffmark Staffmark makes all employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, ancestry, medical condition, age, marital status, national origin, citizen status, political affiliation, union membership, genetic information, physical or mental disability, veteran status, denial of medical or family leave, pregnancy or pregnancy disability leave or any other protected group status as defined by federal, state or local law. We will provide reasonable accommodations throughout the application or interviewing process. If you require a reasonable accommodation, contact us. Staffmark is an E-verify employer. "
145,Sr Business Data Analyst (Shopper Insights and Marketing),data analyst,/rc/clk?jk=26154ec44feb42cc&fccid=a49d99f2875604a1&vjs=3,"Requisition ID 209511 Position Full-Time Luxottica is a global leader in the design, manufacture and distribution of fashion, luxury and sports eyewear. Our wholesale network covers more than 150 countries and our retail presence consists of over 9,100 retail stores across the globe. In North America, our wholesale business is the home to global brands like Ray-Ban, Oakley, and many of the top fashion house brands. Our leading retail brands include; Lens Crafters, Sunglass Hut, Pearle Vision, Target Optical and Sears Optical. We are also home to Eye Med, the fastest growing vision care company in the United States. Luxottica’s Wholesale teams deliver the highest quality frames, best-in-class expertise and a deep understanding of products and markets to support our customers and fuel their businesses all over North America. GENERAL FUNCTION The Shopper Insights and Marketing Sr Analyst is integral to our goal of rapidly growing online sales for our portfolio of brands via our e-retail and omnichannel wholesale partners. Responsible for assisting the Sr. Manager, e Commerce Strategy in managing the performance marketing for key Pure Play and Omnichannel customers Amazon. com, Walmart. com , and providing analytics services to stakeholders, enabling them to make fact-based decisions. The Analyst will Develop analyses that deepen the company’s understanding of the Optical/Sun wholesale and digital landscape, marketplace dynamics and trends in customer behaviors. The role will also be responsible for the management and review of marketing activities within the digital space informing the content development/creation, marketing investments and strategies for our brands on Amazon and other retailers. MAJOR DUTIES AND RESPONSIBILITIES ANALYTICS & REPORTING Manages and leverages digital research platforms e. g. Stackline, Google, etc. to conduct a range of analytics to translate data into actionable insights Identify, develop, manage, and execute analyses to support problem statements and current state measurement e. g. market-share analysis, analysis of brand / product potential with a retailer, etc Report key insight trends to inform the larger team of noteworthy storylines to drive actionable insights Generate weekly, monthly and quarterly data reports for internal and external business reviews DIGITAL MARKETING Drive testing, reporting, analysis and optimization on paid media search, display, OTT and co-op marketing to identify key levers and opportunities to grow Luxottica’s online wholesale business Oversee and manage the e Commerce marketing and co-op budget; evaluating performance, reviewing and monitoring spend, and reallocating according based on performance and ROI Analyze impact sales, incrementality, halo of promotional activities with online / omni-channel retailers. Assist Sr Manager, e Commerce Strategy in planning, set up, management, and reporting on performance marketing campaigns, in conjunction with agency and Amazon Advertising Work cross-functionally with the Brand, Product and Sales teams to consistently stay apprised of product lifecycles and inventory levels and that will better inform our paid search and media strategy STRATEGY Identify content and creative hooks for e Commerce partner sites based on searches and reviews Support in managing and crafting the strategy for performance marketing with wholesale partners Develop analytical frameworks/models for assessing strategic initiatives e. g. , ROI, incremental growth Partner with cross-functional teams to identify growth opportunities and strategic initiatives to support the e Commerce and Department Stores channel BASIC QUALIFICATIONS Bachelor's Degree in Business, Marketing, Economics or relevant experience 3+ years of experience working in e-commerce strategy, business intelligence, or digital marketing/analytics Strong analytical skills with proficiency in Excel Highly organized, detail oriented, and marketing savvy thinker capable or boiling down data and building compelling stories PREFERRED QUALIFICATIONS Experience working in digital marketing, e Commerce CPG, retail or e Commerce experience Knowledge of marketplace management Amazon, Walmart, etc. Upon request and consistent with applicable laws, Luxottica will provide reasonable accommodations to individuals with disabilities who need assistance in the application and hiring process. To request a reasonable accommodation, please call the Luxottica Ethics Compliance Hotline at 1-888-887-3348 or e-mail HR Compliance@luxotticaretail. com be sure to provide your name and contact information for either option so that we may follow up in a timely manner . We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, gender, national origin, social origin, social condition, being perceived as a victim of domestic violence, sexual aggression or stalking, religion, age, disability, sexual orientation, gender identity or expression, citizenship, ancestry, veteran or military status, marital status, pregnancy including unlawful discrimination on the basis of a legally protected pregnancy or maternity leave , genetic information or any other characteristics protected by law. Native Americans receive preference in accordance with Tribal Law. "
146,Associate Research Scientist,research scientist,/rc/clk?jk=e0a0727e5889118d&fccid=bd976cc171c690e0&vjs=3,"The Department of Chemistry at Columbia University is looking to fill the position of Associate Research Scientist with a Ph D in Chemistry and 2-3 years of postdoctoral experience in the area of synthesis and pharmacology of complex alkaloids. The individual in this position will run a program focused on design, synthesis, and pharmacological characterization of psychoactive alkaloids as probes and leads for novel therapeutics in the CNS area. She/he will lead the synthetic work, molecular design, and pharmacological characterization of new compounds. The responsibilities also include design and interpretation of in vitro and in vivo pharmacological studies, ADME and metabolism studies, coordination of a complex web of collaborations with academic labs and contract research organizations, management of multiple projects, training and oversight of graduate students and postdocs in the laboratory, preparation and submission of grants and related fund-raising activities, and preparation of publications and patents. Minimum Degree Required Ph. D Minimum Qualifications This position requires a Ph. D. degree in chemistry and 2-3 year postdoctoral experience in the area of synthesis and pharmacology of complex alkaloids. Preferred Qualifications Experience in writing scientific articles and grants is highly desirable. Additional Information RAPS posting date 08/05/2020 Search Closing Date Special Instructions to the Applicant All applications, including cover letter, CV, and a list with names of three references, must be submitted through Columbia’s online system. Proposed Start Date 11/01/2020 EEO Statement Columbia University is an Equal Opportunity Employer / Disability / Veteran Review Begins 08/05/2020"
147,"Research Scientist 6, Part-Time, New York State Psychiatric Institute, P20358",research scientist,/rc/clk?jk=3a445807c386d167&fccid=d08a2ec9b0ee85bf&vjs=3,"Doctorate Degree and four years of professional research experience in the appropriate field. PREFERRED QUALIFICATIONS The ideal candidate will have a doctorate in neuroscience or related biological field and at least 4 years of post-degree experience in molecular or developmental neuroscience. Post-doctoral research training in molecular, behavioral, or circuit-based techniques used to dissect the underpinnings of neurological, cognitive, or behavioral dysfunction using rodent models is also preferred. The ideal candidate should demonstrate a history of relevant peer-reviewed publications, including first-author publications; possess strong writing and communication skills; and demonstrate leadership skills in the laboratory setting, including track record of supervising or mentoring research assistants, undergraduate students, graduate students, or postdoctoral research fellows. Duties Description This research scientist will work in the Division of Developmental Neuroscience. An improved understanding of neurobiology is necessary for the delivery of new treatment approaches. The Division has a commitment to using rodent models to understand the molecular, developmental, and circuitry consequences of risk factors for mental illness. This research scientist will focus on studying the neurobiology of stress on the developing nervous system. In this role, the scientist will be serving the core mission of the agency to discover how adverse experiences encountered early in life alter the trajectory of neural and behavioral development, sex differences in sensitivity to those signals, and the genetic mechanisms driving those changes. They will also serve as the new Research Director of the Rodent Behavioral Core, developing, implementing, and revising as necessary, a strategic plan for rodent behavioral testing for the entire Department of Psychiatry. Additionally, the incumbent will provide technical assistance to researchers on the design of experimental procedures for rodent behavioral testing as well as provide guidance on the implementation and troubleshoot any issues that arise during the procedures. Additional Comments Background Investigation/Justice Center Review Prospective appointees will be 1 Checked against the Staff Exclusion List SEL maintained by the Justice Center for the Protection of People with Special Needs. Prospective employees whose names appear on the SEL as having been found responsible for serious or repeated acts of abuse or neglect will be barred from appointment and may have their names removed from the eligible list s for the title s if applicable. 2 Investigated through a Criminal Background Check CBC which includes State and federal Criminal History Record Checks. All convictions must be reported; conviction of a felony or misdemeanor, or any falsified or omitted information on the prospective appointee’s employment application may bar appointment or result in removal after appointment. Each case will be determined on its own merits, consistent with the applicable provisions of State and federal laws, rules, and regulations. Prospective employees will be fingerprinted to obtain a record of their criminal history information and may be required to pay any necessary fees. 3 Screened against the Statewide Central Register of Child Abuse and Maltreatment SCR . Prospective employees may be required to pay any necessary fees. Additionally, prospective employees whose names are indicated on the SCR may be barred from appointment. Additional Comments Salary reflects 75% effort. The work schedule is to be determined. "
148,Research Scientist,research scientist,/rc/clk?jk=5515a8df190566e9&fccid=fe2d21eef233e94a&vjs=3,"Ph D in a quantitative field such as Mathematics, Statistics, Physics, Engineering, Computer Science, Economics and 5+ years of industry experience OR MS or greater in a quantitative field and 8+ years of experience Experience with forecasting such as Time-Series ARIMA, Neural Network LSTM, CNN and statistics in a production environment Experience with linear Optimization model to make big-impact decision making The Amazon Topology team determines how many, what kind, and where to place new buildings for Amazon's supply chain. These facilities range from million-square-foot Fulfillment Centers with thousands of robots down to inner-city Prime Now facilities serving orders to be fulfilled within an hour. Each year we spend billions on these facilities and expect high-impact results from the network built upon them. If you are interested in diving into a multi-discipline, high impact space this team is for you. In addition to build decisions, we also use forecasting techniques for long term estimates, optimization, probability in decision making, machine learning to approximate the network, and simulation of how our choices will perform. The team is a mixture of Software Engineers, Operations Research Scientists, Applied Scientists, Business Intelligence Engineers and Product Managers. We are looking for an Applied Scientist who has a deep knowledge of either forecasting or optimization. Those who are strong in forecasting space should have a breadth of other ML experience in a production environment using techniques such as classic time-series ARIMA, Regression, and/or LSTM long-short term Memory neural network. This role will focus on improving our forecasts and build recommendations for Amazon's specialty networks. To help describe some of our challenges, we created a short video about Supply Chain Optimization at Amazon http //bit. ly/amazon-scot Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age Experience with fully automated machine training e. g. automatic re-training, automatic testing on techniques such as Random Forest, Regression, Time-Series and Neural network. Experience building optimization model using XPRESS/Mosel, Gurobi or CPLEX Experience with high-impact decisions > $50M Experience with machine learning in a service oriented architecture Experience with forecasting in production Experience with economic ROI analysis Experience writing production-quality code"
149,AI Research Scientist - Vice President,research scientist,/rc/clk?jk=24c32852a316018c&fccid=aaf3b433897ea465&vjs=3,"AI Research Scientist Vice President The goal of J. P. Morgan AI Research is to explore and advance cutting-edge research in AI, including ML as well as related fields like Cryptography, to develop and discover principles of impact to J. P. Morgan's clients and businesses. J. P. Morgan AI Research has assembled a team of experts in many fields of AI. They pursue primary research in areas related to our research pillars as well as concrete problems related to financial services. They partner with internal teams to accelerate the adoption of AI within the firm. They also work with leading academic faculty around the world on areas of mutual interest. The team is headquartered in New York and present in London and the Bay Area. Conducting AI research in financial services offers unique and exciting opportunities for impact as a member of this highly visible team, you will have the opportunity to realize significant impact not only within J. P. Morgan but also to the broader AI community. Responsibilities As a Research Scientist on the team, you will lead end-to-end research typically within a specialized focus area. You will work on multiple research projects in collaboration with internal and external researchers and with applied engineering teams. You will be integral to all aspects of the research lifecycle such as formulating problems, gathering data, generating hypotheses, developing models and algorithms, conducting experiments, synthesizing results, building prototype applications and communicating the significance of your research. Your output will result in high-impact business applications, open source software, patents and/or publications in AI/ML conferences and journals. You will work with senior leaders to help define, build, and transform our businesses. As a member of the AI research community, you will also have the opportunity to participate in relevant top-tier academic conferences to broaden the impact of your contributions. Preferred Qualifications Ph D in Computer Science especially AI/ML or related fields Research publications in prominent AI/ML venues; e. g. , conferences, journals Strong expertise in one or more specialized areas; e. g. , deep learning DL , reinforcement learning RL , planning, information representation and retrieval, graphs, multiagent systems MAS , natural language processing NLP Practical experience with ML platforms such as Tensorflow/Keras, Py Torch, etc. Comfort with rapid prototyping and disciplined software development processes Practical software engineering experience in collaborative project settings Minimum Requirements Masters degree with 5+ years of relevant work experience or Ph D with 3+ years of relevant work experience in Computer Science, Statistics, Engineering or related fields Extensive programming skills in Python, Java or C++Proficient understanding of fundamental AI and ML techniques; e. g. , A*, regularization Practical experience with statistical data analysis and experimental design Professional Skills Curiosity, creativity, resourcefulness and a collaborative spirit Effective verbal and written communication skills with technical and business audiences Demonstrated ability to work on multi-disciplinary teams with diverse backgrounds Interest in problems related to the financial services domain specific past experience in the domain is not required JP Morgan Chase & Co. , one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J. P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs. Equal Opportunity Employer/Disability/Veterans"
150,"Assistant Research Scientist, Museum Technology",research scientist,/rc/clk?jk=658ddd216653cf16&fccid=8077183a161ef0fd&vjs=3,"New York University NYU Domestic Steinhardt School of Culture, Education and Human Development Office of Research Researchers_OT Location New York, NY Open Date Aug 5, 2020 Description Ability Project Institute of Museum and Library Services IMLS Sensory Tools Grant Successful applicant will advise students in technology development, conduct evaluations of accessible museum technologies, and engage in participatory design with disabled people, caregivers, and accessibility experts. Qualifications Experience. Advanced training in digital accessibility, human-computer interaction, and museum studies. Required Experience in developing, evaluating, and designing assistive technology. Education. MS Degree in Interactive Art, Museum Studies, or related field. Application Instructions Submit a cover letter and resume as soon as possible. "
151,Junior Research Scientist,research scientist,/rc/clk?jk=ae78e8d27c34c4ff&fccid=8077183a161ef0fd&vjs=3,"New York University NYU Domestic Steinhardt School of Culture, Education and Human Development Office of Research Institute of Human Development and Social Change Location New York, NY Open Date Aug 4, 2020 Description The Development & Research in Early Math Education DREME Network, funded by the Heising-Simons Foundation, is a cross-university collaboration that seeks to advance the field of early mathematics research and improve young children’s opportunities to develop math skills from birth through age eight years, with an emphasis on the preschool years. In spring 2018, Dr. Gigliana Melzi joined DREME as part of Family Math, the group targeting professionals’ ability to support family math engagement. The project seeks a full-time Junior Research Scientist to assist with various research activities including cleaning and managing data, coordinating coding and analysis efforts, submitting protocols to NYU’s IRB, assisting with literature reviews, and report writing. The Junior Research Scientist will also assist in the creation of bilingual resources with local NYC partners, and provide direct support to NYU’s active collaboration with Teacher’s College in the adaptation and expansion of video resources for family-facing professionals and families from Spanish-speaking and Spanish-English bilingual backgrounds. This individual will also provide also support related to the Family Math Website. Qualifications BA, BS or comparable degree in social sciences, preferably applied psychology or early childhood development. One year prior experience working in research settings. Basic knowledge and comfort with statistics as used in applied social science research. Candidate must have strong oral and written skills in Spanish and English. Ability to troubleshoot and problem-solve. Flexible and able to juggle competing priorities. Ability to interact effectively and diplomatically with colleagues across all levels of the organization. Application Instructions The review of applicants will begin immediately. For full consideration, please submit a cover letter and resume via Interfolio as soon as possible. "
152,"Director, Epidemiology- Safety Surveillance Research Scientist",research scientist,/rc/clk?jk=4fc1b06e9bebdaac&fccid=5e118f74384e090a&vjs=3,"At Pfizer, you can truly make progress happen through your work. Ours is a uniquely collaborative global community. It will see you work, learn, and innovate with the first-class talent around you, and around the world. We are excited to present an opening for a highly visible Director, Epidemiology- Safety Surveillance Research Scientist position within our Safety organization. Role Summary The Director, Epidemiology- Safety Surveillance Research Scientist develops and executes post-approval pharmacoepidemiology/real world evidence research strategies to assess potential safety risks and investigate new safety signals, assess effectiveness of risk management, ensure compliance with global regulatory commitments, and inform benefit-risk decisions about Pfizer medicines. Role Responsibilities Primary responsibilities include Design and implement database, de novo , or hybrid design studies intended to quantify risks potentially associated with Pfizer products or to assess the effectiveness of risk mitigation activities Independently serve as safety research lead of projects with moderate regulatory/methodologic complexity e. g. , lead and provide recommendations to management re moderately complex communications with internal and external stakeholders about study design/interpretation Design and implement other epidemiology strategies to investigate safety signals arising post-approval, such as critical review of publication on Pfizer products and real world data queries Interact with regulatory agencies on safety epidemiologic issues as needed, via written and/or verbal communications Consult on safety-related issues in pre-approval research strategies, such as defining safety endpoints in standing cohorts Participate in Risk Management Committee activities related to post-approval epidemiology strategies as needed Oversee contributions to epidemiology sections of risk management plans Oversee vendor activities, as applicable Influence the external environment regarding best practices for safety epidemiological studies using real world evidence e. g. , scientific conferences and peer-reviewed journals Work collaboratively with key stakeholders internally such as Safety Risk Leads, RM Co E Leads, GME, Clinical, Medical, Regulatory and externally such as academics, regulators, vendors Mentor or support other SSR colleagues as required Consult on design of key post-approval safety studies required by single-country health authorities outside the US and EU May be responsible for negotiating and overseeing observational study budgets Participates in cross-functional internal and/or external working groups Basic Qualifications Doctoral degree in Epidemiology/Quantitative Public Health discipline with 4 years or more experience in the pharmaceutical industry. Significant experience applying epidemiologic methods to study safety required In-depth understanding of and expertise in epidemiologic methods, including observational and experimental study designs and analysis, and appropriate sample size calculation methodology Experience writing epidemiological sections of scientific documents such as research summaries, publications, grant proposals, risk management plans, etc. Practical experience with implementation of observational or experimental studies Demonstrated ability to negotiate scientific and operational decisions with cross-functional teams, external collaborators; supports regulatory interactions Experience participating in internal or external strategic initiatives related to safety epidemiology Candidate demonstrates a breadth of diverse leadership experiences and capabilities including the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact. Preferred Qualifications Five or more years experience in the pharmaceutical industry, including experience in regulatory interactions regarding safety research Other Job Details Last Date to Apply for Job August 18, 2020 Additional Location Information Preference for role to be based in New York City, NY but also open to Peapack, NJ, Collegeville, PA or Groton, CT. Eligible for Employee Referral Bonus #LI-PFE Sunshine Act Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative. EEO & Employment Eligibility Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer. Medical"
153,"Senior Director, Epidemiology- Safety Surveillance Research Scientist",research scientist,/rc/clk?jk=3bf98c6736070546&fccid=5e118f74384e090a&vjs=3,"At Pfizer, you can truly make progress happen through your work. Ours is a uniquely collaborative global community. It will see you work, learn, and innovate with the first-class talent around you, and around the world. We are excited to present an opening for a highly visible Senior Director, Epidemiology- Safety Surveillance Research Scientist position within our Safety organization. Role Summary The Senior Director, Epidemiology- Safety Surveillance Research Scientist develops and executes post-approval pharmacoepidemiology/real world evidence research strategies to assess potential safety risks and investigate new safety signals, assess effectiveness of risk management, ensure compliance with global regulatory commitments, and inform benefit-risk decisions about Pfizer medicines. Role Responsibilities Primary responsibilities include Using innovative approaches, design and implement database, de novo , or hybrid design studies intended to evaluate risks potentially associated with Pfizer products or to assess the effectiveness of risk mitigation activities Design and implement other epidemiology strategies to investigate safety signals arising post-approval, such as critical review of publication on Pfizer products and real world data queries Independently lead projects with high regulatory/methodologic complexity e. g. , lead and provide recommendations to management re highly complex communications with internal and external stakeholders about study design/interpretation Consult on safety-related issues in pre-approval research strategies, such as defining safety endpoints in standing cohorts Participate in Risk Management Committee activities related to post-approval epidemiology strategies as needed Oversee contributions to epidemiology sections of risk management plans Oversee vendor activities, as applicable Influence the external environment regarding best practices for safety epidemiological studies using real world evidence e. g. , scientific conferences and peer-reviewed journals Work collaboratively with key stakeholders internally such as Safety Risk Leads, RM Co E Leads, GME, Clinical, Medical, Regulatory and externally such as academics, regulators, vendors Mentor or support other SSR colleagues as required Consult on design of key post-approval safety studies required by single-country health authorities outside the US and EU May be responsible for negotiating and overseeing observational study budgets Lead/chair cross-functional internal and/or external working groups related to safety epidemiology Basic Qualifications Doctoral degree in Epidemiology/Quantitative Public Health discipline with 7 years or more experience in the pharmaceutical industry. Significant experience applying epidemiologic methods to study safety required. In-depth understanding of and expertise in epidemiologic methods, including observational and experimental study designs and analysis, and appropriate sample size calculation methodology Experience writing epidemiological sections of scientific documents such as research summaries, publications, grant proposals, risk management plans, etc. Practical experience with implementation of observational or experimental studies Demonstrated ability to lead scientific and operational decision-making with cross-functional teams, external collaborators; supports regulatory interactions Experience leading highly visible internal or external strategic initiatives related to safety epidemiology Candidate demonstrates a breadth of diverse leadership experiences and capabilities including the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact Preferred Qualifications Eight or more years experience in the pharmaceutical industry, including experience in regulatory interactions regarding safety research Other Job Details Last Date to Apply for Job August 18, 2020 Additional Location Information Preference for role to be based in New York City, NY but also open to Peapack, NJ, Collegeville, PA or Groton, CT. Eligible for Employee Referral Bonus #LI-PFE Sunshine Act Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative. EEO & Employment Eligibility Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer. Medical"
154,"City Research Scientist, Level IV A",research scientist,/rc/clk?jk=9e3afc6b09c73a92&fccid=7d71a6e7a63e35a0&vjs=3,"The mission of the New York City Police Department is to enhance the quality of life in New York City by working in partnership with the community to enforce the law preserve, preserve peace, protect the people, reduce fear and maintain order. The NYPD strives to foster a safe and fair city by incorporating Neighborhood Policing into all facets of Department operations, and solve the problems that create crime and disorder through an interdependent relationship between the people and its police, and by pioneering strategic innovation. The Office of Chief of Crime Control Strategies analyzes and monitors trends across the city and develops strategies to reduce Crime. The Office of the Chief of Crime Control Strategies is seeking a qualified City Research Scientist, Level IV A to assist with the planning and development of research methods used to obtain data and assist the Chief of Crime Control in analyzing and interpreting results of large crime data sets. The City Research Scientist, Level IV A will Plan, develop, install and be responsible for research activities in a major field of physical, biological, environmental or social research; Plan and direct research in an assigned field of inquiry designed to add to existing knowledge in the physical, biological, environmental or social sciences as related to the mission and activities of the agency Provide leadership and general direction to agency personnel in planning, conducting, and coordinating specific research projects Select broad scientific approaches to the research problems involved; Approve, modify, or disapprove studies recommended by subordinate employees Plan and carry out activities to develop effective coordination between the research efforts of the agency and other research projects Participate in conferences, meetings, seminars, etc. , on the planning and development of sound research projects; Write technical articles or books Minimum Qual Requirements 1. For Assignment Level I only physical, biological and environmental sciences and public health A master's degree from an accredited college or university with a specialization in an appropriate field of physical, biological or environmental science or in public health. To be appointed to Assignment Level II and above, candidates must have 1. A doctorate degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and one year of full-time experience in a responsible supervisory, administrative or research capacity in the appropriate field of specialization; or 2. A master's degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and three years of responsible full-time research experience in the appropriate field of specialization; or 3. Education and/or experience which is equivalent to ""1"" or ""2"" above. However, all candidates must have at least a master's degree in an appropriate field of specialization and at least two years of experience described in ""2"" above. Two years as a City Research Scientist Level I can be substituted for the experience required in ""1"" and ""2"" above. NOTE Probationary Period Appointments to this position are subject to a minimum probationary period of one year. Additional Information In compliance with Federal Law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. The City of New York offers a comprehensive benefits package including health insurance for the employee and his or her spouse or domestic partner and unemancipated children under age 26, union benefits such as dental and vision coverage, paid annual leave and sick leave, paid holidays, a pension, and optional savings and pre-tax programs such as Deferred Compensation, IRA, and a flexible spending account. To Apply Please Click on ""Apply Now"" Work Location 1 Police Plaza, N. Y. Residency Requirement New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview. "
155,"Research Scientist 3 (Epidemiology) – 07087, 10129",research scientist,/rc/clk?jk=010374001a442962&fccid=f99dd454b90ff309&vjs=3,"Research Scientist 3 RS3 Bachelor’s degree in Biology, Public Health, Epidemiology, Infection Control, Statistics/Biostatistics, Biomedical Science, or a related degree and four years of professional post-degree research experience in that field; or, a Master’s degree in one of the above fields and three years of such experience; or a Doctorate in one of the above fields and one year of such experience. Excellent organizational and written and oral communication skills, which includes both public speaking and preparation of written materials such as reports, abstracts, and publications. Familiarity with surveillance and outbreak response activities and/or infection control experience. Lead or co-author of scientific publications. Experience supervising professional level staff. Advanced knowledge of SAS, and experience using computer-based software packages, such as Epi Info, GIS, Microsoft Access, Word, Excel, etc. Excellent interpersonal skills and the ability to effectively work as part of a team and interact with individuals across all levels of an organization. Duties Description The Research Scientist 3 will perform a broad range of essential epidemiological & research activities & provide essential subject matter expertise to respond to existing public health threats such as 2019 Novel Coronavirus, disease surveillance, research, control & prevention within the Metropolitan Area Regional Office. The incumbent will play a key role in the surveillance & prevention of over 50 reportable communicable diseases & provide epidemiological consultation & technical guidance in the reporting, investigation, diagnosis, prevention & control of communicable diseases to a range of health care professionals including physicians & nurses, local health department staff & state agency representatives. In support of the ongoing threat of coronavirus to New Yorkers, this position is essential to ensure effective & timely disease investigations, response & control, & to protect the health & safety of NYS residents. Additional Comments Permanent, full-time, non-competitive appointment. Routine travel, including occasional overnight travel and travel to areas which may not be served by public transportation, is required. ~20% "
156,Research Scientist,research scientist,/rc/clk?jk=a3c805ed35cc8d5e&fccid=8de1633f2f8eae1d&vjs=3,"Job ID 20000DK4 Available Openings 1 Position Specific Information Research Scientist, Renal Research Institute, LLC, a Fresenius Medical Care N. A. company, New York, NY Purpose and Scope Responsible for assisting with the planning, design, and execution of research studies and producing high-quality and accurate research reports and publications in the fields of renal and kidney disease/failure and nutrition. Principal Duties and Responsibilities Planning, collaborating, and conducting research and development activities for all phases of human subject research. Collecting, storing, and maintaining research data. Preparing abstracts, conference posters/talks, and manuscripts for publication in peer-reviewed scientific journals as required. Organizing technical reports, summaries, presentations and quantitative analyses for presentation to appropriate personnel, externally and internally. Preparing and submitting regulatory documents to appropriate board or agency, including the Institutional Review Board IRB . Designing protocols and human research subject Informed Consent procedure development. Investigating, creating and developing new methods and technologies for project advancement. Preparing sample analysis and/or sample preparation for analysis /shipping. Assists with data analysis and explores techniques involved in biostatistical analytics. Presenting research findings at scientific conferences and meetings. Other duties as assigned. Education Experience and Required Skills Position requires a Master’s degree or equivalent foreign degree in Biology, Chemistry, Nutrition, Physiology, Biomedical or Health Sciences or a closely related life sciences field and 1 year of experience as a Research Assistant. Experience which may be gained concurrently with the primary experience requirement above must include Planning and conducting research and development activities for human subject research, including screening, consenting and enrolling study subjects; conducting study visits; conducting training to study sites; assisting in study monitoring and auditing; and organizing study data and analysis. Working knowledge of basis biostatistical techniques distribution analysis, t-test, ANOVA and linear regression; analyzing and visualizing study data using statistical software python and SAS. Presenting research findings to an audience of scientists or healthcare professionals. Training in Good Clinical Practice GCP International Council for Harmonisation ICH E6 guideline and FDA guidelines Title 21 of the Code of Federal Regulations 21 CFR . "
157,Research Scientist – Content Platform,research scientist,/rc/clk?jk=45f36cab4ef4265f&fccid=fe404d18bb9eef1e&vjs=3,"Spotify is looking for Research Scientists to join our Content Platform Research organization. Spotify’s mission is to unlock the potential of human creativity—by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it. Content Platform is a central enabler for Spotify; together, our teams ensure that Spotify has a complete, available and enriched catalog of music, podcasts, videos and more. We provide the insights that enable a differentiated understanding of artists and their content, attributes, and relationships. We ensure that we can effectively moderate and control our catalog to provide a safe and trusted experience for consumers and artists, and to keep our system free of infringement. You will be part of an interdisciplinary team passionate about ensuring that the foundations of Spotify technologies are state of the art. Our team has strong internal ties to product groups as well as ties to the wider research community by publishing papers, attending conferences, and engaging with academic partners in which you will also contribute. What you’ll do You will participate in cutting-edge research in one of Statistics, Machine Learning, Applied Mathematics, Information Retrieval, or a similar field. You will apply your scientific skills to identify problems, generate hypotheses, and conduct experiments and analyses to validate your hypotheses. You will also help to develop standard methodologies for conducting scientific research in the organization. You will work on practical applications towards understanding and controlling Spotify’s catalog, for example, entity resolution or content classification. You will work in collaboration with other scientists, engineers, product managers, designers, user researchers, and analysts across Spotify to craft creative solutions based on scientific principles to ambitious problems. You will have direct impact on the products, tools and services that Content Platform builds. External engagement such as publishing, giving talks, and attending top conferences is encouraged. Who you are You have a Ph D or comparable experience in either statistics, applied mathematics, machine learning, computer science, engineering, or related areas. You have a solid understanding of statistics, data modelling, machine learning, optimization techniques, or information retrieval. You possess proven hands-on skills in sourcing, cleaning, manipulating, analyzing, visualizing and modeling of real data. You are a creative problem-solver who is passionate about digging into sophisticated problems and devising new approaches to reach results. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 271 million users. "
158,Research Scientist,research scientist,/rc/clk?jk=f35300e657198d44&fccid=63b138dee345c111&vjs=3,"The Division of Experimental Therapeutics is seeking two computationally oriented Research Scientists to join the Dynamic Social Cognition Lab DSC Lab, pateldsclab. net for full-time positions. RFMH is housed within the New York State Psychiatric Institute adjacent to the Columbia University Irving Medical Center and represents a collaboration between all three institutions that has long been a leader in psychiatric research. The DSC Lab is focused on using a combination of naturalistic stimuli such as movies , eye-tracking, and high-precision neuroimaging methods to study social cognition deficits in schizophrenia and other psychiatric disorders. Two new R01-funded projects are starting, one using novel retinotopic mapping methods to study social cognition deficits and another to examine these deficits in individuals at high-risk for developing schizophrenia. Duties/Responsibilities Depending on interests and expertise, your role may involve face-to-face contact with clinical populations, acquisition of f MRI data computer vision methods to analyze eye-tracking data, and/or the use of circuit based computational models to analyze various modalities of f MRI data. You will also have the opportunity to work with our collaborators Clay Curtis at New York University and David Leopold at the National Institute of Mental Health along with other investigators here at RFMH, New York State Psychiatric Institute, and Columbia University Irving Medical Center such as Daniel Javitt, Guillermo Horga, Vincent Ferrera, and Kevin Ochsner. Minimum Qualifications Doctoral degree in a relevant field such as neuroscience, psychology, computer science, or medicine Strong quantitative skills Scripting experience in MATLAB, Python, and/or shell Background in relevant statistical regression, permutation testing and neuroimaging analysis methods graph theory, clustering techniques Experience with behavioral and neuroimaging data acquisition and processing Preferred Qualifications Experience with eye-tracking Interest in neural mechanisms of visuospatial and social cognition as well as clinical psychopathology Desirable experience includes use of machine learning/AI methods for neuroimaging and behavioral data analysis and/or visual feature extraction from movies To apply Submit an application with cover letter and resume through our website https //nyspi. applicantpro. com/jobs/. Only applications submitted through the RFMH website will be accepted. The Research Foundation is a private not-for-profit corporation and is not an agency or instrumentality of the State of New York. Employees of the Research Foundation are not state employees, do not participate in any state retirement system, and do not receive state fringe benefits. Excellent Benefits Package. Employer/Minority/Women/Disabled/Veteran Employer. VEVRAA 41 CFR 60-300. 5 a compliant. "
159,Assistant Research Scientist,research scientist,/rc/clk?jk=af046c20c8ddc9b1&fccid=848e72c84ce4a7a7&vjs=3,"NYU Grossman School of Medicine is one of the nation's top-ranked medical schools. For 175 years, NYU Grossman School of Medicine has trained thousands of physicians and scientists who have helped to shape the course of medical history and enrich the lives of countless people. An integral part of NYU Langone Health, the Grossman School of Medicine at its core is committed to improving the human condition through medical education, scientific research, and direct patient care. For more information, go to med. nyu. edu, and interact with us on Facebook, Twitter and Instagram. Position Summary We have an exciting opportunity to join our team as a Asst Research Scientist. Responsible for collaborating with Principal Investigators PI and working independently within the scientific framework of the PI's laboratory. Job Responsibilities Initiates, interprets, organizes, executes, and coordinates research assignments critical to department's mission. Formulates and conducts research on problems of considerable scope and complexity. Explores subject area and defines scope and selection of problems for investigation through conceptually related studies or series of projects of lesser scope. Makes decisions and recommendations that have a major impact on extensive scientific research activities. Develops new ideas that promote current research. Exercises a high degree of creativity, foresight, and mature judgment in planning, organizing, and guiding extensive scientific research programs and activities of outstanding novelty and/or importance. May manage the operations of a lab for the Principal Investigator by supervising Post Docs and technicians. Oversees the maintenance and upkeep of all instruments and laboratory facilities. Ensures timely and accurate completion of research projects. May provide students with technical guidance and direction in the operation of various sample preparation techniques, extraction lines, and equipment as well as general laboratory practices and safety protocols. May seek grant funding from external sources in support of their independent research projects with consent of the PI and the Department Chair. May participate in writing grant proposals. May be responsible research/lab quality control and compliance. May develop protocol for the collection and quality of research data. Develops research techniques and methodologies. May determine and establish laboratory policies, procedures and practices. Maintains a thorough knowledge of the advances in specialized fields through attendance of scientific conferences. May make significant contributions to scientific papers. Is a resource/ author/ co-author for reports and presentations. May provide recommendations for equipment purchases and for the redesign of lab space. May coordinate the activity and experimental design between various labs, scientific staff, and collaborators. Identifies and resolves technical problems. Minimum Qualifications To qualify you must have a MS degree in a related discipline and 0-1 year of experience. Qualified candidates must be able to effectively communicate with all levels of the organization. NYU Grossman School of Medicine provides its staff with far more than just a place to work. Rather, we are an institution you can be proud of, an institution where you'll feel good about devoting your time and your talents. NYU Grossman School of Medicine is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sex, sexual orientation, transgender status, gender dysphoria, national origin, age, religion, disability, military and veteran status, marital or parental status, citizenship status, genetic information or any other factor which cannot lawfully be used as a basis for an employment decision. We require applications to be completed online. If you wish to view NYU Grossman School of Medicine's EEO policies, please click here. Please click here to view the Federal ""EEO is the law"" poster or visit https //www. dol. gov/ofccp/regs/compliance/posters/ofccpost. htm for more information. To view the Pay Transparency Notice, please click here. Required Skills Required Experience"
160,Assistant Research Scientist,research scientist,/pagead/clk?mo=r&ad=-6NYlbfkN0BgHAp2-YRN2xG75wB11geajKffmlHLOh1r6ymGvyUjYvCw2GMQZv-vizpTkCPBbK9hJsjLNdOMNo4spwlj-lVoMvrvnssrbRzaqnJU4x5Sm8eM-D0B9EpfDrWgFOWKnVSEjEaCgyGw1dgV2XUvKUy64I5e7hfF69roK6XLVe_DzmuEiB-DhiCu2MYqIffPNsNhQRn5hWqOUiqe90zc6jcDNGQWG5rf5diuhs9L35d0LsK8yX3OjxEs4GyCb_m08Wy5dtGNgQR1tEj7DVz8enIECuLsK01-XlXCS1IIOeMSy1KM68ItPDtGvEgYss8YPi-ypad55LHKDvKnh-vCjBbxqpSJuiDt0_lOhmRHQvfR8jybpGhYbfAHT6hLg_oWLYxnmNVtCC_hRnPFwsnjmRWY-SQqkUm3jGkslcjQVpfEFzkykQR8O40hFy2dyx9jx_NCPw5sy9q12lZazN4B654bmdTvf3RjvOxVCEZCtUYC9k8dNJXju7s3ujTDd6Ya8xeaM5WUcjHCiyXER0HS4T-Tvm1uYiEA339xDANf9tvht-1Tco44CeLkCd7UEfefJtWd9X3ZPKq_1kIRVdeCVIFbKvP_MKH6ywQ7ODGOD-wPbIw-RwgWExRsaiFa5BJMdTq4vDpDYFOoX_ohPmpoFWPT503OZqghOVES2fbaXfA06Oq1bnQR7eIgz4N3OtPqK3OmMJZZTLBQ1-0ZS7sG4wYUb9GAn7aD9YOSW8DqOx1AlhaOOk0ZHkzRV8pHji6SsvyhXV7pVrzrI_F_y1wDxVIW3y5RyorVhwRBQhA1N6plQFuz5HbhoG9e8kCpcGcwlJTzvfqTDqQMxVvl3K7fyZVSAtaAufu7LI35kCP97eTviRE1yVB4ApbKf12EmUrBSUorssjXBfDXT9uF63ra1DPb1h67eTAKhJMySurtEMmLQLUTmmUZNbRLrqgpgSAiDfzIfIu_FX2lDPJPCNVwDE_KJznp17GBPGr9Vk2oijo6ASJtWQxl6QHZKOIbQPADK1jCRfv0IctK3V9kK11FZsQ7Ng43bWF-ERhUrGUBK3Gvthx3SHVSGIMftdibnKkeN7tY60AmwrW0AxG12QIpk-4Z&p=13&fvj=0&vjs=3,"NYU Grossman School of Medicine is one of the nation's top-ranked medical schools. For 175 years, NYU Grossman School of Medicine has trained thousands of physicians and scientists who have helped to shape the course of medical history and enrich the lives of countless people. An integral part of NYU Langone Health, the Grossman School of Medicine at its core is committed to improving the human condition through medical education, scientific research, and direct patient care. For more information, go to med. nyu. edu, and interact with us on Facebook, Twitter and Instagram. Position Summary We have an exciting opportunity to join our team as a Asst Research Scientist. Responsible for collaborating with Principal Investigators PI and working independently within the scientific framework of the PI's laboratory. Job Responsibilities Initiates, interprets, organizes, executes, and coordinates research assignments critical to department's mission. Formulates and conducts research on problems of considerable scope and complexity. Explores subject area and defines scope and selection of problems for investigation through conceptually related studies or series of projects of lesser scope. Makes decisions and recommendations that have a major impact on extensive scientific research activities. Develops new ideas that promote current research. Exercises a high degree of creativity, foresight, and mature judgment in planning, organizing, and guiding extensive scientific research programs and activities of outstanding novelty and/or importance. May manage the operations of a lab for the Principal Investigator by supervising Post Docs and technicians. Oversees the maintenance and upkeep of all instruments and laboratory facilities. Ensures timely and accurate completion of research projects. May provide students with technical guidance and direction in the operation of various sample preparation techniques, extraction lines, and equipment as well as general laboratory practices and safety protocols. May seek grant funding from external sources in support of their independent research projects with consent of the PI and the Department Chair. May participate in writing grant proposals. May be responsible research/lab quality control and compliance. May develop protocol for the collection and quality of research data. Develops research techniques and methodologies. May determine and establish laboratory policies, procedures and practices. Maintains a thorough knowledge of the advances in specialized fields through attendance of scientific conferences. May make significant contributions to scientific papers. Is a resource/ author/ co-author for reports and presentations. May provide recommendations for equipment purchases and for the redesign of lab space. May coordinate the activity and experimental design between various labs, scientific staff, and collaborators. Identifies and resolves technical problems. Minimum Qualifications To qualify you must have a MS degree in a related discipline and 0-1 year of experience. Qualified candidates must be able to effectively communicate with all levels of the organization. NYU Grossman School of Medicine provides its staff with far more than just a place to work. Rather, we are an institution you can be proud of, an institution where you'll feel good about devoting your time and your talents. NYU Grossman School of Medicine is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sex, sexual orientation, transgender status, gender dysphoria, national origin, age, religion, disability, military and veteran status, marital or parental status, citizenship status, genetic information or any other factor which cannot lawfully be used as a basis for an employment decision. We require applications to be completed online. If you wish to view NYU Grossman School of Medicine's EEO policies, please click here. Please click here to view the Federal ""EEO is the law"" poster or visit https //www. dol. gov/ofccp/regs/compliance/posters/ofccpost. htm for more information. To view the Pay Transparency Notice, please click here. "
161,AI Research Scientist - Senior Associate,research scientist,/rc/clk?jk=75579f6e2f10eef8&fccid=aaf3b433897ea465&vjs=3,"AI Research Scientist Senior Associate The goal of J. P. Morgan AI Research is to explore and advance cutting-edge research in AI, including ML as well as related fields like Cryptography, to develop and discover principles of impact to J. P. Morgan's clients and businesses. J. P. Morgan AI Research has assembled a team of experts in many fields of AI. They pursue primary research in areas related to our research pillars as well as concrete problems related to financial services. They partner with internal teams to accelerate the adoption of AI within the firm. They also work with leading academic faculty around the world on areas of mutual interest. The team is headquartered in New York and present in London and the Bay Area. Conducting AI research in financial services offers unique and exciting opportunities for impact as a member of this highly visible team, you will have the opportunity to realize significant impact not only within J. P. Morgan but also to the broader AI community. Responsibilities As a Research Scientist on the team, you will conduct end-to-end research typically within a specialized focus area. You will work on multiple research projects in collaboration with internal and external researchers and with applied engineering teams. You will be integral to all aspects of the research lifecycle such as formulating problems, gathering data, generating hypotheses, developing models and algorithms, conducting experiments, synthesizing results, building prototype applications and communicating the significance of your research. Your output will result in high-impact business applications, open source software, patents and/or publications in AI/ML conferences and journals. As a member of the AI research community, you will also have the opportunity to participate in relevant top-tier academic conferences to broaden the impact of your contributions. Preferred Qualifications Ph D in Computer Science especially AI/ML or related fields Research publications in prominent AI/ML venues; e. g. , conferences, journals Expertise in one or more specialized areas; e. g. , deep learning DL , reinforcement learning RL , planning, information representation and retrieval, graphs, multiagent systems MAS , natural language processing NLP Practical experience with ML platforms such as Tensorflow/Keras, Py Torch, etc. Comfort with rapid prototyping and disciplined software development processes Practical software engineering experience in collaborative project settings Minimum Requirements Masters degree in Computer Science, Statistics, Engineering or related fields Extensive programming skills in Python, Java or C++Proficient understanding of fundamental AI and ML techniques; e. g. , A*, regularization Practical experience with statistical data analysis and experimental design Professional Skills Curiosity, creativity, resourcefulness and a collaborative spirit Clear and effective verbal and written communication skills Demonstrated ability to work on multi-disciplinary teams with diverse backgrounds Interest in problems related to the financial services domain specific past experience in the domain is not required JP Morgan Chase & Co. , one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J. P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs. Equal Opportunity Employer/Disability/Veterans"
162,Assistant Research Scientist,research scientist,/rc/clk?jk=56305594cafebe26&fccid=8077183a161ef0fd&vjs=3,"New York University NYU Domestic Steinhardt School of Culture, Education and Human Development Office of Research Metropolitan Center for Research and Equity and the Transformation of Schools Location New York Open Date Jul 14, 2020 Description NYU Metropolitan Center for Research CPA is seeking several part-time staff to work as assistant research scientists to support our work with high school students providing STEAM evidenced based instructional support. CPA is funded by various sources and the program services during the Academic and summer components are focused on specific objectives of the grants. Services emphasize culturally responsive education and both in-person and virtual. We are seeking part-time assistant researches in STEAM areas to support our team in the organization and preparation of instructional materials; management of online databases and communication with schools, parents and funding sources. Assist CPA Director with project- related reporting and documentation tasks. Qualifications The ideal candidate is, first and foremost, committed to working with young people and is organized, attentive to detail, creative in developing engage instruction to support all students. Application Instructions Please submit Resume/Cover Letter. "
163,Associate Research Scientist / Biologist,research scientist,/rc/clk?jk=1bd9e9396d07f6df&fccid=9d5b038171bf7047&vjs=3,"Fast-paced, early-stage biotech company is seeking an Associate Research Scientist / Biologist focused on strengthening individual scientific rigor while independently executing on approved wet-lab experimentation primarily related to early stage and lab scale projects. This position is located in Green Island, NY. Ideal candidates will have strong laboratory experience using standard molecular and cellular biology techniques. The applicant will be directly supporting the Functional Biology team focused on analytic testing methods used to characterize our mycelium material at the lab scale. The ability to flex between executing routine analytic tests and working with the team to develop and bring online new techniques and technologies for discovery biology purposes will be key. Additional background or interest in mycology or mushroom cultivation biology is highly desirable. "
164,Senior Principal Research Scientist,research scientist,/rc/clk?jk=cc8cf5424a9f76b1&fccid=e5f32920c98e523d&vjs=3,"VIAVI NASDAQ VIAV has a 90+ year history of technical innovations that have evolved to keep pace and address our customer’s most pressing business issues. We make equipment, software, and systems that help to plan, deploy, certify, monitor, and optimize all kinds of networks like those for mobile phones, service providers, large businesses and data centers. VIAVI is also a leader in high performance thin film optical coatings and engineered diffusers, providing light management solutions to anti-counterfeiting, consumer electronics, automotive, defense and instrumentation markets. We are the people behind the products that help keep the world connected – at home, school, work, at play, and everywhere in between. VIAVI employees are fierce about supporting customer success and we welcome people who bring their best every day to the company to question, to collaborate and to push for solutions that will delight our customers. Senior Optical Scientist/Engineer VIAVI Solutions is seeking a motivated and creative Senior Principal Research Scientist to design and fabricate optical beam-shaping components and novel testing/metrology systems for characterizing optical beam-shaping products. The ideal candidate will have strong knowledge of the fundamentals of physical optics, electromagnetic theory, electromagnetic wave propagation, and diffraction, and has experience designing and building custom optical components and systems. A working knowledge of optical fabrication and photolithography techniques would also be a benefit. The successful candidate will interact closely with the Director of R&D and various other functional leaders within the company as well as with external customers. This position is located in West Henrietta, NY. VIAVI’s facility designs and manufactures light diffuser products for applications in 3D sensing, micro-refractive and diffractive optics for consumer electronics, government, healthcare, and automotive markets. The roles of the Senior Optical Scientist/Engineer will include Customer Interface Develop deep technical understanding of customers’ requirements for illumination optics and other optical beam-shaping applications. Work with customers to develop custom optical components and systems, including new testing and metrology approaches that are acceptable to the customer and accurately characterize the important attributes of products while at the same time being practical and cost-effective. Optical System Specification, Design, Assembly, and Integration Knowledgeable and keeps up to date with current state-of-the-art in optical beam shaping methods, components and systems, especially as it applies to sensor products for the consumer electronics, automotive, medical and defense markets. Demonstrated ability to design and apply novel optical components and systems in the development of new products, particularly in the area of 3D sensing and imaging systems. Create optical metrology and test system specifications that meet customer and company requirements. Translate metrology requirements into practical optical system designs. Source components for designs such as light sources, detectors, imagers, lenses, etc. which may be integrated into partially- or highly- automated systems. Assemble and integrate components into functional optical systems. Test and verify performance of new optical systems. Program Management Internal leader of multi-disciplinary teams for optical component and system development projects. Plan projects and develop schedules and budgets for optical metrology systems. Coordinate resources as needed to ensure completion of projects to schedule and within budget. Regularly communicate progress with internal teams and external customers. Close coordination with Viavi quality team and customers’ quality teams to ensure tools are designed, built, tested, and verified to be in compliance with appropriate Quality Systems. Qualifications Qualifications M. S. or Ph. D. in Optics, Physics, Applied Physics, Electrical Engineering, or equivalent. 3-5 years of experience in Optical Systems Scientist/Engineering role. Experience in the design and develop of novel optical components and systems. Experience developing optical metrology systems for the test and measurement of optical components, including design, component sourcing, assembly, integration, automation, and test. Experience modeling and developing optical systems using optical design software such as Zemax, Code V, ASAP, or Light Tools. Strong problem-solving skills. Strongly motivated and inquisitive individual. Excellent written & verbal communication skills. Computer programming skills, preferably MATLAB. Proficient with MS Office tools – Excel/Office. This position may require use of information which is subject to the International Traffic in Arms Regulations ITAR . All applicants must be U. S. Persons within the meaning of ITAR. ITAR defines a U. S. Person as a U. S. Citizen or National, U. S. Lawful Permanent Resident i. e. 'Green Card Holder' , Political Asylee, Refugee, or Temporary Resident granted amnesty. If you have what it takes to push boundaries and seize opportunities, apply to join our team today. VIAVI Solutions is an equal opportunity and affirmative action employer – minorities/females/veterans/persons with disabilities. Work Locations Rochester, NY USA 330 Clay Road Rochester 14623 Job Research Scientist Schedule Full-time Shift Day Job Job Posting Jul 14, 2020, 11 22 34 AM"
165,Junior Research Scientist (part-time),research scientist,/rc/clk?jk=63b6063467d86e4b&fccid=8077183a161ef0fd&vjs=3,"New York University NYU Domestic Steinhardt School of Culture, Education and Human Development Office of Research Researchers_AP Location New York, NY Open Date Jul 13, 2020 Description This position requires a well-organized, highly motivated individual with experience planning and implementing complex research studies involving families with young children. This is a part-time hourly position. Responsibilities Collaborate with Dr. Natalie Brito and members of her lab in the planning and implementation of research studies involving language and neurocognitive development in infants and young children. This involves recruiting study participants, overseeing data collection and coding, and synthesizing data for analysis. This position also involves administrative and management duties that ensure the lab’s operations. Qualifications Professional degree minimum of a BA/BS degree in Psychology or a related field is preferred. Strong interpersonal and communication skills required along with the ability to prepare and conduct remote as well as in-person Strong management and collaboration skills to effectively manage multiple tasks, meet deadlines and guide a team through Application Instructions Submit Resume and Cover Letter"
166,Research Scientist - Neural Interfaces (PhD),research scientist,/rc/clk?jk=0b08fa3d2b295f87&fccid=1639254ea84748b5&vjs=3,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities we're just getting started. Facebook Reality Labs is looking for a Research Scientist to help us unleash human potential by erasing the bottlenecks between intent and action. Our neural interface research lies at the intersection of computational neuroscience, machine learning, signal processing, statistics, biophysics and human-computer interaction. Plan and execute cutting-edge applied research to advance neural interface capabilities. Collaborate with engineering teams to translate fundamental scientific knowledge into new technology. Use quantitative research methods to define, iterate upon and advance key areas of our research agenda. Must obtain work authorization in the country of employment at the time of hire and maintain ongoing work authorization during employment. Currently has, or is in the process of obtaining, a Ph D in computational neuroscience, machine learning, physics, computer science, or related fields. Experience with research-oriented implementation skills, including fluency with libraries for scientific computing e. g. scipy ecosystem and machine learning e. g. scikit-learn, Py Torch, Tensor Flow . Experience with quantitative skills mathematics, statistics and experience learning new technical knowledge and skills rapidly. Experience working independently to design, execute, interpret, and present research studies. Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions , sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb. com. "
167,"Postdoctoral Research Scientist, CTRL-labs (PhD)",research scientist,/rc/clk?jk=67a966fe37146b44&fccid=1639254ea84748b5&vjs=3,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities we're just getting started. Facebook Reality Labs is seeking a Postdoctoral Research Scientist to help us unleash human potential by eliminating the bottlenecks between intent and action. To achieve this, we’re building a practical neural interface drawing on the rich motor neuron signals that can be measured non-invasively with neuron-level resolution. Our research lies at the intersection of computational neuroscience, machine learning, signal processing, statistics, biophysics, motor learning, perceptual psychophysics, and human-computer interaction. We’re looking for people who want to shape the future of this technology and are excited about joining our collaborative research team that has grown out of the acquisition of CTRL-labs. Plan and execute applied research to advance neural interface capabilities. Collaborate with engineering teams to translate fundamental scientific knowledge into new technology. Use quantitative research methods to define, iterate upon and advance key areas of our research agenda. Currently has, or is in the process of obtaining, a Ph D in the field of computational neuroscience, machine learning, physics, computer science, or related fields. Interpersonal skills cross-group collaboration and cross-culture collaboration. Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment. Research-oriented implementation skills, including fluency with libraries for scientific computing e. g. Sci Py ecosystem and machine learning e. g. Scikit-learn, Py Torch, Tensor Flow . Proficiency with quantitative methods mathematics, statistics and experience learning new technical knowledge and skills rapidly. 3+ years of experience working autonomously to design, execute, interpret, and present research studies. Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex including pregnancy, childbirth, or related medical conditions , sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb. com. "
168,Post-doctoral Research Scientist,research scientist,/rc/clk?jk=57857c9c27f8bae1&fccid=bd976cc171c690e0&vjs=3,"Columbia University Post-doctoral Position The Department of Industrial Engineering and Operations Research IEOR at Columbia University in New York City invites applications for two post-doctoral positions for the project ``Risk-Aware Power System Control, Dispatch and Market Incentive’’, funded by ARPA-E. The post-doc will work with professors Daniel Bienstock, Agostino Capponi, and Garud Iyengar at Columbia. The post-doc will work on the analytics, engineering design, and development of a risk dashboard for operations in power systems. The dashboard will rely on advanced techniques from risk management and newly designed asset and systemic risk metrics to continuously track systems and market conditions, as well as to proactively dispatch available resources to avoid insecure operations. An important goal of the project is to inform independent system operators on how to use renewable, demand and storage resources for reducing risk, and on how to adequately compensate these resources for reducing systemic risks. The candidate will have the unique opportunity to work on a cutting edge and potentially transformative framework that integrates advanced risk management concepts and techniques from financial engineering into power flows and wholesale market operations, such as load and renewable forecasts. Minimum Degree Required Ph. D. or DES or Sc. D. Minimum Qualifications Candidates must have a Ph D degree in Electrical, Mechanical, Control, Environmental Engineering, Operations Research, or Applied Mathematics. We welcome candidates with a demonstrated record of research accomplishment and a strong commitment to excellence in education to apply. In addition, the ideal candidate should be familiar with power engineering and control theory. Exposure to financial risk management is desirable, but not mandatory. The candidate should also be very familiar with programming languages, such as C/C++ and Python. The candidate will also benefit from interactions with various faculty from the IEOR Department, the school of Engineering and Applied Sciences, the Business School, and the School of International and Public Affairs. This is a one-year position with possibility of renewal depending on the progress achieved and continued funding. Preferred Qualifications Additional Information Applicants can consult ieor. columbia. edu/ for more information about the department. RAPS posting date 07/06/2020 Search Closing Date Special Instructions to the Applicant Candidates should apply at http //pa334. peopleadmin. com/postings/5868 and submit the following curriculum-vitae, a copy of their official degree transcript, and a representative research paper. The applicants should also arrange to have at least two letters of recommendation submitted electronically. The position will remain open until filled and applications will be reviewed as they are submitted. Proposed Start Date 09/01/2020 EEO Statement Columbia University is an Equal Opportunity/Affirmative Action employer—-Race/Gender/Disability/Veteran. Review Begins"
169,Staff Machine Learning Engineer - Health ML,machine learning engineer,/rc/clk?jk=f300ec7c5a0c3cb4&fccid=7a3824693ee1074b&vjs=3,"Company Description null Job Description Are you an engineer who’s interested in tackling very challenging adversarial problems and passionate about defending online users against abuse, spam, and manipulation? Do you love working on challenging problems that require a multi-disciplinary approach, creative solutions, and rapid product iterations? Will you be proud to work on a real-time, scalable system that serves millions of users daily? If so, you should join us. Who We Are The Health ML engineering team is responsible for building scalable detection systems that keep spam, manipulation, and abuse at bay. We use ML and relevance techniques to make Twitter safer and to limit the spread of misinformation on the platform. Our team works across the product to detect abusive and spammy users and content, increase action on bad actors, drive changes in user behavior, and detect and remediate accounts that are violating the terms of service on Twitter. We develop, maintain, and contribute to several machine learning models and systems, including Models that detect unwanted interactions Models to prioritize human review of accounts violating Twitter's policies to more quickly take action and limit their damage Detection of bots that misuse the platform or spread misinformation Detection of repeat abusive offenders who create new accounts after being suspended Real-time rule engines and clustering systems to identify and action on clusters of bad actors at scale What You’ll Do Although you will work on cutting-edge problems, this position is not a pure research position. You will participate in the engineering life-cycle at Twitter, including designing distributed systems, writing production code and data pipelines, conducting code reviews and working alongside our infrastructure and reliability teams. You’ll apply data science, machine learning, and/or graph analysis techniques to a variety of modeling and relevance problems involving users, their social graph, their tweets, and their behavior. Qualifications Who You Are You’re a relevance engineer, applied data scientist, or machine-learning engineer who wants to work on exciting algorithmic and deep infrastructure issues to improve the health of the public conversation on Twitter. You’re experienced at solving large scale relevance problems and comfortable doing incremental quality work while building brand new systems to enable future improvements. You are experienced in one or more of the following machine learning including deep learning , information retrieval, recommendation systems, social network analysis. You are a strong technical advocate with a background in Java, C++, or Scala, and Python. Experience with a number of ML techniques and frameworks, e. g. data discretization, normalization, sampling, linear regression, decision trees, deep neural networks, etc 3+ years experience with one or more DL software frameworks such as Tensorflow, Py Torch, Theano You strive to find the right balance between moving fast to deliver quality improvements to users and accumulating technical debt that drags down productivity. You have a collaborative working style with a strong focus on disciplined execution and results. You like to ground decisions in data and reasoning and solve root causes of problems rather than surface issues. You are adept at communicating relevant information clearly and concisely. You look ahead to identify opportunities and thrive in a culture of innovation. Requirements M. S. or Ph D in Computer Science or Machine Learning related degree; or equivalent work experience in the field 7+ years experience leading and delivering effective ML solutions for large scale production use cases. Additional Information We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status. San Francisco applicants Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. "
170,Machine Learning Engineer Inter/Sr (visa sponsorship for Canada),machine learning engineer,/company/Qu%C3%A9bec-Immigration-Office/jobs/Machine-Learning-Engineer-Inter-Senior-551920bf74ee8a10?fccid=bf0edac03b213e18&vjs=3,"**Only for residents in the USA**Job location in Montreal, Canada*****Machine Learning Engineer Intermediate/Senior No R&D profiles or researchers only As a passionate ML Engineer, you will design and build machine learning systems, tools, and validation strategies to support new machine learning applications.  You should have a proven history of bringing machine learning initiatives into production, and in generating new ML ideas.  You will have the ability to influence your peers, and leadership can enable others to adopt your vision. As an ML Engineer, you will collaborate with other Machine Learning Engineers and Data Scientists.  Why work with us ?Since entering Montreal’s tech scene 6 years ago, our client continues to grow rapidly while the team forges ahead on a path of inventing change and creating real impact for our customers. We’re looking for an Intermediate/Senior Machine Learning Engineer who is innovative, eager to own their success and energized by the opportunity to pursue a career that’s as challenging as it is rewarding. We empower you to raise your voice, make a real impact not just within our fast growing company, but in everyday lives of tens of thousands of our customers. What your day to day looks like Develop and deploy ML/DL models from raw data - Design ML/DL applications that demonstrate the technology's real-world potential - Use best SW Engineering methodologies to implement the applications - Deploy the applications on Cloud to release their full power - Drive your ML/DL projects from proposal to production Are you right for the role? Technical Skills Good understanding of machine learning theory and mathematical modeling - Ability to develop machine learning models on large and complex data - Practical experience with ML/DL libraries, like Keras/Tensor Flow, Torch, Scikit-learn - Intermediate/Advanced programming skills in Python 3+ years , or any other popular programming languages like Java or C++ - Good knowledge in Database/Datawarehouse and proficiency in using SQL - Experience with Cloud Computing 1-2 years , like AWS, GCP, Azure - Knowledge in developing micro-service - Data driven mindset - Passionate to design high reliability & high-performance solutions Personal Skills Ability to work both independently and as a member of a team - Good communication skills and initiative is a must - Self-driven individual and able to engage with different teams - Fast learner, high capacity for abstract thinking with hands-on mentality - Curious about new tech and advances in the industry, think further than the solution appears to require Nice to Have Any AWS certificates - Experience in data processing, like using Spark or Flink for ETL jobs - Container experience, like Docker or Kubernetes - Familiar with Linux systems and Shell programming - RES Tful API and Webserver experience or knowledge - Knowledge in GPU computing - CI/CD knowledge or experience with Jenkins - Good knowledge in GIT - Agile experience What’s in it for you ? Work alongside super talented and friendly people that like to drive innovation - Competitive salary & vacation packages - Complimentary breakfast and lunch daily prepared by our in-house chef- Generous referral program- Year-round events, from massive to casual and everything in between - 50% off your Opus Transit subscription  - Beautiful office right near Namur metro You’ll be Joining   We’re DJ’s, video game heroes, photographers, movie buffs, musicians, grill masters and everything in between. We’re fiercely proud of our teammates, our work, and the impact we’re making to help tens of thousands of entrepreneurs who rely on our e-commerce platform to monetize their content. If you're inspired to do the best work of your life, then join the team. Job Type Full-time Pay $55. 00 $67. 00 per hour Benefits Health Insurance Paid Time Off Parental Leave Relocation Assistance Schedule Monday to Friday Experience Cloud Computing 1 year Required Python or Java or C++ 3 years Required Work Location One location Visa Sponsorship Potentially Available Yes Immigrant visa sponsorship e. g. , green card sponsorship Company's website http //unbelavenir. gouv. qc. ca/en/Work Remotely Yes"
171,Machine Learning Engineering Manager,machine learning engineer,/rc/clk?jk=9c4603ea1137ffa9&fccid=fe404d18bb9eef1e&vjs=3,"The Personalization team makes deciding what to play next easier and more enjoyable for every listener. From Daily Mix to Discover Weekly, we’re behind some of Spotify’s most-loved features. We built them by understanding the world of music and podcasts better than anyone else. Join us and you’ll keep millions of users listening by making great recommendations to each and every one of them. The Lifetime Value team within Personalization is looking for an experienced Engineering Manager in New York City. You will be contributing to a highly scaled ML platform that will be used to evaluate the ROI of critical company bets, drive value-based recommendations and decisions, and predict complex user behaviors in collaboration with business leads, product managers, and data scientists across many business units. The long term goal of the team is to apply best-in-class technology and research to drive value for both our users and our business. What You’ll Do Lead one or more teams of multi-discipline engineers, taking accountability for delivery of a production-scale ML platform, and overseeing technological health Coach engineers’ architectural vision, operational excellence, and career development Collaborate with product management to set and execute the right goals, and coordinate delivery against those goals with our partner teams in research and data science Provide leadership to other efforts that fit your passion and Spotify’s needs at the intersection of other product areas, business units, and across Spotify In partnership with a product manager and a team of engineers, develop a deep understanding of the long term behavior of our users and what drives it, working to model those user needs with cutting-edge ML techniques Foster a strong partnership with the platform teams that provide tools and data to our product-focused teams Who You Are You thrive when helping individuals develop to be their best You thrive when shipping phenomenal products that meet users’ needs You are passionate about creative machine-learning-driven products, specifically user behavior models, and the substantial challenges they can address, especially within Spotify’s mission to unlock the potential of human creativity You have experience in cultivating a strong agile engineering culture by building and empowering highly effective teams to learn quickly and deliver value through rapid iteration You are an experienced manager with strong people skills and mentorship and leadership experience You have previous industry experience with the application of machine learning to predict user behaviors, particularly how to scale such models to hundreds of millions of users You love staying up-to-date with the state-of-the-art in machine learning and causal inference through review of research You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 286 million users. "
172,Machine Learning Engineer,machine learning engineer,/rc/clk?jk=c88e1dfd6bb6562d&fccid=bf8d0f5b45d49fcf&vjs=3,"About the Role The Machine Learning Engineer will join a core group of Data Scientists who play an important role in generating strategic insights across the VNSNY organization, and will be responsible for Developing, testing, and deploying machine learning algorithms to support and improve business processes Creating and maintaining a framework for deploying machine learning algorithms Building applications to implement, track, and monitor predictive models; ensuring that the accuracy of deployed algorithms is monitored on an ongoing basis Engineering computational solutions and algorithms to meet the predictive needs of business and clinical units across VNSNY Conducting end-to-end analysis that includes data gathering from internal and external sources, specifying requirements, processing, compiling and validating data, ensuring deliverables are met, and preparing presentations"
173,Staff Machine Learning Engineer - HBO Max,machine learning engineer,/rc/clk?jk=56fd6138c4c0310f&fccid=a710372f724c1e2e&vjs=3,"Staff Machine Learning Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178110BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As a Staff Machine Learning ML Engineer on Personalization & Content Science team, you will design and build ML workflows to fuel analyses and inferences in collaboration with other technology teams in Warner Media and possibly AT&T to design and implement end-to-end solutions within a cloud-based analytic infrastructure. The ML based insights will support the HBO Max product management, marketing and content teams understand what makes our customers tick and how to target new ones. The Daily Collaborate with product owners, data scientists and engineering teams to evaluate effectiveness of deployed ML model. Work with data scientists, data engineers and solution analysts to design best-suited, robust and scalable ML endpoints for content discovery, personalization. Design and develop robust and scalable experimentation approach for model evaluation. Experience in writing production level-code and engage in code reviews. Lead machine learning model deployment and production execution. Experience of deploying ML models on big data volumes and optimize performance. Extensive experience with ML frameworks and expertise with at least one. Contribute in research, design, experimentation, development, deployment, monitoring and maintenance of ML model lifecycle. Research and implement best practices to enhance existing machine learning infrastructure. Design and implement data quality framework on content metadata pipeline Maintain and own all workflow documents up to date. The Essentials MS/Ph. D in Computer Science, Information Science, engineering or equivalent experience. Minimum 4 years of relevant and hands-on experience in machine learning with track record of processing, enriching and extracting value from large datasets. Strong experience in incrementally building big data product and production pipelines to support data and analytic functions. Familiar with various data science techniques, hands on experience with ML frameworks, concepts and implement models developed into production environment. Flexibility and comfort working in a dynamic, team environment with possible remote organization with minimal documentation and process. Strong problem solving and conceptual thinking skills. Entertainment or media industry a big plus. Knowledge and experience in the following technologies and frameworks is critical Apache Spark, Apache Mx Net, Tensor Flow, Scikit-Learn, Py Torch, AWS S3, EMR, Sagemaker, Personalize, Lambda, Step functions, Cloud Watch, etc. Python, Spark, Py Spark, Spark Scala, SQL, Hadoop, Scala, Jupyter notebook Relational, non-relational databases Snowflake, Dynamo DB and graph databases Content metadata, media supply chain The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
174,Senior Machine Learning Engineer - BLAW,machine learning engineer,/rc/clk?jk=cbfc61e525eb5da4&fccid=f770da67b3b51c62&vjs=3,"Bloomberg Law BLAW is changing the legal industry by delivering the most sophisticated research platform on the market with a focus on automation, analytics and real-time answers. We provide our users with fast access to legal content and analysis, practice tools, company information and market intelligence through advanced search & analytic capabilities. We are committed to changing the way legal professionals conduct their day to day tasks by automating research and providing analytical solutions to help them get real-time answers and better serve their clients. Our goal is to use innovative technologies to deliver best in class solutions on a fast and reliable web platform that will impact the future practice of law. Who are we? The Bloomberg Law Machine Learning Team are Researchers and Engineers who have a passion for solving complex problems in the legal domain. As a member of our team, you will focus on leveraging Artificial Intelligence AI technologies to build workflow solutions aimed at making attorneys more efficient in completing their everyday legal tasks. We use Information Extraction, Natural Language Processing NLP and Machine Learning ML techniques such as named entity disambiguation, text classification, clustering and topical modeling to extract and identify relevant, meaningful and actionable information such as case outcomes, legal entities, document structure, legal principles, legal topics from legal data. We work closely with product managers, software engineers and legal domain experts using agile development. We’ll trust you to Lead all phases of machine learning model development life cycles from data gathering to productionizing models, optimizing model performance and monitoring Design, write, test and maintain modular production-quality code Partner with engineers and legal domain experts to design, experiment and evaluate Machine learning models for new ideas You'll need to have 3+ years of experience in Artificial Intelligence AI , Natural language Processing NLP , Machine Learning ML , Statistical Models, and Text Analytics on large data sets 3+ year of experience programming in Python, Java or C++ Experience in all phases of machine learning application life cycles from data gathering and preparation to optimizing model performance BA, BS, MS, Ph D in Computer Science, Data Science or related technology field We'd love to see Publications or presentations in relevant communities such as ACL, AAAI, SIGIR, KDD, EMNLP, ICML, Neur IPS or equivalent Experience with distributed computational frameworks YARN, Spark, Hadoop, Kubernetes, Docker Legal domain experience And do check out our blog, Tech At Bloomberg. com, to learn more about our publications and projects in data science. Bloomberg is an equal opportunities employer, and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. "
175,Senior Machine Learning Engineer,machine learning engineer,/rc/clk?jk=781f6bddacaba446&fccid=25039f950f78d541&vjs=3,"Sirius XM and Pandora have joined together to create the leading audio entertainment company in the U. S. Together, we are uniquely positioned to lead a new era of audio entertainment by delivering the most compelling subscription and ad-supported audio experiences to millions of listeners in the car, at home and on the go. Our talent, content, technology and innovation continue to be at the forefront, and we want you to be a part of it! Check out our current openings below and at www. siriusxm. com/careers. Position Summary As part of the Shared Science Foundation team, you will design and build systems that solve prevalent science problems across Sirius XM and Pandora’s digital products. These extensible, shared systems power personalization, content understanding, and advertising and marketing science. You will collaborate with scientists, engineers, and product managers to develop tools, platforms, and infrastructure that accelerates innovation of the entire organization. Your hybrid skill set of machine learning, software engineering, and empathetic communication uniquely positions you to architect systems with multiple stakeholders. Duties and Responsibilities Design and build the next-generation representation of Sirius XM and Pandora listeners’ tastes and interests, incorporating music and non-music listening, favorites, contextual variation, and behavioral, advertising, and marketing signals. Architect and build large-scale machine learning infrastructure for data exploration, prototyping, and production. Write production code and data pipelines and conduct code reviews. Promote and role-model best practices of data science, engineering, and communication throughout the organization. Supervisory Responsibilities None Minimum Qualifications 2+ years of industry experience as an applied data scientist or ML engineer. Requirements and General Skills Excellent written and verbal communication skills, with the ability to effectively advocate technical solutions to scientists, engineers, and product audiences. Demonstrated ability to collaborate with and coordinate teams. Self-motivated, growth-oriented, and driven to pursue solutions to challenging problems. Must have legal right to work in the U. S. Technical Skills Production experience implementing machine learning pipelines and models at scale in Python, Java, Scala, or similar languages. Proficiency with distributed processing and warehousing frameworks e. g. , Spark, Hadoop, Hive, Tez, etc. . Experience with the research and development workflow/life-cycle for large-scale batch and streaming machine learning systems. Experience architecting distributed systems and familiarity with software design patterns. Ability to gather stakeholder requirements and evaluate technical trade-offs. Bonus Skills M. S. or Ph. D. in a quantitative field CS, EE, Statistics, Physics, Math, etc. . Passion for data-driven development, reliability, and disciplined experimentation. Experience designing or building feature stores to integrate and version heterogeneous data types from heterogeneous data sources. Experience with any of the following Cloud computing Google Cloud Platform, Amazon Web Services, Azure Technologies Kafka, Airflow, Composer ML-frameworks Tensor Flow, Py Torch, Vowpal Wabbit, scikit-learn Knowledge of professional software engineering practices including coding standards, code reviews, source control management, build processes, testing, and operations. Our goal at Sirius XM+Pandora is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. Sirius XM+Pandora is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws. The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice. "
176,Data Engineer II,data engineer,/rc/clk?jk=4f976e647c13367a&fccid=7ca1385998532a85&vjs=3,"Condé Nast is a global media company producing the highest quality content with a footprint of more than 1 billion consumers in 32 territories through print, digital, video and social platforms. The company’s portfolio includes many of the world’s most respected and influential media properties including Vogue, Vanity Fair, Glamour, Self, GQ, The New Yorker, Condé Nast Traveler/Traveller, Allure, AD, Bon Appétit and Wired, among others. Condé Nast Entertainment was launched in 2011 to develop film, television and premium digital video programming. Job Description The Data Engineering team within the Data Organization have a wide range of responsibilities and play a critical role in shaping how Condé Nast enables its business using data. The team is responsible for building data pipelines, data products and tools that enable our Data Scientists, Analysts in various business units, Business Intelligence Engineers and Executives to solve challenging use cases in our industry. We are seeking a Data Engineer Level II who will build, maintain and influence a range of initiatives across various technical and business areas within Condé Nast. If you are looking for a challenging environment and to work with a world class team of data engineers in a well balanced environment and seasoned company, read on RESPONSIBILITIES Responsibilities include, but are not limited to Design, build and test batch and streaming data pipelines using Spark, Airflow and AWS/GCP services Build efficient code to transform raw data Collaborate with other Data Engineers to implement a shared technical vision. Follow agile processes with a focus on delivering production-ready, testable deliverables, and automated code Participate in the entire software development lifecycle, from concept to release Perform code reviews and ensure other engineers are following best engineering practices Ensure data quality and data integrity across datasets Assist in executing data storage and data cataloging strategy within the enterprise Collaborate with both internal and external data providers to ensure data feed stability Partner with Dev Ops to continually ensure stability to our infrastructure and platform Contribute to shared data engineering tooling & standards to improve productivity Support, monitor and optimize current and future data infrastructure and platform Ensure SL As between the data engineering team and other stakeholders Evaluate technologies and conduct proof-of-concepts MINIMUM QUALIFICATIONS BS, or equivalent industry experience in Computer Science, Software Engineering, or other related Science/Technology/Engineering/Math fields 2+ years experience building batch or real-time data pipelines At least 3 years of relevant experience in software development Experience in writing reusable/efficient code to automate analysis and data processes Experience translating business/technical requirements into actual code Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from web analytics, consumer analytics, user behavior and advertising Experience implementing scalable, distributed, and highly available systems using AWS services such Kinesis, Dynamo DB, S3 Proficiency in Python/Py Spark, Scala or Java Proficiency in SQL Experience with Spark, AWS ecosystem, Hadoop, Pig, Hive, Flink, or Beam. Experience with orchestration tools such as Airflow Experience with CI/CD tools Experience with Git version control, and other software adjacent tools What happens next? If you are interested in this opportunity, please apply below, and we will review your application as soon as possible. You can update your resume or upload a cover letter at any time by accessing your candidate profile. C ondé Nas t is an equal opportunity workplace. Duties and responsibilities may be adjusted based on years of experience. Salary is also commensurate with experience. "
177,"AWS Data Engineer (AWS, Python & Data Lake)",data engineer,/rc/clk?jk=c8b8fd0a5bb4b80b&fccid=c6740ca0f8a588b9&vjs=3,"AWS Data Engineer AWS, Python, Elastic Search, Data Lake Location New York, NY The Architect's primary responsibility will be to design, develop, test and deliver solutions using AWS technology stack. Must have good experience with Python. . Duties and Responsibilities Participate in the design, development and delivery of solutions on AWS technology stack Develop efficient and high performant code Develop unit tests to maximize code coverage and end to end business scenarios Support existing architecture and code Documents code and design as needed Work with other teams to understand scope and design efficient solution Collaborates within the Agile framework, fosters a team approach to software development and delivery Ensure all code developed follows Dev Ops practices Minimum Qualifications Bachelors Degree in Computer Science, Computer Engineering, Information Systems or similar Must have good experience with Python. Architect and implement solutions on AWS Elasticsearch Engineer solutions on AWS based data processing stack Work with business and IT teams to understand and capture search requirements Hands on experience with EC2, S3, Lambda, Dynamo DB, RDS, SQS, SNS, Elastic Search, Logstash and Kibana Experience consuming and build REST AP Is Strong SQL skills Experience with ETL architecture and using ETL tools Experience writing multithreaded and high performant software Experience with Dev Ops tools such as Jenkins, Ansible, Artifactory and Consul Preferred Qualifications Master Degree in Computer Science AWS Solution Architect or Developer Certification Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry Experience with Cloud Data Warehouse "
178,Lead Data Engineer,data engineer,/company/Medly-Pharmacy/jobs/Lead-Data-Engineer-ebb82639c9e73a33?fccid=c392217ba496c768&vjs=3,"We’re Medly, a human-centered digital pharmacy that prioritizes patient well-being. Our mission is to deliver prescriptions to patients at prices they can afford regardless of income, lifestyle, language or diagnosis. We live our mission to democratize healthcare by leveraging technology to create accessible tools for improving lives. Visit us online at www. Medly Pharmacy. com. We are looking for a Lead Data Engineer with the desire to make an impact and help us achieve our patient-centric vision. Your overall mission As a Lead Data Engineer, you’ll support the scaling of our data science infrastructure and capability through mentoring and growing engineers by providing resources and establishing best practices. You’ll manage the team’s organizational communications as an advocate for data science, ensuring that your colleagues’ satisfaction with their contributions. What you’ll do Adopt and establish best practices for data infrastructure and data governance Manage technical debt and anticipate data science and business intelligence needs Manage the data engineering roadmap and help to bring the organization towards an automated, scalable and fault-tolerant infrastructure Coach engineers and team members on translating business requirements into technical designs and solutions Create opportunities for continuous learning and professional development Manage third-party relationships What you’ll gain Competitive compensation and benefits Fantastic room for growth and development The ability to make a noticeable impact and improve lives What you’ll need 5 or more years of experience working with data technologies BI or data science including time spent leading data teams B. S. in Computer Science, Mathematics, Statistics, Engineering or a related field Experience with SQL, R, Python, Java, SPSS, Tableau or other business intelligence tools Experience working in cloud platforms AWS, Azure Experience working with cross-functional product teams Proven track record translating and productionizing machine learning algorithms Ability to maintain data infrastructure and scalable applications Strong understanding of data warehouse and application from design to deployment Knowledge of best practices for SDLC, source control and testing Work on complex BI ecosystems and design / develop solutions Passion for mentoring colleagues Medly Pharmacy is an equal opportunity employer. We proudly celebrate diversity and are devoted to creating an inclusive environment for all employees regardless of race, color, religion, national origin, sex, sexual orientation, age, disability, military service, or any other non-merit factor. Job Type Full-time Benefits 401 k Dental Insurance Flexible Schedule Health Insurance Paid Time Off Vision Insurance Schedule Monday to Friday Work Location One location Company's website https //www. medlypharmacy. com/Work Remotely Temporarily due to COVID-19"
179,QA Data Engineer,data engineer,/rc/clk?jk=ec7b893524686f66&fccid=c40fae22b6a11e7d&vjs=3,"Summary We exist to help people achieve financial clarity. At Thrivent, we believe money is a tool, not a goal. Driven by a higher purpose at our core, we are committed to providing financial advice, investments, insurance, banking and generosity programs to help people make the most of all they’ve been given. At our heart, we are a membership-owned fraternal organization, as well as a holistic financial services organization, dedicated to serving the unique needs of our clients. We focus on their goals and priorities, guiding them toward financial choices that will help them live the life they want today—and tomorrow. Join our newly created Data Office as a QA Data Engineer! We have two opportunities including mid and lead level, within the Information Delivery team. We are in the early stages of a Snowflake cloud data warehouse implementation and Confluent Kafka streaming data implementation. You will be responsible for data warehouse, BI, and Kafka quality assurance. This role will require in depth understanding of data quality assurance and testing tools. Job Description Job Duties and Responsibilities Develop, implement physical and virtual test data management and test environment solutions across the organization, to contribute towards the success of technology initiatives Develop test data management strategies and plans relevant to each test environment Dev, Sys and ITE to provide right-sized, production-like, re-usable and secured test data per data requirements Key responsibilities include source system data analysis, business & privacy data requirements definition, data discovery, profiling, masking & monitoring rules definition and overall data provisioning for setting up a test environment. Works closely with application subject matter experts/business partners/testers to determine appropriate data sources and to define data provisioning mechanism based on the need for test data Learn and contribute to the design of data management and any future transformation of the test environment landscape Works very closely with Application Availability, Compliance and Support teams leads to maintain and support all test environments to maintain SL As and applicable policies / standards Participates in all phases of a solution delivery including test data and env requirements gathering and provisioning the required test data in the appropriate test environments Conducts full lifecycle activities including requirements analysis and design, develop and deliver capabilities, and continuously monitor performance and quality control plans to identify improvements around Test Data and Test Env management process Lead initiatives to design processes to capture and review test data requirements and test data provisioning techniques and plans Lead work to advance and support information management practices within business processes, applications and technology that underpin the Test Data and Test Env management discipline e. g. establishing quality processes, performing analysis, participating in technology implementation planning and verification to ensure successful installation of software and/or projects , implementing data provisioning processes, providing the right amount of data in an appropriately provisioned test environment Provide leadership for Test Data & Test Env management related tasks in support of projects Lead the Management and proactive improvement of Thrivent's Test Data & Test Env provisioning practice by analyzing the current systems environment, leveraging proven practices, applications, technology, tools and platforms to support and enhance the discipline Handle budget responsibilities Leads the delivery, support and maintenance of solutions with one or more business and technology areas. Organizational impact results from mid-large sized projects Lead discussions with tool / product vendors in support of evaluating solutions for process improvements or future offerings Lead set up of test data and virtual environment based on functional test requirements utilizing Enterprise standard tool sets Required Job Qualifications Bachelor’s degree or equivalent in MIS, Computer Science, Mathematics, Business or related field. 5+ years of experience in Technology related field including prior lead experience. For the senior level position 8+ years of experience in Technology related field including 3+ years prior lead experience. Advanced in-depth knowledge of Test Data and Test Env Management concepts and tools. Strong organizational, analytical, critical thinking and leadership skills. Demonstrated leadership on mid-large-scale project impacting strategic partners. Advanced in-depth knowledge of data warehouse and BI systems design, streaming architecture, and architecting and implementing large business systems is desired. Thrivent provides Equal Employment Opportunity EEO without regard to race, religion, color, sex, gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state, or federal law. This policy applies to all employees and job applicants. Thrivent is committed to providing reasonable accommodation to individuals with disabilities. If you need a reasonable accommodation, please let us know by sending an email to human. resources@thrivent. com or call 800-847-4836 and request Human Resources. "
180,Remote Data Engineer - Data Mapping - Hadoop/Spark,data engineer,/company/C&G-Consulting-Services/jobs/Remote-Data-Engineer-45d79e28eba076c9?fccid=dd6a8cbdd6a3ccba&vjs=3,"Spark, Hadoop data extraction experience Strong data mapping experience Programming around extracting data and databases, running transformations and landing it into a database Performance tuning knowledge Reconciliation – write the main data flow jobs Main Focus from enrollment data, claims data Spark and Hadoop Development experience with Big Data Systems, pipelines, and data processing Data ingestion and data processing using Spark and Hadoop Job Type Contract Pay $70. 00 $75. 00 per hour Schedule Monday to Friday Work Location Fully Remote Work Remotely Yes"
181,Snowflake Data Engineer (remote),data engineer,/rc/clk?jk=b31d2117ddcc3815&fccid=c40fae22b6a11e7d&vjs=3,"Summary We exist to help people achieve financial clarity. At Thrivent, we believe money is a tool, not a goal. Driven by a higher purpose at our core, we are committed to providing financial advice, investments, insurance, banking and generosity programs to help people make the most of all they’ve been given. At our heart, we are a membership-owned fraternal organization, as well as a holistic financial services organization, dedicated to serving the unique needs of our clients. We focus on their goals and priorities, guiding them toward financial choices that will help them live the life they want today—and tomorrow. Join our newly created Data Office as a Cloud Data Engineer! We have two positions open for mid and senior level within the data warehouse implementation team. We are in the early stages of a Snowflake cloud data warehouse implementation. You will be responsible for general ETL development and implementing new solutions. You will have the opportunity to help Thrivent modernize our hybrid technology solutions including the opportunity to work on modern warehousing and integration technologies. This role will require in depth understanding of cloud data integration tools and cloud data warehousing Snowflake experience is critical , the ability to lead and execute to help drive and see tangible result. Job Description Job Duties and Responsibilities Lead the implementation, execution, and maintenance of Data Integration technology solutions Lead work to advance and support information management practices within business processes, applications and technology that underpin the EIM discipline e. g. establishing data quality processes, performing data analysis, participating in technology implementation planning and verification to ensure successful installation of software and/or projects, implementing data integration processes, administering content, etc. . Provide leadership for Data Integration tasks supporting projects Lead the Management and proactive improvement of Thrivent's data by analyzing the current systems environment, leveraging proven practices, applications, technology, tools and platforms to support and enhance the information landscape. Revenue generated Budget responsibilities Leads the delivery, support and maintenance of solutions with one or more business and technology areas. Organizational impact results from mid-large sized projects Required Job Qualifications Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field 5+ years of experience in Technology related field including prior lead experience. For the senior level position require 8+ years of experience including 3+ years prior lead experience. Advanced in-depth knowledge of data integration concepts and tools Strong organizational, analytical, critical thinking and leadership skills Demonstrated leadership on mid-large-scale project impacting strategic partners Thrivent provides Equal Employment Opportunity EEO without regard to race, religion, color, sex, gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state, or federal law. This policy applies to all employees and job applicants. Thrivent is committed to providing reasonable accommodation to individuals with disabilities. If you need a reasonable accommodation, please let us know by sending an email to human. resources@thrivent. com or call 800-847-4836 and request Human Resources. "
182,Informatica Big Data Engineer,data engineer,/rc/clk?jk=f4aecb169213ba48&fccid=a4e4e2eaf26690c9&vjs=3,"Join Accenture and help transform leading organizations and communities around the world. The As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. Job Description Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes. Extract, Transform and Load data primarily in Informatica Powercenter and Big Data Management with an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented. Demonstrate an understanding of technology and digital frameworks in the context of data integration utilizing Informatica. Ensure code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts. Basic Qualifications Minimum 4 years of experience developing and implementing Informatica Powercenter or Informatica Big Data Management Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years Preferred Qualifications Experience in ETL Tools in addition to Informatica, including Business Objects Data Services BODS , Data Stage, Ab Initio, Talend, and Pentaho Experience implementing or supporting Data Integration of Big Data with Sqoop or similar tools Knowledge of Big Data Solutions such as Hadoop Ecosystem Database experience Teradata, Oracle, SQL Server, DB2, Azure SQL Strong knowledge and experience of SQL Understanding of Entity relationship data models and Dimensional Models Experience with development and production support Professional Skill Requirements It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients typically Monday through Thursday . Proven success in contributing to a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication written and oral and interpersonal skills All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career. Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture i. e. , H1-B visa, F-1 visa OPT , TN visa or any other non-immigrant status . Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration. Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities. Equal Employment Opportunity All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or"
183,Snowflake Data Engineer (remote),data engineer,/rc/clk?jk=987e1d136d1dd1cf&fccid=c40fae22b6a11e7d&vjs=3,"Summary We exist to help people achieve financial clarity. At Thrivent, we believe money is a tool, not a goal. Driven by a higher purpose at our core, we are committed to providing financial advice, investments, insurance, banking and generosity programs to help people make the most of all they’ve been given. At our heart, we are a membership-owned fraternal organization, as well as a holistic financial services organization, dedicated to serving the unique needs of our clients. We focus on their goals and priorities, guiding them toward financial choices that will help them live the life they want today—and tomorrow. Join our newly created Data Office as a Cloud Data Engineer! We have two positions open for mid and senior level within the data warehouse implementation team. We are in the early stages of a Snowflake cloud data warehouse implementation. You will be responsible for general ETL development and implementing new solutions. You will have the opportunity to help Thrivent modernize our hybrid technology solutions including the opportunity to work on modern warehousing and integration technologies. This role will require in depth understanding of cloud data integration tools and cloud data warehousing Snowflake experience is critical , the ability to lead and execute to help drive and see tangible result. Job Description Job Duties and Responsibilities Lead the implementation, execution, and maintenance of Data Integration technology solutions Lead work to advance and support information management practices within business processes, applications and technology that underpin the EIM discipline e. g. establishing data quality processes, performing data analysis, participating in technology implementation planning and verification to ensure successful installation of software and/or projects, implementing data integration processes, administering content, etc. . Provide leadership for Data Integration tasks supporting projects Lead the Management and proactive improvement of Thrivent's data by analyzing the current systems environment, leveraging proven practices, applications, technology, tools and platforms to support and enhance the information landscape. Revenue generated Budget responsibilities Leads the delivery, support and maintenance of solutions with one or more business and technology areas. Organizational impact results from mid-large sized projects Required Job Qualifications Bachelor’s degree or equivalent experience in MIS, Computer Science, Mathematics, Business or related field 5+ years of experience in Technology related field including prior lead experience. For the senior level position require 8+ years of experience including 3+ years prior lead experience. Advanced in-depth knowledge of data integration concepts and tools Strong organizational, analytical, critical thinking and leadership skills Demonstrated leadership on mid-large-scale project impacting strategic partners Thrivent provides Equal Employment Opportunity EEO without regard to race, religion, color, sex , gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state , or federal law. This policy applies to all employees and job applicants. Thrivent is committed to providing reasonable accommodation to individuals with disabilities. If you need a reasonable accommodation , please let us know by sending an email to human. resources@thrivent. com or call 800-847-4836 and request Human Resources. "
184,Cloud/Data Engineer,data engineer,/company/Nasdaq/jobs/Cloud-Data-Engineer-30e7cbce2df3b364?fccid=34185ebde4f4f63b&vjs=3,"About us Nasdaq Nasdaq NDAQ is a leading global provider of trading, clearing, exchange technology, listing, information and public company services. Through its diverse portfolio of solutions, Nasdaq enables customers to plan, optimize and execute their business vision with confidence, using proven technologies that provide transparency and insight for navigating today's global capital markets. As the creator of the world's first electronic stock market, its technology powers more than 100 marketplaces in 50 countries, and 1 in 10 of the world's securities transactions. Nasdaq is home to approximately 4,000 total listings with a market value of approximately $15 trillion. To learn more, visit http //www. nasdaq. com Global Technology is responsible for developing World Class leading technology to support our Markets, Market Makers not only in the US but across the Globe. Within our greater technology organization we are committed to excellence in every aspect of software development from Capability Conception to Project Execution. There are many technical teams – spread across the Globe with a mind to usher in innovative technology to meet Nasdaq’s constantly growing needs and opportunities. The Team This role offers the right candidate an opportunity to participate in our World Class Index Calculation and Dissemination Technology. At the heart of that technology our Research & Development team uses a cloud based data store to support research and development. This role will provide AWS, SCALA, SPARK and SQL expertise, and source data management support to the Index Administration Team. Your role and responsibilities Big Data Engineering on Cloud Platform Good understanding of Big data technologies – Spark SQL Programming SPARK in Scala Proficiency in SQL Strong data analysis and troubleshooting skills Regularly interacts with Business ,Data Vendors and Cloud Dev Ops Team Domain knowledge of Capital Market is plus Knowledge of shell scripts and other languages including Python, R, Java is plus Unit testing/verification of new development Work with QA Test analyst to ensure test coverage Including Integration & Regression testing Develops new program logic and/or assembles standard logic modules to create new applications You will need the following skills and experience At least 2 years of hands on experience on Big Data Engineering on AWS Minimum 1 year experience with Spark, Scala Must be able to write complex sql queries Experience with RDB and sql server databases Could be MS or Oracle… Experience with Large data sets Excellent teamwork/collaboration skills Excellent communication skills Written & Oral Good knowledge of linux OS, shell scripting Experience working on complex distributed information systems Experience with version control systems, preferable SVN, GIT. Strong work ethic in a mission-critical 24x7 diverse environment with multiple vendor/customer relationships. And it would be great if you have experience with AWS Certification is plus Preferred – Experience with Financial Markets options a plus Preferred – Working knowledge of various Financial Market data feeds Beneficial having experience with log aggregation tools such as Elasticsearch, Splunk, Datadog, etc Must be able to work Saturdays and rotational schedule Sounds like you? Please follow through by clicking the “Apply” link and submitting your application. If your skills and experience are a match, we will be in touch soon, and in the meantime please visit our website and social media channels to learn more about our innovative business, inclusive culture and where a career at Nasdaq can take you. Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information or any other status protected by applicable law. Job Type Full-time Pay $120,000. 00 $130,000. 00 per year Benefits 401 k Dental Insurance Health Insurance Paid Time Off Tuition Reimbursement Vision Insurance Schedule Monday to Friday Experience SQL 2 years Required Spark and Scala 2 years Required Location New York, NY 10036 Required Visa Sponsorship Potentially Available No Not providing sponsorship for this job Work Remotely Temporarily due to COVID-19"
185,QA Data Engineer,data engineer,/rc/clk?jk=b6d2aa1c9dbc3269&fccid=c40fae22b6a11e7d&vjs=3,"Summary We exist to help people achieve financial clarity. At Thrivent, we believe money is a tool, not a goal. Driven by a higher purpose at our core, we are committed to providing financial advice, investments, insurance, banking and generosity programs to help people make the most of all they’ve been given. At our heart, we are a membership-owned fraternal organization, as well as a holistic financial services organization, dedicated to serving the unique needs of our clients. We focus on their goals and priorities, guiding them toward financial choices that will help them live the life they want today—and tomorrow. Join our newly created Data Office as a QA Data Engineer! We have two opportunities including mid and lead level, within the Information Delivery team. We are in the early stages of a Snowflake cloud data warehouse implementation and Confluent Kafka streaming data implementation. You will be responsible for data warehouse, BI, and Kafka quality assurance. This role will require in depth understanding of data quality assurance and testing tools. Job Description Job Duties and Responsibilities Develop, implement physical and virtual test data management and test environment solutions across the organization, to contribute towards the success of technology initiatives Develop test data management strategies and plans relevant to each test environment Dev, Sys and ITE to provide right-sized, production-like, re-usable and secured test data per data requirements Key responsibilities include source system data analysis, business & privacy data requirements definition, data discovery, profiling, masking & monitoring rules definition and overall data provisioning for setting up a test environment. Works closely with application subject matter experts/business partners/testers to determine appropriate data sources and to define data provisioning mechanism based on the need for test data Learn and contribute to the design of data management and any future transformation of the test environment landscape Works very closely with Application Availability, Compliance and Support teams leads to maintain and support all test environments to maintain SL As and applicable policies / standards Participates in all phases of a solution delivery including test data and env requirements gathering and provisioning the required test data in the appropriate test environments Conducts full lifecycle activities including requirements analysis and design, develop and deliver capabilities, and continuously monitor performance and quality control plans to identify improvements around Test Data and Test Env management process Lead initiatives to design processes to capture and review test data requirements and test data provisioning techniques and plans Lead work to advance and support information management practices within business processes, applications and technology that underpin the Test Data and Test Env management discipline e. g. establishing quality processes, performing analysis, participating in technology implementation planning and verification to ensure successful installation of software and/or projects , implementing data provisioning processes, providing the right amount of data in an appropriately provisioned test environment Provide leadership for Test Data & Test Env management related tasks in support of projects Lead the Management and proactive improvement of Thrivent's Test Data & Test Env provisioning practice by analyzing the current systems environment, leveraging proven practices, applications, technology, tools and platforms to support and enhance the discipline Handle budget responsibilities Leads the delivery, support and maintenance of solutions with one or more business and technology areas. Organizational impact results from mid-large sized projects Lead discussions with tool / product vendors in support of evaluating solutions for process improvements or future offerings Lead set up of test data and virtual environment based on functional test requirements utilizing Enterprise standard tool sets Required Job Qualifications Bachelor’s degree or equivalent in MIS, Computer Science, Mathematics, Business or related field. 5+ years of experience in Technology related field including prior lead experience. For the senior level position 8+ years of experience in Technology related field including 3+ years prior lead experience. Advanced in-depth knowledge of Test Data and Test Env Management concepts and tools. Strong organizational, analytical, critical thinking and leadership skills. Demonstrated leadership on mid-large-scale project impacting strategic partners. Advanced in-depth knowledge of data warehouse and BI systems design, streaming architecture, and architecting and implementing large business systems is desired. Thrivent provides Equal Employment Opportunity EEO without regard to race, religion, color, sex , gender identity, sexual orientation, pregnancy, national origin, age, disability, marital status, citizenship status, military or veteran status, genetic information, or any other status protected by applicable local, state , or federal law. This policy applies to all employees and job applicants. Thrivent is committed to providing reasonable accommodation to individuals with disabilities. If you need a reasonable accommodation , please let us know by sending an email to human. resources@thrivent. com or call 800-847-4836 and request Human Resources. "
186,Lead Data Engineer,data engineer,/rc/clk?jk=487bb4f7c893dc12&fccid=a10b1d7176fac1c0&vjs=3,"Overview Now in its second century, Audubon is dedicated to protecting birds and other wildlife and the habitat that supports them. Audubon’s mission is engaging people in bird conservation on a hemispheric scale through science, policy, education and on-the-ground conservation action. By mobilizing and aligning its network of Chapters, Centers, State and Important Bird Area programs in the four major migratory flyways in the Americas, the organization will bring the full power of Audubon to bear on protecting common and threatened bird species and the critical habitat they need to survive. And as part of Bird Life International, Audubon will join people in over 100 in-country organizations all working to protect a network of Important Bird Areas around the world, leveraging the impact of actions they take at a local level. What defines Audubon’s unique value is a powerful grassroots network of nearly 500 local chapters, 23 state offices, 41 Audubon Centers, Important Bird Area Programs in 50 states, and 700 staff across the country. Audubon is a federal contractor and an Equal Opportunity Employer EOE . Position Summary This is a remote position and reports to the VP of Data and Analytics, the Lead Data Engineer will be charged with implementing data infrastructure that is core to the Data and Analytics team mission. This infrastructure should be built using modern-day data tools and platforms and will support the technology strategy and services that enable data-powered work and maximize Audubon’s mission impact. Data science, analytics, geographic mapping, and business intelligence are essential to how Audubon does its work across business teams including conservation, science, advocacy, marketing, fundraising, and finance. The Data and Analytics team plays a key role in empowering that work and needs an engineer who will be responsible for building and maintaining the infrastructure that will enable this mission. They will work closely with the VP of Data and Analytics to design and implement a new strategic data platform for the National Audubon Society that will support projects such as Migratory Bird Initiative, a multi-year project to track and aggregate varied data sources about migratory bird species, where they go, and the threats they face Integrate data from multiple sources e. g. , email, website interactions, CRM and volunteer activity, voter files, demographic data to support inclusion and equity metrics into a data warehouse and building optimized views and workflows for analysis, reporting, and dashboard visualization Identify, evaluate, and implement infrastructure solutions for unified business intelligence, data warehousing, scaling and parallel processing, and machine learning Proactive data analysis to identify issues and implement a plan to improve the quality and integrity of operational data, particularly constituent data across data-heavy systems like Salesforce and Every Action Advise on the design, build, and maintenance of structured databases to track curated data such as bird species taxonomies, bird-friendly native plants by geolocation, and important bird areas, along with product and engineering staff Convert raster data of predictive bird species ranges based on climate change models to vector features to enable enterprise GIS analysis and further conservation research Essential Functions Contribute to the design of a new strategic data infrastructure to support the National Audubon Society mission. Take ownership of building and maintaining the new data infrastructure, including data platforms, data pipelines, and analytical and business intelligence tools. Develop run books and documentation to detail all data infrastructure design and operations, including backup and recovery. Collaborate with the Enterprise Geographic Information Systems GIS team to leverage enterprise GIS and mapping technologies as part of the overall data architecture. Advise the VP of Data and Analytics, as well as others throughout the organization, on data-related tools and platforms, data best practices for integrity, testing, and modeling, and analytical approaches. Collaborate with the peer Engineering team to ensure data quality and to advise on the design and implementation of system integrations and API services. Work with the VP of Data and Analytics and the Director of IT to support data governance initiatives and to ensure the reliability and security of data services and infrastructure. Qualifications and Experience 10+ years of experience as a Data Engineer, or in similar roles, with increasing levels of responsibility. Degree in Computer Science, Statistics, Data Science, Mathematics, or related quantitative field strongly preferred. Significant hands-on experience building and working with data lakes, warehouses, marts, pipelines, and providing reporting or analytical services. Fluency in Linux-based operating systems. Experience with RDBM Systems and deep SQL experience. Experience with cloud data warehouse solutions like Redshift, Snowflake, Azure Data Warehouse, and/or Big Query. Experience with cloud infrastructure e. g. , AWS EC2, S3, Lambda or equivalent services from Azure or Google . Experience with Linux scripting, Python, Java, Scala, . NET, R or other programming languages proven to be robust and widely used for ETL/ELT and data analytics. Experience with business intelligence tools e. g. , Tableau, Looker, Power BI . Experience with or strong understanding of ETL/ELT and workflow tools like AWS Glue and Apache Airflow. Experience with or strong understanding of big data architectures and data modeling to efficiently process large volumes of data, including solutions like Spark, Hadoop, EMR, Kafka, etc. Experience supporting data science notebook environments such as Jupyter Hub or R Studio preferred. Experience building data architectures that support web application such as RES Tful web services. Familiarity with web data and mapping visualization technologies such as D3. js, Plotly, Arg GIS API for Java Script, and Leaflet a plus. Familiarity with machine learning/data science platforms and services like Sagemaker, Domino, Tensor Flow, etc. a plus. Demonstrated ability to communicate technical information to non-technical audiences. Self-starter who can work as part of a virtual team and remain motivated in a dynamic environment. Genuine passion for conservation and the mission of the National Audubon Society. "
187,Data Engineer,data engineer,/rc/clk?jk=3604ed5b4e7d4225&fccid=435926113439e52e&vjs=3,"About Us Chartbeat's software helps the world’s leading media companies understand, measure, and optimize the attention earned by their content. Our products are used by over 80% of the world’s top publishers, and we measure tens of billions of page views each month and nearly 100 million articles per year. This makes for a deeply rich dataset to collect, transform, and analyze in order to realize our mission of helping quality content thrive. The Team The data we process each day is massive in scale Chartbeat’s products are used by over 80% of the world’s top publishers, and we measure nearly 100 billion pageviews each month; over 1 million messages pass through our data systems each second and support measuring tens of millions of concurrent visitors across our customers’ sites. This makes for a deeply rich dataset to collect, transform, and analyze in order to realize our mission of helping quality content thrive. We are seeking curious Data Engineers to join us on this adventure by helping build the components, systems, and dashboards that newsrooms use to learn about and grow their audiences. Our data systems use an exciting array of technologies. Most of our data systems use Python, but key parts of our systems are written in Clojure, Java, C, and Lua where the requirements demand it. We use a variety of database technologies, including Postgre SQL, H Base, Mongo DB, Redshift, and Redis. Everything is connected by message buses powered by Apache Kafka and Rabbit MQ. As a data engineer at Chartbeat, you will be designing, building, scaling, and maintaining our data pipelines, data stores, analysis engines, and AP Is. You will be on a small cross-functional project team typically, a few engineers, a designer, a data scientist, and a product manager that will define your day-to-day activity — your team will collaborate to plan sprint goals and execute on those goals. About you Strong programming abilities in Python and/or Java or Clojure and the JVM Familiarity with architectural patterns used by different databases in large, high-scale applications particular experience with Redshift, H Base, or Postgre SQL is a plus Interest in working with and building distributed systems e. g. technologies like Zookeeper, Kafka Excellent verbal and written communication skills and a commitment to building an inclusive and collaborative working environment An interest in designing technical solutions to open-ended product problems Passion and enthusiasm about learning and teaching new technologies About working at Chartbeat This is a permanent, full-time position with excellent benefits—including flexible hours and generous parental leave. Chartbeat strives to provide comprehensive healthcare options for our employees and to ensure that our healthcare and other benefits are LGBTQ-inclusive. You'll be joining a group of focused, hard-working, people who are passionate about doing work that's challenging and fun—and who strive to maintain a healthy work/life balance. Chartbeat is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to apply. Chartbeat is an Equal Opportunity Employer and does not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by applicable national, federal, state, or local law. We Participate in E-Verify"
188,"Principal Data Engineer, Core Data & Analytics",data engineer,/rc/clk?jk=5365a5fa644d888b&fccid=f2cbb7715eae48aa&vjs=3,"At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference. Provides thought leadership, coaching, and strategic advice to other data engineering professionals Recognized data engineering expert owns one or more data engineering domain Deep hands-on expertise to create and continuously improve foundational data activation capabilities, platforms, frameworks, service Leads and drives the design and build of complex data optimization and preparation Designs and performs complex data extraction, assessment, translation, transformation, and load processes Leads other engineers in building data equity, it's underlying code, tools, databases, and related infrastructure Ensures existing data engineering pipelines are administered and maintained to their full extent of engineering needs Translates strategic needs into engineering goals milestones Analyzes complex data needs and identifies available internal and external sources. Utilizes business and analytical data modeling skills to design data integration and structure approaches. Utilizes programming skills to access and extract data from diverse sources residing on multiple platforms and implement large, complex data models by combining, synthesizing and structuring data from databases, files and spreadsheets. Assures data consistency and reliability. Performs quality checks, contributes to metadata and data dictionaries, documents repeatable extract, transform and load processes. Bachelor's degree in Computer Science, MIS, or related field 10+ years of experience in data engineering Data systems architecture experience, designing larger scale data engineering solutions, break down to smaller goals, prioritization Database administration experience Cloud experience Analytics understanding is a must Experience with relational data engineering eco systems preferably complex disbursed data Code/ETL knowledge in SQL and one or more of the following Informatica, python, scala, java, Bash, Snap Logic Experience in end to end data engineering solutions Demonstrated success in leading a team Ability to offer expertise, guidance, and mentoring in regard to best practices Can execute alone but is an awesome team player Project management experience ability to lead projects from conception to delivery Ability to own certain domains; to include maintaining streamlining, architecting, and developing additional capabilities Master's degree in Computer Science, MIS, or related field Financial Services industry background This job is not covered by the existing Collective Bargaining Agreement. Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law. "
189,Data Engineer,data engineer,/rc/clk?jk=d46a660ddf943ccd&fccid=7f8b9d8c8a5d8a90&vjs=3,"Vettery is changing the way people hire and get hired. We use machine learning and real-time data to match talented job-seekers with inspiring companies. Our goal is to enrich and automate the recruiting process, make hiring more rewarding for everyone, and create a happier and more accountable working world. Since launching in 2015, we've made thousands of matches on our marketplace. We're currently working with over 80,000 job-seekers and 20,000 companies, from Fortune 500 giants to startups based out of co-working spaces. We've built powerful machine learning capabilities, and our matching algorithm is becoming more intelligent with each passing day. With an eye on the future, we're expanding our reach across major cities in the US, and around the globe. Vettery engineers are building highly scalable distributed solutions to enable exponential customer and business growth, shaping our tech stack and business decisions with their input. Our Data Engineering team works closely with our Data Science team to productize their machine learning models. As a Data Engineer, you will join this growing team and help build our data infrastructure so that the platform can continue to scale. Key responsibilities and expectations Design, develop and maintain end-to-end data pipelines across multiple data sources and systems of record Develop and design data models, data structures and ETL jobs for data science consumption Manage and maintain cloud based data and analytics platform Productize data science models Contribute to decision making around architecture and new technology Mentor junior engineers Work in a highly agile development environment and practice Agile/Scrum methodology Participate in hackathons, team outings, lunches and other team building events Candidate Qualifications Self-starter who can work in a highly demanding environment and maintains a positive attitude 3+ years of data engineering experience, including collaboration with data scientists and leading projects Experience building ETL Extract, Transform, Load and data pipelines, ideally in multiple environments and with multiple methods Strong experience with SQL and Python; experience with big data pipelines spark, hive, etc. is a plus Cloud experience with AWS is required Experience with writing consumable AP Is is required; experience dealing with AP Is for data access is a plus Awareness of new and emerging Big Data technologies and trends Experience with any of the following is a plus EMR, docker containers, AWS Batch Experience with Airflow is a plus Experience with Kafka is a plus Experience with in-memory caching Redis or Memcache is a plus Experience with Platform Automation experience CI/CD, data lake, data access layer is a plus Advanced degree in computer science or engineering is a plus With the above said, we understand that no candidate is perfectly qualified for any job and believe that diversity of background and thought makes for better problem solving and creative thinking, which is why we're dedicated to adding new perspectives to the team. Even more important than your resume is a positive attitude, passion for the work, personal drive for growth, and the ability to thrive in a fluid and collaborative environment. We want you to learn new things in this role, and we encourage you to apply even if your experience doesn't align perfectly to what is listed here. Vettery has five key values that are the foundation of our company culture, which every employee embodies Positivity We're positive when things get difficult so we can stay motivated and lift each other up. We're very team focused in everything we do so contagious positive energy is extremely impactful. Ownership We take pride in our work and take on a lot of responsibility from day one. All of us have the ability to see how our work and performance impact the success of the business. Grit We love getting in the trenches and building from the ground up. Even though we've significantly grown since our tiny startup days, we still have that scrappy mentality and love that there's still a lot to accomplish. Awareness We're focused, strategic, and constantly learning from our experiences. Each of us knows what's expected of us as a corporate citizen and within our teams. Collaboration We learn from one another and are constantly working with each other, within and across teams. Every team has an impact on others and we take pride in clear lines of communication. Why you'll love working at Vettery We love coming to work on Monday. It's easy to love the work you do when you see the positive impact it has, and helping someone find their dream job can change their life forever. We believe in our mission, love the work, and have fun doing it together. Plus, coming to work in our sunny Flatiron office is easy when there are so many things to look forward to Flag Football games, Thursday Game Night, Cross-Team lunches, company happy hours, volunteer events, adorable pups, ping-pong, and your favorite snacks. We know life is about more than just work. We have an open vacation policy so you can take the time you need to relax and rejuvenate, contribute to the cost of insurance coverage health, vision, and dental , and offer a fully paid 12-week parental leave, 401k, commuter benefits, and gym membership discounts. We invest in your development. A company is only as strong as its team, and we want to help strengthen every member of our team. We give everyone the opportunities and support they need to reach that next professional level through company-sponsored General Assembly classes and conferences, in-house training, a culture of continuous feedback, and the chance to run with projects. We're consistently recognized for our culture. We're listed #5 on Fortune's 60 Best Companies to Work for in New York City, included in Fortune's Best Workplaces in Technology for 2020, and have been previously honored at Crain's Best Places to Work Awards and included in Inc Best Workplaces. Equal Opportunity Employer/Veterans/Disabled Vettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status. "
190,Data Engineer,data engineer,/rc/clk?jk=85dfa323d0a80f0e&fccid=54641a0cf597de67&vjs=3,"The Center for Court Innovation is a non-profit organization that works to create a more effective and humane justice system by performing original research and helping launch reforms by guiding justice innovators, nationally and internationally. The Center creates operating projects that have been documented to reduce the use of jail and prison, assist victims of crime, and strengthen communities. The Technology Support and Strategy Department is seeking a Data Engineer to support our extensive data needs and ensure our databases integrate data from diverse data sources and data outputs be usable by our analysts, researchers and staff. Our organization runs about 35 projects that work with justice-involved individuals and communities, as well as projects that build safe communities, across New York and New Jersey. We are in the process of moving the majority of our case management to Salesforce; however, we need to interface with data from across the organization on a variety of platforms, as well as data sources from external government and non-profit organizations. We also have several platforms used by administrative departments which requires integrating data from multiple sources. Reporting to the Information Architect, the Data Engineer will be responsible for designing, automating, and troubleshooting the cleaning, processing, and distribution of confidential data on a mass scale. This role would also involve collaborating with the senior project manager and the research and data analytics team, analyzing requirements and developing solutions. Responsibilities include but are not limited to Work with a variety of stakeholders both within our organization and with external partner agencies to determine data needs and engineer solutions to combine, clean, and transform existing data sets from multiple sources to meet those needs; Advise on possible solutions where it is not already possible to meet the data needs of our staff or our funders; Develop and maintain automated solutions to continually clean, transform, and validate data at scale; Develop and maintain efficient, reliable, and secure methods to retrieve and store data; Create and maintain a data warehouse, including establishing and documenting appropriate processes; Advise management on recommended software and infrastructure investments to better store and process data at scale; Collaborate closely with our research and data analytics team to meet their complex data needs; Assist with automating data flow between internal administrative technology platforms e. g. intranet, timesheets, etc ; Work with internal stakeholders to understand their reporting needs and create appropriate data marts, as needed; Additional relevant tasks, as needed. Qualifications Bachelor’s degree in Computer Science, Engineering or a related field is required; 3 years of experience as a Data Engineer or similar role is required; Experience working with Relational Databases Oracle and MS SQL Server is preferred; Experience with ETL processes is required; Experience with data warehouses is required; Experience working with XML, JSON and other data formats is required; Familiarity with BI/Visualization tools like Tableau, Power BI, Visokio is a plus; Programming experience in Python, SQL, SOQL or any programming language is a plus; Data integration experience is required including with Salesforce is a plus ; AWS/Azure experience is preferred; Familiarity with data security best practices is preferred; interest and willingness to learn in this area is a must. Additional Requirements Excellent communication skills; Attention to detail; Analytical mind and problem-solving aptitude; Strong organizational skills. Compensation Salary range starts at $90,000 and is commensurate with experience. Excellent benefits package including comprehensive health, dental and life insurance, four weeks paid vacation, paid sick time, Transit Chek, Flexible Spending Account plans for health and dependent care, a 403 b retirement plan and a 401 k retirement plan with a generous employer match. The Fund for the City of New York/Center for Court Innovation is an equal opportunity employer. The Center does not discriminate on the basis of race, color, religion, gender identity, gender expression, pregnancy, national origin, age, military service eligibility, veteran status, sexual orientation, marital status, disability, or any other category protected by law. We strongly encourage and seek applications from women, people of color, members of the lesbian, gay, bisexual, and transgender communities as well as individuals with prior contact with the criminal justice system. In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete an employment eligibility verification document form upon hire. Only applicants under consideration will be contacted. No phone calls please. Powered by Jazz HR Kmh H31gc Ny"
191,Data Engineer,data engineer,/rc/clk?jk=a04d189d8dc1b267&fccid=758966cb05365905&vjs=3,"We are looking for a Data Engineer to join our Data Engineering Team. If you have the passion to work on exciting data pipeline and data integration projects, then lets talk! Send your resume to careers@aretove. com Responsibilities In consultation with the project leadership perform data collection from various upstream and downstream systems. Build and optimize new and existing processes supporting data transformation, data structures, meta data, dependency and workload management. Execute extract, transform and load ETL operations on large datasets including data identification, mapping, aggregation, cleansing and analyzing. Create data pipeline by connecting to and ingesting data from third party databases and AP Is. Ability to clearly articulate and present data, deliver insights and recommendations to business stakeholders. Skills/Experience Bachelor’s degree or professional experience in a related field. You enjoy wrangling huge amounts of data and exploring new data sets. Strong analytic skills and ability to transform unstructured into structured data. Proficient in writing SQL queries. Ability to work independently, deal well with ambiguous and undefined problems. Ability to work in a fast-paced and agile environment. "
192,Data Engineer,data engineer,/rc/clk?jk=51e00ff5f8d74263&fccid=105ecfd0283f415f&vjs=3,"Duration 4+ months Requirement Advanced SQL Candidate Must Have Advanced SQL skills and experience. Experience with Snowflake SQL is a plus. Strong Communicator Candidate Must Have ability to clearly and confidently communicate with business client, IT client and 3rd party partners. Python Programming Language The candidate Should Have Python skills and experience. Or, they should have some Python training and at least have experience with other procedural language of similar or higher complexity such as Java, Scala, etc. Snowflake DB Experience with the Snowflake DB is a plus. Apache Airflow The candidate Should Have experience with an ETL Orchestration tool. Airflow experience is preferred, but experience with another ETL orchestration tool of similar or equal complexity such as Ni Fi would work as well. AWS Candidate Should Have experience with AWS and products such as Spark . Github Experience with Github is a plus- or, experience could be with a similar version control tool such as Bitbucket. Jira Experience with Jira is a plus- or, experience could be with a similar Agile development life cycle too such as Version One or Rally The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13. 2 billion. "
193,"Data Engineer: Bloomingdale's, New York, NY",data engineer,/rc/clk?jk=104375064cfb508b&fccid=5e6dd49bdb583f94&vjs=3,"Job Overview Bloomingdales is looking for a Data Engineer to work with technology partners and deliver cutting edge data systems that supports the Business Intelligence and Analytics functions within Marketing to maximize sales and drive customer loyalty. This individual will be responsible for integrating and automating data flows, designing and building data marts and managing big data systems, working with diverse on-premise systems and the Azure cloud. The Data Engineer will also be responsible for working with a wide variety of Bloomingdale’s associates and external vendors on data requirements to support strategic business needs. Performs other duties as assigned. Essential Functions Integrate data from multiple databases across different platforms such as Oracle, SQL Server, Teradata and Hadoop to integrate data and build a data lake and analytical data marts. Work with the Lead Data Architect to determine high level data strategy for the Enterprise Data Warehouse within a Microsoft Azure environment. Work extensively with Azure Data Warehouse MPP database platform. Create logical and physical data models for operational applications and all analytic applications. Participate in ETL design and act as the subject matter expert on source and target data structures. Work with various outside vendors and internal support teams in maintaining and/or enhancing processes and data integrations. Support business by generating ad-hoc analysis, using SQL, for overall business as needed. Perform extensive data quality and data analysis checks to ensure integrity of database for business use. Work with business partners on the full lifecycle of data related projects from requirements analysis and design through development. Maintain and manage scheduling and distribution of reporting. Manage and monitor server processes and jobs for on-premise and Azure cloud environments. Investigate and resolve root cause for complex data issues. Script and automate processes. Diagram and document databases and data flows. Regular, dependable attendance and punctuality. Qualifications Education/Experience Bachelor’s degree in Information Technology, Computer Science. Minimum 4 years’ experience in database development and administration. Experience with data integration and database design. Experience with Hadoop, Spark, Hive experience preferred. Experience with ETL tools. Microsoft SSIS preferred. Azure Data Factory nice to have. Experience with enterprise reporting tools. Business Objects, Microsoft SSRS, Power BI or Tableau preferred. Communication Skills Excellent data interpretation and communication skills; Ability to summarize complex findings clearly and concisely, ability to communicate effectively to all levels of management. Mathematical Skills Basic math functions such as addition, subtraction, multiplication, and division. Reasoning Ability Strong conceptual/analytical/creative thinker. Strong skills in SQL to analyze, manipulate data, investigate data problems, and develop reports. Must be able to work independently with minimal supervision. Strong organizational and time management skills; ability to adhere to deadlines. Physical Demands This position involves regular walking, standing, hearing, and talking. May occasionally involve stooping, kneeling, or crouching. Involves close vision, color vision, depth perception, and focus adjustment. Other Skills Knowledge of data warehousing methodologies. Familiarity with a programming/scripting language such as Java, . Net, Power Shell or Python. Work Hours Ability to work a flexible schedule based on department and company needs. This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment. "
194,Data Engineer,data engineer,/rc/clk?jk=9c643b83fe2dc7ba&fccid=b4048be2884af072&vjs=3,"Summary BNY Mellon Investment Management is one of the world's leading investment management organizations and one of the top U. S. wealth managers. Our business encompasses BNY Mellon's affiliated investment management firms, wealth management organization, and global distribution companies. Our goal is to build and deliver investment and wealth management strategies and solutions to meet our clients’ needs. Drawing on deep expertise, we collaborate with our clients to tailor our best ideas and resources to meet their specific requirements. Through our global network we have developed a significant understanding of local requirements. We pride ourselves on providing dedicated service through our teams. With extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments planned giving programs, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management can help our clients reach their goals. The Role Reporting to the Head of Investment Management IM Data Solutions, the Data Engineer will lead, design and architect solutions to complex data integration, process re-engineering, data science, and automation problems. This is a unique opportunity to join the IM Data Solutions team, a small, multi-disciplinary team that is transforming the way information is created, used and communicated within BNY Mellon IM. The group works directly with BNY Mellon’s IM firms on strategic projects, creating valuable tools and insights for decision makers across portfolio management, sales, marketing, and other key areas of the business. The Data Solutions team capitalizes on opportunities across business lines using the best tools and practices, like machine learning, econometrics, agile development scrum/kanban , and modern collaborative data platforms. Responsibilities Lead/contribute data engineering and/or data science projects to support IM firms Use domain expertise to understand business requirements and design the right solution Build or implement data pipelines, databases, visualizations, and other data tools hands-on Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets Qualifications BA/BS Degree advanced degrees and/or CF As are great too 5+ years general business experience preferred, especially in financial services, asset management, and/or management consulting or similar environments 5+ years of data engineering, data architecture, data science, and/or software engineering experience Excel proficiency SQL proficiency Python expertise Experience with machine learning algorithms and techniques Experience with modern data pipelines and/or data operating platforms e. g. Dataiku, Alteryx, Spark Able to lead ad-hoc and structured product teams within an agile framework Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums. Primary Location United States-New York-New York Internal Jobcode 70124 Job Asset Management Organization IM Infrastructure-HR14826 Requisition Number 2008484"
195,Data Engineer (Big Data)**NO C2C**,data engineer,/rc/clk?jk=51a0442806f601cd&fccid=5d7f543d6829250b&vjs=3,"WHO WE ARE BSI Solutions, Inc. is a globally recognized innovative focused software company; that specializes in partnering with corporations to digitally transform their operations through innovation. We believe in fostering a collaborative working environment with a family-oriented culture that provides exceptional leadership and growth opportunities. If you are interested in joining our family here at BSI Solutions, check out all of the many opportunities we have available and apply! POSITION SUMMARY Provides expert content/professional leadership on complex Engineering assignments/projects. Designs and develops a consolidated, conformed enterprise data warehouse and data lake which store all critical data across Customer, Provider, Claims, Client and Benefits data. Designs, develops and implements methods, processes, tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns. Participates in the delivery of the definitive enterprise information environment that enables strategic decision-making capabilities across enterprise via an analytics and reporting. Exercises considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives. Uses deep professional knowledge and acumen to advise functional leaders. Focuses on providing thought leadership within Information Management but works on broader projects, which require understanding of wider business. Recognized internally as a subject matter expert. Location Onsite AFTER Covid Denver, Co or Bloomfield, CT or New York, NY Contract 12 mo. + **NO CORP TO CORP ACCEPTED** KEY CHARACTERISTICS/WHAT YOU'LL DO Minimize ""meetings"" to get requirements and have direct business interactions Write referenceable & modular code Design and architect the solution independently Be fluent in particular areas and have proficiency in many areas Have a passion to learn Take ownership and accountability Understands when to automate and when not to Have a desire to simplify Be entrepreneurial / business minded Have a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact Take risks and champion new idea SKILLS/QUALIFICATIONS NEEDED TO SUCCEED 2+ years being part of Agile teams & Scrum or Kanban must 3+ years of working in an object-oriented language C, C++, Java, Scala, or other OO compiled language must 2+ years of scripting Java Script, Python, R, Ruby, Perl, etc. must 3+ years of database & SQL must Experience with Git/SVN and other code versioning tools Experience with big data technologies a plus, such as Hadoop, Hive,impala, H Base Spark Understanding of data streaming tools like Kafka, S Kyline is a plus Excellent troubleshooting skills Strong communication skills Fluent in BDD and TDD development methodologies Work in an agile CI/CD environment Jenkins experience a plus Knowledge and/or experience with Health care information domains is a plus BSI Solution, Inc. is proud to be an Equal Employment Opportunity and affirmative action employer. We celebrate diversity and do not discriminate based on race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law. "
196,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
197,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
198,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
199,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
200,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
201,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
202,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
203,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
204,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
205,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
206,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
207,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
208,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
209,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
210,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
211,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
212,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
213,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
214,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
215,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
216,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
217,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
218,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
219,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
220,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
221,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
222,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
223,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
224,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
225,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
226,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
227,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
228,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
229,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
230,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
231,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
232,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
233,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
234,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
235,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
236,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
237,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
238,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
239,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
240,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
241,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
242,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
243,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
244,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
245,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
246,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
247,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
248,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
249,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
250,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
251,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
252,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
253,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
254,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
255,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
256,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
257,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
258,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
259,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
260,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
261,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
262,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
263,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
264,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
265,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
266,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
267,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
268,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
269,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
270,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
271,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
272,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
273,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
274,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
275,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
276,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
277,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
278,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
279,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
280,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
281,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
282,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
283,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
284,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
285,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
286,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
287,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
288,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
289,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
290,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
291,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
292,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
293,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
294,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
295,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
296,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
297,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
298,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
299,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
300,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
301,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
302,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
303,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
304,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
305,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
306,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
307,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
308,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
309,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
310,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
311,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
312,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
313,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
314,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
315,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
316,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
317,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
318,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
319,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
320,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
321,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
322,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
323,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
324,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
325,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
326,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
327,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
328,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
329,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
330,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
331,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
332,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
333,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
334,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
335,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
336,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
337,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
338,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
339,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
340,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
341,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
342,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
343,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
344,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
345,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
346,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
347,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
348,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
349,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
350,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
351,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
352,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
353,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
354,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
355,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
356,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
357,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
358,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
359,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
360,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
361,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
362,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
363,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
364,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
365,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
366,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
367,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
368,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
369,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
370,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
371,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
372,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
373,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
374,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
375,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
376,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
377,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
378,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
379,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
380,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
381,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
382,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
383,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
384,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
385,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
386,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
387,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
388,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
389,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
390,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
391,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
392,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
393,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
394,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
395,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
396,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
397,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
398,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
399,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
400,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
401,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
402,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
403,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
404,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
405,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
406,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
407,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
408,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
409,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
410,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
411,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
412,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
413,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
414,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
415,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
416,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
417,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
418,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
419,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
420,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
421,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
422,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
423,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
424,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
425,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
426,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
427,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
428,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
429,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
430,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
431,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
432,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
433,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
434,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
435,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
436,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
437,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
438,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
439,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
440,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
441,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
442,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
443,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
444,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
445,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
446,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
447,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
448,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
449,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
450,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
451,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
452,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
453,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
454,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
455,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
456,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
457,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
458,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
459,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
460,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
461,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
462,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
463,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
464,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
465,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
466,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
467,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
468,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
469,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
470,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
471,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
472,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
473,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
474,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
475,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
476,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
477,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
478,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
479,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
480,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
481,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
482,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
483,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
484,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
485,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
486,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
487,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
488,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
489,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
490,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
491,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
492,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
493,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
494,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
495,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
496,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
497,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
498,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
499,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
500,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
501,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
502,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
503,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
504,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
505,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
506,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
507,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
508,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
509,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
510,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
511,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
512,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
513,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
514,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
515,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
516,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
517,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
518,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
519,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
520,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
521,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
522,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
523,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
524,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
525,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
526,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
527,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
528,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
529,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
530,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
531,Senior Data Engineer,data engineer,/rc/clk?jk=d4e803e05755612a&fccid=be3b11aa573faee7&vjs=3,"Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform ETL workflows. Fundamental Components Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Writes ETL Extract / Transform / Load processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with data science team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case. Background Experience Strong problem solving skills and critical thinking ability. Strong collaboration and communication skills within and across teams. 5 or more years of progressively complex related experience. Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources. Ability to understand complex systems and solve challenging analytical problems. Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, My SQL or No SQL or similar. Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment. Experience building data transformation and processing solutions. Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or Ph D preferred. Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline. Education Master's degree Percent of Travel Required 0 10% Business Overview Aetna, a CVS Health company, we are joined in a common purpose helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience active, veterans, reservists and National Guard as well as military spouses to apply for CVS Health job opportunities. "
532,Sr. Data Engineer - HBO Max,data engineer,/rc/clk?jk=90c7579f8697cc9e&fccid=a710372f724c1e2e&vjs=3,"Sr. Data Engineer HBO Max Location Burbank, California | Los Angeles, California | New York, New York | Seattle, Washington Business HBO Max Position Type Full Time Job ID 178115BR Company Overview Warner Media is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including HBO, HBO Max, Warner Bros. , TNT, TBS, tru TV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. HBO Max is the future of entertainment and storytelling. Warner Media’s new streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the Warner Media library including Warner Bros. , New Line, DC, CNN, TNT, TBS, tru TV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J. J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa Mc Carthy, Robert Zemeckis, Ellen De Generes, and other visionaries. The Job As the Senior Data Engineer on the HBO Max Data Insights & Operations team, you will be responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency and quality. This position will work closely with various divisions including Product, Content and Marketing. The Daily Strong experience in developing, constructing, testing, and maintaining existing and new architectures. Responsible for aligning architecture with business requirements. Ability to identify ways to improve data reliability, efficiency, and quality. Responsible for preparing data for predictive and prescriptive modeling. Ability to deploy sophisticated analytics programs, machine learning and statistical methods. Conduct research for industry and business questions that come up. The Essentials BS or MS degree in computer science or related field and direct experience in Data / Engineering / Application lifecycle. Minimum of 4-5 years of experience in data science engineering, with big data tools like Hadoop and Spark. Robust experience with relational SQL and No SQL databases like Mongo DB, Dynamo DB, Redshift/Post GRE, maria DB, etc. Experience with data pipeline and workflow management tools like airflow or alternatives. In depth experience and knowledge with AWS cloud services S3, EC2, EMR, RDS, Redshift, Lambda, API-Gateway, AWS GLUE, etc. Experience with stream-processing systems Spark-Streaming, AWS Kinesis, Kinesis Firehose etc. Must be experienced with object-oriented/object function scripting languages Python, Java, Scala, etc. Preferred experience of 3+ years of minimum experience with Python, Data Warehouse, Linux/Bash scripting development, Software Development Cycle and Data Modeling a must. The Perks Exclusive Warner Media events and advance screenings Paid time off every year to volunteer Access to well-being tools, resources, and freebies Access to in-house learning and development resources Part of the Warner Media family of powerhouse brands Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. "
533,Data Engineer,data engineer,/rc/clk?jk=975d796fbe28676c&fccid=39e46bf8aca75117&vjs=3,"Kindbody is a new generation of health and fertility care. We provide fertility, gynecology, and wellness services in modern, tech-enabled clinics delivering best-in-class clinical care, accessible pricing, and a seamless patient experience. We serve patients and employers in New York, San Francisco, Los Angeles, and nationally online and through our partner clinics. Join our experienced team of rockstars from Stanford, Mt. Sinai, One Medical, Progyny, Flatiron Health, Google, and Goldman Sachs. We are hiring a Data Engineer to build our Analytics, Data Warehouse, and Data Pipelines. The Data Engineer will work with many business teams including marketing, sales, operation, clinical, engineering, and executives to scale Kindbody's data infrastructure to enable Kindbody's strategic growth. Our Tech Stack AWS RDS, GCP Postgre SQL. CI/CD Pipelines Segment Metabase Responsibilities Understand the data needs of different stakeholders across business teams. Provide proactive solutions to enable stakeholders to extract insights and value from a variety of data sources at Kindbody. Own business intelligence and analytics platform tools to deliver near-real-time data insights to both internal and external stakeholders. Design best practices for data processing, data modeling. Create and optimize data pipelines. Understand data interactions, dependencies, data transformations across complex data pipelines, and how they impact business decisions. Attributes / Personal Skill Own it. Take technical responsibility for a family of systems be on the lookout for opportunities for improvement to keep our production systems scaling and performing fast Speak up! Contribute to the conversation concerning sprints and deliverables Get involved. Design and architect systems that can span multiple teams and/or products Be thoughtful. Communicate with leadership, product owners, other technologists and users to build useful, relevant, dependable solutions Think critically. Break down complex problems into elegant technical solutions to intelligently solve problems for our patients and internal stakeholders Ability to dive in, go with the flow, prioritize, and support the needs of a rapidly growing startup! Be curious! Actively contribute to the adoption of strong architecture, best practices, and new technologies Desired Skills and Experience 2+ years of experience as a Data Engineer or in a similar role. 2+ years of experience in using data visualization tools such as Metabase, Looker, Power BI 3+ years of hands-on SQL experience. Experience with data modeling, data warehousing, and building ETL pipelines. Experience working with Healthcare data feeds will be a plus. Knowledge either in R or Python and working with data warehousing solutions such as AWS Redshift, Google Big Query, Snowflake will be a plus. Knowledge of data management fundamentals and data storage principles. Demonstrated strength in data modeling, ETL development, and data warehousing. Experience working with AWS big data technologies EMR, Redshift, S3 . Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy. Empathy and patience providing support to all levels of tech literacy. Experience building HIPAA-compliant software preferred. Kindbody provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. "
534,Data Engineer,data engineer,/rc/clk?jk=bfa4083f19c45002&fccid=ca94b6e9b2058331&vjs=3,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By providing unparalleled access to property intelligence, Reonomy products empower individuals, teams, and companies to unlock insights and discover new opportunities. Headquartered in New York, Reonomy has raised $128 million from top investors, including Sapphire Ventures, Bain Capital, Softbank, Primary Ventures, Georgian Partners, Wells Fargo Strategic Capital, Citi Ventures, and Untitled Investments. Our clients represent the biggest names in CRE, including Newmark Knight Frank, CBRE, Cushman & Wakefield, and Tishman Speyer. If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning! ABOUT THE ROLE As a Data Engineer at Reonomy, you will tackle hard challenges every day! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE Saa S solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, Elastic Search and Docker. Responsibilities include Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer Creating data systems that ensure quality and consistency on our data platform Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data Playing a major role in the future architecture of our rapidly expanding backend platform Writing high quality code, participating actively in code reviews, and consistently helping to ship software ABOUT YOU 6+ years of experience in a Data Engineering capacity Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets Proven ability leveraging database technologies to solve non-trivial, large-scale problems Advanced/Expert knowledge in SQL and data analysis Experience programming in both typed Scala, Java, etc. . and non-typed Python, Ruby, etc. . languages on production projects Experience using Scala libraries such as Cats, shapeless, fs2 Experience building modern, data-driven, web applications with emphasis on strong software design methodologies A serious passion for data History of excellence and responsibility in previous engineering positions BENEFITS Competitive salary Company stock options 100% coverage on medical, vision and dental health plans Unlimited Vacation 401k plan and commuter benefits Access to our Continuing Education Stipend Perks WFH package, FREE Class Pass membership, FREE Headspace premium membership, FREE Citi Bike membership, & team outings! Applicants must be currently authorized to work in the United States on a full-time basis. We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means. "
535,Big Data Engineer,data engineer,/company/CloudZenix-LLC/jobs/Big-Data-Engineer-af3fd3d5ae2dc643?fccid=715153f89882dda2&vjs=3,"Job Description We are looking for a Big Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Responsibilities Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Implementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changes Defining data retention policies{{Add any other responsibility that is relevant}}Skills and Qualifications Proficient understanding of distributed computing principles Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data Dev Ops roles for this}}Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data Dev Ops roles for this}}Proficiency with Hadoop v2, Map Reduce, HDFS Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala Experience with Spark {{if you are including or planning to include it}}Experience with integration of data from multiple data sources Experience with No SQL databases, such as H Base, Cassandra, Mongo DB Knowledge of various ETL techniques and frameworks, such as Flume Experience with various messaging systems, such as Kafka or Rabbit MQ Experience with Big Data ML toolkits, such as Mahout, Spark ML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}}Good understanding of Lambda Architecture, along with its advantages and drawbacks Experience with Cloudera/Map R/Hortonworks {{you can specify the distribution you are currently using or planning to use here}}{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here The Hadoop Ecosystem Table}Job Type Contract Pay $45. 00 $50. 00 per hour Schedule Monday to Friday Experience Kafka 6 years Preferred Cloudera 5 years Preferred Hive 8 years Preferred Machine Learning 2 years Preferred No SQL 5 years Preferred Spark 5 years Preferred Map R 5 years Preferred Big Data 10 years Preferred Flume 3 years Preferred ETL 3 years Preferred Work authorization United States Required Required travel 100% Required Security Clearance Required Confidential Preferred Work Remotely Temporarily due to COVID-19"
536,Data Engineer (Remote),data engineer,/rc/clk?jk=bc758126d33e3581&fccid=79549f0666027dc4&vjs=3,"Overview Overview Strategic is looking for a Data Engineer to build out our first data warehouse. You’ll be our first Data Engineer and have a critical role in shaping our data architecture, ETL processes and systems. You’ll work across the business with our Product, Marketing, Sales, Analytics and Customer Service teams every day to improve our data warehouse and the quality of our BI platform. Additionally, you’ll play a critical role in building out our data architecture in Azure. Do you enjoy diving into technical problems with your team and helping them succeed? Great. Come work at Strategic! As a Data Engineer, you’ll contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing, and operating stable, scalable, and efficient solutions to flow data from production systems into our data warehouse. Additionally, you’ll work closely with our Analytics teams to QA and vet our data to ensure that the Power BI dashboards that run our business are up-to-date and accurate. Creating and maintaining a collaborative culture built on helping other teammates while learning new technologies and skills is a critical aspect of this role. We’re a relatively small Dev Ops team in a stable yet fast-growing financial company. Responsibilities Products we craft We are building tools for our sales team to help relieve people of their burden of debt. Leveraging technology to make a person’s life better is something you will do every day. Learning something new every day is something we hope! you will do as your build out your data pipelines and ETL processes. Some of our applications are custom developed in-house, and others integrate with Saa S products and Salesforce. Tools we use For managing our work, we use Jira Kanban boards along with Confluence for documentation. We also use Slack and Zoom for video conferencing. Within devops we are beginning to use Terraform for our production infrastructure and Splunk for unified logging aggregation/correlation . All of our software deployments are moving to Git Hub Actions. About the Role As a Data Engineer at SFS, you'll collaborate closely with our Development team and work within our strong Dev Ops team. Some of the areas where you will be assisting will include, in no specific order Interfacing with other engineers to extract, transform, and load ETL data from a wide variety of in-house and third-party data sources. Ensuring we have data consistency on both production and analytical databases. You’ll own the integrity of our data from end to end, and the company will make high impact decisions based on this data. Architecting and building a data warehouse to provide timely data to a variety of third-party applications Salesforce, Mule Soft, etc . Designing and building tools that make our data pipelines more reliable, manageable and resilient. Triaging, identifying, and scaling issues within our data warehouse. Collaborating with internal data customers to gather requirements. Evolving our data engineering function in areas of data architecture, machine learning and predictive analytics. At Strategic, we are a team of learners, who look for the best ideas from across anywhere. We also try new things, test them with our teams, and look at how to improve our software, process, and potential. Qualifications Qualifications What we're looking for Bachelors in Computer Science or similar experience 4+ years experience with at least one relational database—Microsoft SQL, My SQL, Postgre SQL, etc. Data Warehousing and Data Lake experience using modern tools. Experience with large-scale data pipelines and ETL tooling. Strict confidentiality of sensitive customer data. Advanced experience working with Git rebasing/merging, cherry picking, conflict resolution, etc. A team player with a solution oriented attitude with both the technical and soft skills to get things done. Bonus points Experience with Microsoft Azure or Amazon Web Services Business Intelligence, Analytics or Finance experience. Experience with Big Data Technologies. About Strategic Strategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States. Strategic is committed to hiring, training, cultivating, promoting, and celebrating an environment where we have a welcoming and fulfilling place for all people to call home. Diversity of race, thought, sexual orientation, age, veteran status, religion, and disability will empower us to thrive as individuals, as teams, and as a company. "
537,"Manager, Data Engineer",data engineer,/rc/clk?jk=efff29956a6f7e17&fccid=1b0043cf80779505&vjs=3,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. Our teams work on the latest data technologies in the Cloud space including Big Query, Redshift, Spark, etc. This position will be focused on building out a data warehouse, data integration, or machine learning automation projects in various cloud environments. Primary Responsibilities Deliver quality work on defined tasks with little oversight and direction. Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews. Participate in integrated test sessions of components and subsystems on test and production servers. system designs. Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs. Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed. Required Skills and Experience BS in Computer Science or equivalent education/professional experience is required. 5+ years in a data engineering role with demonstrable experience with data integration and data warehouse projects. Experience in column oriented database technologies i. e. Big Query, Redshift, Vertica, etc. and traditional database systems SQL Server, Oracle, My SQL, etc. Experience in data integration projects and automation via ETL Tools i. e. Talend, Informatica, SSIS, etc. Experience in data integration automation via Google Data Flow, Apache Beam, Spark, AWS EMR, etc. a huge plus. Experience with data modeling, warehouse design, and fact/dimension concepts. Experience architecting and building data marts, warehouses, etc. Experience working with different query languages i. e. PL-SQL, T-SQL . Experience working with code repositories and continuous integration i. e. Git, Jenkins, etc. Understanding of development and project methodologies. Ability to work collaboratively in teams with other specialized individuals. Able to work in a fast-paced, technical environment. Good verbal and written communication skills. #LI-JF"
538,Data Engineer,data engineer,/rc/clk?jk=0d7e8cf8b0d04263&fccid=e16a3feb243b7110&vjs=3,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work. ** COMPANY OVERVIEW Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U. S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership. Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us. POSITION OVERVIEW Summary The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems. Responsibilities Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc. Assemble large, complex data sets that meet functional / non-functional business requirements. Create and maintain optimal data pipeline architecture. Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality. Partner with business leaders to determine and prioritize delivery initiatives. Define or influence system, technical and application architectures for major areas of development. Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance. Engage with business partners to report formally and informally on technology strengths, weaknesses, successes and challenges on a regular basis. Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture. Highly organized and analytic, capable of solving business problems using technology. Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time. Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms. Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data. Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc. QUALIFICATIONS Undergraduate degree required, MBA or other advanced degree preferred. 3+ years of experience as a member of an information technology team. Minimum of 2 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL Minimum of 2 years of professional experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc. Ideally a strong Life Insurance background. Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements. Experience building solutions for streaming applications. Should be able to lead critical aspects of the data management and application management. Experience in UNIX shell scripting, batch scheduling and version control tools. Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs. Cultural awareness with excellent interpersonal and relationship building skills. Passion and drive for continuous improvement and transformational change, with a business owner mentality. Strong written and verbal communication skills. At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs. In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission. Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family. Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status. Global Atlantic Financial Company Employee Candidate Privacy Notice"
